\documentclass{phil-paper}

\title{Rational Reinforcement Learning}
\author{Paul Talma}

\begin{document}
Reinforcement learning is a framework for modeling sequential decision-making.
In addition to its use in cognitive and neural modeling [CITE: gureckis and love], reinforcement learning is one of the foundational machine learning approaches to artificial intelligence.
According to the ``standard model,'' artificial intelligence aims to design \emph{rational agents}, understood as agents that maximize their expected utility [CITE: Russell and Norvig].
Reinforcement learning agents do not in general maximize expected utility.
Instead, they follow a policy learned through experience.
In favorable circumstances, with a good learning algorithm, and with enough training, the agent learns an approximately optimal policy.
Nonetheless, reinforcement learners fall short of full Bayesian rationality.
Do these shortcomings merely reflect the difficulty of building an ideal Bayesian agent---engineering compromises in the face of the intractability of full Bayesian rationality?
Or can the tools of reinforcement learning be vindicated from within the Bayesian perspective?


% \include{bdt}

% \include{rl-summary}

% \include{connecting-rl-bdt}


\include{phil-rl-lit-review}
	
\end{document}

