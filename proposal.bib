@article{abelDefinitionContinualReinforcement2023,
  title = {A {{Definition}} of {{Continual Reinforcement Learning}}},
  author = {Abel, David and Barreto, Andre and Roy, Benjamin Van},
  year = {2023},
  abstract = {In a standard view of the reinforcement learning problem, an agent's goal is to efficiently identify a policy that maximizes long-term reward. However, this perspective is based on a restricted view of learning as finding a solution, rather than treating learning as endless adaptation. In contrast, continual reinforcement learning refers to the setting in which the best agents never stop learning. Despite the importance of continual reinforcement learning, the community lacks a simple definition of the problem that highlights its commitments and makes its primary concepts precise and clear. To this end, this paper is dedicated to carefully defining the continual reinforcement learning problem. We formalize the notion of agents that ``never stop learning'' through a new mathematical language for analyzing and cataloging agents. Using this new language, we define a continual learning agent as one that can be understood as carrying out an implicit search process indefinitely, and continual reinforcement learning as the setting in which the best agents are all continual learning agents. We provide two motivating examples, illustrating that traditional views of multi-task reinforcement learning and continual supervised learning are special cases of our definition. Collectively, these definitions and perspectives formalize many intuitive concepts at the heart of learning, and open new research pathways surrounding continual learning agents.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/6BK3UH6C/Abel et al. A Deﬁnition of Continual Reinforcement Learning.pdf}
}

@article{abelStateAbstractionCompression2019,
  title = {State {{Abstraction}} as {{Compression}} in {{Apprenticeship Learning}}},
  author = {Abel, David and Arumugam, Dilip and Asadi, Kavosh and Jinnai, Yuu and Littman, Michael L. and Wong, Lawson L.S.},
  year = {2019},
  month = jul,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {01},
  pages = {3134--3142},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33013134},
  urldate = {2025-01-28},
  abstract = {State abstraction can give rise to models of environments that are both compressed and useful, thereby enabling efficient sequential decision making. In this work, we offer the first formalism and analysis of the trade-off between compression and performance made in the context of state abstraction for Apprenticeship Learning. We build on Rate-Distortion theory, the classic Blahut-Arimoto algorithm, and the Information Bottleneck method to develop an algorithm for computing state abstractions that approximate the optimal tradeoff between compression and performance. We illustrate the power of this algorithmic structure to offer insights into effective abstraction, compression, and reinforcement learning through a mixture of analysis, visuals, and experimentation.},
  copyright = {https://www.aaai.org},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/5JBV94MD/Abel et al. (2019) - State Abstraction as Compression in Apprenticeship Learning.pdf}
}

@article{abelStateAbstractionsLifelong2018,
  title = {State {{Abstractions}} for {{Lifelong Reinforcement Learning}}},
  author = {Abel, David and Arumugam, Dilip and Lehnert, Lucas and Littman, Michael L},
  year = {2018},
  abstract = {In lifelong reinforcement learning, agents must effectively transfer knowledge across tasks while simultaneously addressing exploration, credit assignment, and generalization. State abstraction can help overcome these hurdles by compressing the representation used by an agent, thereby reducing the computational and statistical burdens of learning. To this end, we here develop theory to compute and use state abstractions in lifelong reinforcement learning. We introduce two new classes of abstractions: (1) transitive state abstractions, whose optimal form can be computed efficiently, and (2) PAC state abstractions, which are guaranteed to hold with respect to a distribution of tasks. We show that the joint family of transitive PAC abstractions can be acquired efficiently, preserve near optimal-behavior, and experimentally reduce sample complexity in simple domains, thereby yielding a family of desirable abstractions for use in lifelong reinforcement learning. Along with these positive results, we show that there are pathological cases where state abstractions can negatively impact performance.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/Z6MZ27J5/Abel et al. State Abstractions for Lifelong Reinforcement Learning.pdf}
}

@article{abelThreeDogmasReinforcement2024,
  title = {Three {{Dogmas}} of {{Reinforcement Learning}}},
  author = {Abel, David and Ho, Mark K and Harutyunyan, Anna},
  year = {2024},
  abstract = {Modern reinforcement learning has been conditioned by at least three dogmas. The first is the environment spotlight, which refers to our tendency to focus on modeling environments rather than agents. The second is our treatment of learning as finding the solution to a task, rather than adaptation. The third is the reward hypothesis, which states that all goals and purposes can be well thought of as maximization of a reward signal. These three dogmas shape much of what we think of as the science of reinforcement learning. While each of the dogmas have played an important role in developing the field, it is time we bring them to the surface and reflect on whether they belong as basic ingredients of our scientific paradigm. In order to realize the potential of reinforcement learning as a canonical frame for researching intelligent agents, we suggest that it is time we shed dogmas one and two entirely, and embrace a nuanced approach to the third.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/J6GXZVRV/Abel et al. (2024) - Three Dogmas of Reinforcement Learning.pdf}
}

@incollection{akaikeInformationTheoryExtension1998,
  title = {Information {{Theory}} and an {{Extension}} of the {{Maximum Likelihood Principle}}},
  booktitle = {Selected {{Papers}} of {{Hirotugu Akaike}}},
  author = {Akaike, Hirotogu},
  editor = {Parzen, Emanuel and Tanabe, Kunio and Kitagawa, Genshiro},
  year = {1998},
  pages = {199--213},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-1694-0_15},
  urldate = {2024-11-22},
  abstract = {In this paper it is shown that the classical maximum likelihood principle can be considered to be a method of asymptotic realization of an optimum estimate with respect to a very general information theoretic criterion. This observation shows an extension of the principle to provide answers to many practical problems of statistical model fitting.},
  isbn = {978-1-4612-1694-0},
  langid = {english},
  keywords = {Autoregressive Model,Final Prediction Error,Maximum Likelihood Principle,Statistical Decision Function,Statistical Model Identification},
  file = {/Users/paultalma/Zotero/storage/YIQ4UR52/Akaike (1998) - Information Theory and an Extension of the Maximum Likelihood Principle.pdf}
}

@misc{AlgorithmicLevelBridge,
  title = {The {{Algorithmic Level Is}} the {{Bridge Between Computation}} and {{Brain}} - {{Love}} - 2015 - {{Topics}} in {{Cognitive Science}} - {{Wiley Online Library}}},
  urldate = {2025-01-28},
  howpublished = {https://onlinelibrary.wiley.com/doi/10.1111/tops.12131},
  file = {/Users/paultalma/Zotero/storage/BVEYGVLD/tops.html}
}

@misc{amirStatesGoaldirectedConcepts2024,
  title = {States as Goal-Directed Concepts: An Epistemic Approach to State-Representation Learning},
  shorttitle = {States as Goal-Directed Concepts},
  author = {Amir, Nadav and Niv, Yael and Langdon, Angela},
  year = {2024},
  month = jan,
  number = {arXiv:2312.02367},
  eprint = {2312.02367},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.02367},
  urldate = {2025-01-28},
  abstract = {Our goals fundamentally shape how we experience the world. For example, when we are hungry, we tend to view objects in our environment according to whether or not they are edible (or tasty). Alternatively, when we are cold, we may view the very same objects according to their ability to produce heat. Computational theories of learning in cognitive systems, such as reinforcement learning, use the notion of "state-representation" to describe how agents decide which features of their environment are behaviorally-relevant and which can be ignored. However, these approaches typically assume "ground-truth" state representations that are known by the agent, and reward functions that need to be learned. Here we suggest an alternative approach in which state-representations are not assumed veridical, or even pre-defined, but rather emerge from the agent's goals through interaction with its environment. We illustrate this novel perspective by inferring the goals driving rat behavior in an odor-guided choice task and discuss its implications for developing, from first principles, an information-theoretic account of goal-directed state representation learning and behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Information Theory,Mathematics - Information Theory,Quantitative Biology - Neurons and Cognition},
  file = {/Users/paultalma/Zotero/storage/6MWF73CZ/Amir et al. (2024) - States as goal-directed concepts an epistemic approach to state-representation learning.pdf;/Users/paultalma/Zotero/storage/4EZANSLG/2312.html}
}

@article{andersonMechanismsValuelearningGuidance2018,
  title = {Mechanisms of Value-Learning in the Guidance of Spatial Attention},
  author = {Anderson, Brian A. and Kim, Haena},
  year = {2018},
  month = sep,
  journal = {Cognition},
  volume = {178},
  pages = {26--36},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2018.05.005},
  urldate = {2025-01-28},
  abstract = {The role of associative reward learning in the guidance of feature-based attention is well established. The extent to which reward learning can modulate spatial attention has been much more controversial. At least one demonstration of a persistent spatial attention bias following space-based associative reward learning has been reported. At the same time, multiple other experiments have been published failing to demonstrate enduring attentional biases towards locations at which a target, if found, yields high reward. This is in spite of evidence that participants use reward structures to inform their decisions where to search, leading some to suggest that, unlike feature-based attention, spatial attention may be impervious to the influence of learning from reward structures. Here, we demonstrate a robust bias towards regions of a scene that participants were previously rewarded for selecting. This spatial bias relies on representations that are anchored to the configuration of objects within a scene. The observed bias appears to be driven specifically by reinforcement learning, and can be observed with equal strength following non-reward corrective feedback. The time course of the bias is consistent with a transient shift of attention, rather than a strategic search pattern, and is evident in eye movement patterns during free viewing. Taken together, our findings reconcile previously conflicting reports and offer an integrative account of how learning from feedback shapes the spatial attention system.},
  keywords = {Attentional capture,Real world scenes,Reward learning,Selective attention,Spatial attention},
  file = {/Users/paultalma/Zotero/storage/X4BTYHU7/Anderson and Kim (2018) - Mechanisms of value-learning in the guidance of spatial attention.pdf;/Users/paultalma/Zotero/storage/7S275ML4/S0010027718301252.html}
}

@article{annurev:/content/journals/10.1146/annurev-control-030323-022510,
  title = {Deep Reinforcement Learning for Robotics: A Survey of Real-World Successes},
  author = {Tang, Chen and Abbatematteo, Ben and Hu, Jiaheng and Chandra, Rohan and {Mart{\'i}n-Mart{\'i}n}, Roberto and Stone, Peter},
  year = {2024},
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  publisher = {Annual Reviews},
  doi = {10.1146/annurev-control-030323-022510},
  abstract = {Reinforcement learning (RL), particularly its combination with deep neural networks, referred to as deep RL (DRL), has shown tremendous promise across a wide range of applications, suggesting its potential for enabling the development of sophisticated robotic behaviors. Robotics problems, however, pose fundamental difficulties for the application of RL, stemming from the complexity and cost of interacting with the physical world. This article provides a modern survey of DRL for robotics, with a particular focus on evaluating the real-world successes achieved with DRL in realizing several key robotic competencies. Our analysis aims to identify the key factors underlying those exciting successes, reveal underexplored areas, and provide an overall characterization of the status of DRL in robotics. We highlight several important avenues for future work, emphasizing the need for stable and sample-efficient real-world RL paradigms; holistic approaches for discovering and integrating various competencies to tackle complex long-horizon, open-world tasks; and principled development and evaluation procedures. This survey is designed to offer insights for both RL practitioners and roboticists toward harnessing RL\&apos;s power to create generally capable real-world robotic systems.},
  file = {/Users/paultalma/Zotero/storage/5C39E6ZF/Tang et al. (2024) - Deep reinforcement learning for robotics a survey of real-world successes.pdf}
}

@article{aronowitzLocatingValuesSpace2024,
  title = {Locating {{Values}} in the {{Space}} of {{Possibilities}}},
  author = {Aronowitz, Sara},
  year = {2024},
  month = oct,
  journal = {Philosophy of Science},
  pages = {1--20},
  issn = {0031-8248, 1539-767X},
  doi = {10.1017/psa.2024.42},
  urldate = {2024-12-06},
  abstract = {Where do values live in thought? A straightforward answer is that we (or our brains) make decisions using explicit value representations which are our values. Recent work applying reinforcement learning to decision-making and planning suggests that more specifically, we may represent both the instrumental expected value of actions as well as the intrinsic reward of outcomes. In this paper, I argue that identifying value with either of these representations is incomplete. For agents such as humans and other animals, there is another place where reward can be located in thought: the division of the space of possibilities or `state space'.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/ENI25UYR/Aronowitz (2024) - Locating Values in the Space of Possibilities.pdf}
}

@incollection{aronowitzPartsImperfectAgent2023,
  title = {The {{Parts}} of an {{Imperfect Agent}}},
  booktitle = {Oxford {{Studies}} in {{Philosophy}} of {{Mind Volume}} 3},
  author = {Aronowitz, Sara},
  editor = {Kriegel, Uriah},
  year = {2023},
  month = jul,
  edition = {1},
  pages = {3--28},
  publisher = {Oxford University PressOxford},
  doi = {10.1093/oso/9780198879466.003.0001},
  urldate = {2025-01-31},
  abstract = {Formal representations drawn from rational choice theory have been used in a variety of ways to fruitfully model the way in which actual agents are approximately rational. This analysis requires bridging between ideal normative theory, in which the mechanisms, representations, and other such internal parts are in an important sense interchangeable, and descriptive psychological theory, in which understanding the internal workings of the agent is often the main goal of the entire inquiry. In this paper, I raise a problem brought on by this gap: for almost every theory of approximate rationality, there will be an empirically indistinguishable alternative that individuates the parts of the agent in a significantly different way.},
  isbn = {978-0-19-887946-6 978-0-19-198934-6},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/RY5A7NAN/Aronowitz (2023) - The Parts of an Imperfect Agent.pdf}
}

@article{aronowitzPlanningTheoryBelief2023,
  title = {A Planning Theory of Belief},
  author = {Aronowitz, Sara},
  year = {2023},
  journal = {Philosophical Perspectives},
  volume = {37},
  number = {1},
  pages = {5--17},
  issn = {1520-8583},
  doi = {10.1111/phpe.12178},
  urldate = {2024-12-06},
  copyright = {{\copyright} 2023 The Authors. Philosophical Perspectives published by Wiley Periodicals LLC.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/9U3EQACH/Aronowitz (2023) - A planning theory of belief.pdf;/Users/paultalma/Zotero/storage/B223SN5J/phpe.html}
}

@article{arumugamBayesianReinforcementLearning2024,
  title = {Bayesian {{Reinforcement Learning With Limited Cognitive Load}}},
  author = {Arumugam, Dilip and Ho, Mark K. and Goodman, Noah D. and Van Roy, Benjamin},
  year = {2024},
  month = apr,
  journal = {Open Mind},
  volume = {8},
  pages = {395--438},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00132},
  urldate = {2025-01-28},
  abstract = {All biological and artificial agents must act given limits on their ability to acquire and process information. As such, a general theory of adaptive behavior should be able to account for the complex interactions between an agent's learning history, decisions, and capacity constraints. Recent work in computer science has begun to clarify the principles that shape these dynamics by bridging ideas from reinforcement learning, Bayesian decision-making, and rate-distortion theory. This body of work provides an account of capacity-limited Bayesian reinforcement learning, a unifying normative framework for modeling the effect of processing constraints on learning and action selection. Here, we provide an accessible review of recent algorithms and theoretical results in this setting, paying special attention to how these ideas can be applied to studying questions in the cognitive and behavioral sciences.},
  file = {/Users/paultalma/Zotero/storage/SMD86JA7/Arumugam et al. (2024) - Bayesian Reinforcement Learning With Limited Cognitive Load.pdf;/Users/paultalma/Zotero/storage/VZ5HKE58/Bayesian-Reinforcement-Learning-With-Limited.html}
}

@article{baiCostlyExplorationProduces2024,
  title = {Costly Exploration Produces Stereotypes with Dimensions of Warmth and Competence.},
  author = {Bai, Xuechunzi and Griffiths, Thomas L. and Fiske, Susan T.},
  year = {2024},
  month = nov,
  journal = {Journal of Experimental Psychology: General},
  issn = {1939-2222, 0096-3445},
  doi = {10.1037/xge0001694},
  urldate = {2025-01-28},
  abstract = {Stereotypes are multidimensional, including features that go beyond sheer good--bad valence. Current psychological theories, which focus on social, cognitive, and sample biases, do not explain the origins of such complex stereotypes. In this article, we show that a novel psychological mechanism can reproduce the multidimensional stratification of social groups and the resulting complex stereotypes: When individuals make self-interested decisions based on past experiences in an environment where exploring new options carries an implicit cost and when options share similar attributes, they are more likely to separate groups along multiple dimensions. A further set of intervention experiments provides causal evidence that reducing exploration cost can substantially mitigate even complex stereotypes.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/TFFYCKU5/Bai et al. (2024) - Costly exploration produces stereotypes with dimensions of warmth and competence..pdf}
}

@article{baiGloballyInaccurateStereotypes2022,
  title = {Globally {{Inaccurate Stereotypes Can Result From Locally Adaptive Exploration}}},
  author = {Bai, Xuechunzi and Fiske, Susan T. and Griffiths, Thomas L.},
  year = {2022},
  month = may,
  journal = {Psychological Science},
  volume = {33},
  number = {5},
  pages = {671--684},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/09567976211045929},
  urldate = {2025-01-28},
  abstract = {Inaccurate stereotypes---perceived differences among groups that do not actually differ---are prevalent and consequential. Past research explains stereotypes as emerging from a range of factors, including motivational biases, cognitive limitations, and information deficits. Considering the minimal forces required to produce inaccurate assumptions about group differences, we found that locally adaptive exploration is sufficient: An initial arbitrary interaction, if rewarding enough, may discourage people from investigating alternatives that would be equal or better. Historical accidents can snowball into globally inaccurate generalizations, and inaccurate stereotypes can emerge in the absence of real group differences. Using multiarmed-bandit models, we found that the mere act of choosing among groups with the goal of maximizing the long-term benefit of interactions is enough to produce inaccurate assessments of different groups. This phenomenon was reproduced in two large online experiments with English-speaking adults (N = 2,404), which demonstrated a minimal process that suffices to produce biased impressions.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/4Q5DZZWJ/Bai et al. (2022) - Globally Inaccurate Stereotypes Can Result From Locally Adaptive Exploration.pdf}
}

@article{bartoLookingBackActor2021,
  title = {Looking {{Back}} on the {{Actor}}--{{Critic Architecture}}},
  author = {Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
  year = {2021},
  month = jan,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  volume = {51},
  number = {1},
  pages = {40--50},
  issn = {2168-2232},
  doi = {10.1109/TSMC.2020.3041775},
  urldate = {2024-10-29},
  abstract = {This retrospective describes the overall research project that gave rise to the authors' paper ``Neuronlike adaptive elements that can solve difficult learning control problems'' that was published in the 1983 Neural and Sensory Information Processing special issue of the IEEE Transactions on Systems, Man, and Cybernetics. This look back explains how this project came about, presents the ideas and previous publications that influenced it, and describes our most closely related subsequent research. It concludes by pointing out some noteworthy aspects of this article that have been eclipsed by its main contributions, followed by commenting on some of the directions and cautions that should inform future research.},
  keywords = {Actor-critic,Animals,Computer architecture,Context modeling,hedonistic neuron,Neurons,pole blancing,Psychology,reinforcement learning (RL),Supervised learning,Synapses,temporal-difference learning},
  file = {/Users/paultalma/Zotero/storage/VGTPNNRH/Barto et al. - 2021 - Looking Back on the Actor–Critic Architecture.pdf;/Users/paultalma/Zotero/storage/Q2QYJA9S/9306925.html}
}

@article{bartoNeuronlikeAdaptiveElements1983,
  title = {Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems},
  author = {Barto, Andrew G. and Sutton, Richard S. and Anderson, Charles W.},
  year = {1983},
  month = sep,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {SMC-13},
  number = {5},
  pages = {834--846},
  issn = {2168-2909},
  doi = {10.1109/TSMC.1983.6313077},
  urldate = {2024-10-29},
  abstract = {It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences.},
  keywords = {Adaptive systems,Biological neural networks,Neurons,Pattern recognition,Problem-solving,Supervised learning,Training},
  file = {/Users/paultalma/Zotero/storage/ZHVIMQSL/Barto et al. - 1983 - Neuronlike adaptive elements that can solve difficult learning control problems.pdf;/Users/paultalma/Zotero/storage/2I3AXADQ/6313077.html}
}

@article{battermanMinimalModelExplanations2014a,
  title = {Minimal {{Model Explanations}}},
  author = {Batterman, Robert W. and Rice, Collin C.},
  year = {2014},
  month = jul,
  journal = {Philosophy of Science},
  volume = {81},
  number = {3},
  pages = {349--376},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/676677},
  urldate = {2025-01-16},
  abstract = {This article discusses minimal model explanations, which we argue are distinct from various causal, mechanical, difference-making, and so on, strategies prominent in the philosophical literature. We contend that what accounts for the explanatory power of these models is not that they have certain features in common with real systems. Rather, the models are explanatory because of a story about why a class of systems will all display the same large-scale behavior because the details that distinguish them are irrelevant. This story explains patterns across extremely diverse systems and shows how minimal models can be used to understand real systems.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/QEURRWAJ/Batterman and Rice (2014) - Minimal Model Explanations.pdf}
}

@article{beinSchemasReinforcementLearning2025,
  title = {Schemas, Reinforcement Learning and the Medial Prefrontal Cortex},
  author = {Bein, Oded and Niv, Yael},
  year = {2025},
  month = jan,
  journal = {Nature Reviews Neuroscience},
  pages = {1--17},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/s41583-024-00893-z},
  urldate = {2025-01-28},
  abstract = {Schemas are rich and complex knowledge structures about the typical unfolding of events in a context; for example, a schema of a dinner at a restaurant. In this Perspective, we suggest that reinforcement learning (RL), a computational theory of learning the structure of the world and relevant goal-oriented behaviour, underlies schema learning. We synthesize literature about schemas and RL to offer that three RL principles might govern the learning of schemas: learning via prediction errors, constructing hierarchical knowledge using hierarchical RL, and dimensionality reduction through learning a simplified and abstract representation of the world. We then suggest that the orbitomedial prefrontal cortex is involved in both schemas and RL due to its involvement in dimensionality reduction and in guiding memory reactivation through interactions with posterior brain regions. Last, we hypothesize that the amount of dimensionality reduction might underlie gradients of involvement along the ventral--dorsal and posterior--anterior axes of the orbitomedial prefrontal cortex. More specific and detailed representations might engage the ventral and posterior parts, whereas abstraction might shift representations towards the dorsal and anterior parts of the medial prefrontal cortex.},
  copyright = {2025 Springer Nature Limited},
  langid = {english},
  keywords = {Cognitive neuroscience,Human behaviour,Learning algorithms}
}

@article{beinSchemasReinforcementLearning2025a,
  title = {Schemas, Reinforcement Learning and the Medial Prefrontal Cortex},
  author = {Bein, Oded and Niv, Yael},
  year = {2025},
  month = jan,
  journal = {Nature Reviews Neuroscience},
  pages = {1--17},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/s41583-024-00893-z},
  urldate = {2025-02-03},
  abstract = {Schemas are rich and complex knowledge structures about the typical unfolding of events in a context; for example, a schema of a dinner at a restaurant. In this Perspective, we suggest that reinforcement learning (RL), a computational theory of learning the structure of the world and relevant goal-oriented behaviour, underlies schema learning. We synthesize literature about schemas and RL to offer that three RL principles might govern the learning of schemas: learning via prediction errors, constructing hierarchical knowledge using hierarchical RL, and dimensionality reduction through learning a simplified and abstract representation of the world. We then suggest that the orbitomedial prefrontal cortex is involved in both schemas and RL due to its involvement in dimensionality reduction and in guiding memory reactivation through interactions with posterior brain regions. Last, we hypothesize that the amount of dimensionality reduction might underlie gradients of involvement along the ventral--dorsal and posterior--anterior axes of the orbitomedial prefrontal cortex. More specific and detailed representations might engage the ventral and posterior parts, whereas abstraction might shift representations towards the dorsal and anterior parts of the medial prefrontal cortex.},
  copyright = {2025 Springer Nature Limited},
  langid = {english},
  keywords = {Cognitive neuroscience,Human behaviour,Learning algorithms},
  file = {/Users/paultalma/Zotero/storage/UAZ79JVM/Bein and Niv (2025) - Schemas, reinforcement learning and the medial prefrontal cortex.pdf}
}

@article{belkinReconcilingModernMachinelearning2019,
  title = {Reconciling Modern Machine-Learning Practice and the Classical Bias--Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year = {2019},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {32},
  pages = {15849--15854},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1903070116},
  urldate = {2025-01-17},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias--variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias--variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This ``double-descent'' curve subsumes the textbook U-shaped bias--variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
  file = {/Users/paultalma/Zotero/storage/PG944I6I/Belkin et al. (2019) - Reconciling modern machine-learning practice and the classical bias–variance trade-off.pdf}
}

@article{benacerrafWhatNumbersCould1965,
  title = {What {{Numbers Could Not Be}}},
  author = {Benacerraf, Paul},
  year = {1965},
  journal = {Philosophical Review},
  volume = {74},
  number = {1},
  pages = {47--73},
  publisher = {Duke University Press},
  doi = {10.2307/2183530}
}

@misc{bennettValuefreeReinforcementLearning2021,
  title = {Value-Free Reinforcement Learning: {{Policy}} Optimization as a Minimal Model of Operant Behavior},
  shorttitle = {Value-Free Reinforcement Learning},
  author = {Bennett, Daniel and Niv, Yael and Langdon, Angela},
  year = {2021},
  month = feb,
  publisher = {OSF},
  doi = {10.31234/osf.io/ew58m},
  urldate = {2025-01-28},
  abstract = {Reinforcement learning is a powerful framework for modelling the cognitive and neural substrates of learning and decision making. Contemporary research in cognitive neuroscience and neuroeconomics typically uses value-based reinforcement-learning models, which assume that decision-makers choose by comparing learned values for different actions. However, another possibility is suggested by a simpler family of models, called policy-gradient reinforcement learning. Policy-gradient models learn by optimizing a behavioral policy directly, without the intermediate step of value-learning. Here we review recent behavioral and neural findings that are more parsimoniously explained by policy-gradient models than by value-based models. We conclude that, despite the ubiquity of `value' in reinforcement-learning models of decision making, policy-gradient models provide a lightweight and compelling alternative model of operant behavior.},
  archiveprefix = {OSF},
  langid = {american},
  file = {/Users/paultalma/Zotero/storage/V8QUWV84/Bennett et al. (2021) - Value-free reinforcement learning Policy optimization as a minimal model of operant behavior.pdf}
}

@article{blockInvertedEarth1990,
  title = {Inverted {{Earth}}},
  author = {Block, Ned},
  year = {1990},
  journal = {Philosophical Perspectives},
  volume = {4},
  eprint = {2214187},
  eprinttype = {jstor},
  pages = {53--79},
  publisher = {[Ridgeview Publishing Company, Wiley]},
  issn = {1520-8583},
  doi = {10.2307/2214187},
  urldate = {2024-12-20}
}

@incollection{boolosHumesPrincipleAnalytic1997,
  title = {Is {{Hume}}'s {{Principle Analytic}}?},
  booktitle = {Language, Thought, and Logic: Essays in Honour of {{Michael Dummett}}},
  author = {Boolos, George},
  editor = {Heck, Richard G.},
  year = {1997},
  publisher = {Oxford University Press}
}

@article{borgerAbstractStateMachines2005,
  title = {Abstract {{State Machines}}: A Unifying View of Models of Computation and of System Design Frameworks},
  shorttitle = {Abstract {{State Machines}}},
  author = {B{\"o}rger, Egon},
  year = {2005},
  month = may,
  journal = {Annals of Pure and Applied Logic},
  series = {Festschrift on the Occasion of {{Helmut Schwichtenberg}}'s 60th Birthday},
  volume = {133},
  number = {1},
  pages = {149--171},
  issn = {0168-0072},
  doi = {10.1016/j.apal.2004.10.007},
  urldate = {2025-01-30},
  abstract = {We capture the principal models of computation and specification in the literature by a uniform set of transparent mathematical descriptions which---starting from scratch---provide the conceptual basis for a comparative study.11Helmut Schwichtenberg gewidmet, dem Freund und Kollegen seit der gemeinsamen Studienzeit in M{\"u}nster.},
  file = {/Users/paultalma/Zotero/storage/RRQSFK3E/S0168007204001320.html}
}

@article{botvinickHierarchicallyOrganizedBehavior2009,
  title = {Hierarchically Organized Behavior and Its Neural Foundations: {{A}} Reinforcement Learning Perspective},
  shorttitle = {Hierarchically Organized Behavior and Its Neural Foundations},
  author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andew G.},
  year = {2009},
  month = dec,
  journal = {Cognition},
  series = {Reinforcement Learning and Higher Cognition},
  volume = {113},
  number = {3},
  pages = {262--280},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2008.08.011},
  urldate = {2025-01-28},
  abstract = {Research on human and animal behavior has long emphasized its hierarchical structure---the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
  keywords = {Prefrontal cortex,Reinforcement learning},
  file = {/Users/paultalma/Zotero/storage/U6BTPY5D/Botvinick et al. (2009) - Hierarchically organized behavior and its neural foundations A reinforcement learning perspective.pdf;/Users/paultalma/Zotero/storage/D727TM2T/S0010027708002059.html}
}

@article{botvinickHierarchicallyOrganizedBehavior2009a,
  title = {Hierarchically Organized Behavior and Its Neural Foundations: {{A}} Reinforcement Learning Perspective},
  shorttitle = {Hierarchically Organized Behavior and Its Neural Foundations},
  author = {Botvinick, Matthew M. and Niv, Yael and Barto, Andew G.},
  year = {2009},
  month = dec,
  journal = {Cognition},
  series = {Reinforcement Learning and Higher Cognition},
  volume = {113},
  number = {3},
  pages = {262--280},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2008.08.011},
  urldate = {2025-01-28},
  abstract = {Research on human and animal behavior has long emphasized its hierarchical structure---the divisibility of ongoing behavior into discrete tasks, which are comprised of subtask sequences, which in turn are built of simple actions. The hierarchical structure of behavior has also been of enduring interest within neuroscience, where it has been widely considered to reflect prefrontal cortical functions. In this paper, we reexamine behavioral hierarchy and its neural substrates from the point of view of recent developments in computational reinforcement learning. Specifically, we consider a set of approaches known collectively as hierarchical reinforcement learning, which extend the reinforcement learning paradigm by allowing the learning agent to aggregate actions into reusable subroutines or skills. A close look at the components of hierarchical reinforcement learning suggests how they might map onto neural structures, in particular regions within the dorsolateral and orbital prefrontal cortex. It also suggests specific ways in which hierarchical reinforcement learning might provide a complement to existing psychological models of hierarchically structured behavior. A particularly important question that hierarchical reinforcement learning brings to the fore is that of how learning identifies new action routines that are likely to provide useful building blocks in solving a wide range of future problems. Here and at many other points, hierarchical reinforcement learning offers an appealing framework for investigating the computational and neural underpinnings of hierarchically structured behavior.},
  keywords = {Prefrontal cortex,Reinforcement learning},
  file = {/Users/paultalma/Zotero/storage/P95DXF6C/hierarchical RL.pdf;/Users/paultalma/Zotero/storage/6Y8GZQ4P/S0010027708002059.html}
}

@misc{bowersSuccessesFailuresArtificial2024,
  title = {The Successes and Failures of {{Artificial Neural Networks}} ({{ANNs}}) Highlight the Importance of Innate Linguistic Priors for Human Language Acquisition},
  author = {Bowers, Jeffrey S.},
  year = {2024},
  month = nov,
  publisher = {OSF},
  doi = {10.31234/osf.io/5wt9m},
  urldate = {2024-11-11},
  abstract = {Artificial Neural Networks (ANNs) equipped with general learning algorithms, but no linguistic knowledge, can learn to associate words with objects in naturalistic scenes when trained on head-mounted video recordings from a single child's first-person experience. Similarly, ANNs can master syntax when trained on a similar amount of linguistic data a child experiences in a few years. These findings have been taken to challenge the view that innate linguistic priors play a role in child language acquisition. Here I show that the training environments and learning resources of ANN and humans are poorly matched, and accordingly, conclusions regarding human language priors are not merited. I also review three sets of findings that strongly suggest ANNs are missing human inductive biases: (1) children (but not ANNs) create new well-structured languages when only exposed to degraded ones; (2) ANN (but not humans) learn impossible and possible human languages in similar ways with similar facility; and (3) humans (but not ANNs) show a critical period for language learning. In this last case, adding an ``innate'' inductive prior to the ANN results in better ANN-human alignment. Just as is the case regarding claims of ANN-human alignment in the domain of vision, conclusions regarding ANN-human alignment in the domain of language is characterized by a lack of severe testing of hypotheses.},
  archiveprefix = {OSF},
  langid = {american},
  file = {/Users/paultalma/Zotero/storage/XS3CMSJX/Bowers (2024) - The successes and failures of Artificial Neural Networks (ANNs) highlight the importance of innate l.pdf}
}

@article{bowlingSettlingRewardHypothesis2023,
  title = {Settling the {{Reward Hypothesis}}},
  author = {Bowling, Michael and Martin, John D and Abel, David and Dabney, Will},
  year = {2023},
  abstract = {The reward hypothesis posits that, ``all of what we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).'' We aim to fully settle this hypothesis. This will not conclude with a simple affirmation or refutation, but rather specify completely the implicit requirements on goals and purposes under which the hypothesis holds.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/LBWME3MC/Bowling et al. Settling the Reward Hypothesis.pdf}
}

@article{brendanritchieRecognizingWhyVision2022,
  title = {Recognizing Why Vision Is Inferential},
  author = {Brendan Ritchie, J.},
  year = {2022},
  month = feb,
  journal = {Synthese},
  volume = {200},
  number = {1},
  pages = {25},
  issn = {1573-0964},
  doi = {10.1007/s11229-022-03508-1},
  urldate = {2025-01-06},
  abstract = {A theoretical pillars of vision science in the information-processing tradition is that perception involves unconscious inference. The classic support for this claim is that, since retinal inputs underdetermine their distal causes, visual perception must be the conclusion of a process that starts with premises representing both the sensory input and previous knowledge about the visible world. Focus on this ``argument from underdetermination'' gives the impression that, if it fails, there is little reason to think that visual processing involves unconscious inference. Here an alternative means of support for this pillar is proposed, based on another foundational challenge for the visual system: recognizing invariant properties of objects in the environment even though anything we encounter is never seen exactly the same way twice. Explaining how the visual system solves this invariance problem requires positing visual processes that exhibit many commonalities with inductive inference. Thus, this novel ``argument from invariance'' reveals one way in which visual processing clearly involves unconscious inference.},
  langid = {english},
  keywords = {Inference,Mental representation,Object recognition,Vision}
}

@article{brownHumansAdaptivelyResolve2022,
  title = {Humans Adaptively Resolve the Explore-Exploit Dilemma under Cognitive Constraints: {{Evidence}} from a Multi-Armed Bandit Task},
  shorttitle = {Humans Adaptively Resolve the Explore-Exploit Dilemma under Cognitive Constraints},
  author = {Brown, Vanessa M. and Hallquist, Michael N. and Frank, Michael J. and Dombrovski, Alexandre Y.},
  year = {2022},
  month = dec,
  journal = {Cognition},
  volume = {229},
  pages = {105233},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2022.105233},
  urldate = {2025-01-06},
  abstract = {When navigating uncertain worlds, humans must balance exploring new options versus exploiting known rewards. Longer horizons and spatially structured option values encourage humans to explore, but the impact of real-world cognitive constraints such as environment size and memory demands on explore-exploit decisions is unclear. In the present study, humans chose between options varying in uncertainty during a multi-armed bandit task with varying environment size and memory demands. Regression and cognitive computational models of choice behavior showed that with a lower cognitive load, humans are more exploratory than a simulated value-maximizing learner, but under cognitive constraints, they adaptively scale down exploration to maintain exploitation. Thus, while humans are curious, cognitive constraints force people to decrease their strategic exploration in a resource-rational-like manner to focus on harvesting known rewards.},
  keywords = {Cognitive constraints,Exploitation,Exploration,Learning},
  file = {/Users/paultalma/Zotero/storage/NABBZ7WD/S0010027722002219.html}
}

@article{burgeIndividualismMental1979,
  title = {Individualism and the {{Mental}}},
  author = {Burge, Tyler},
  year = {1979},
  journal = {Midwest Studies in Philosophy},
  volume = {4},
  number = {1},
  pages = {73--122},
  publisher = {Oxford University Press: Oxford},
  doi = {10.1111/j.1475-4975.1979.tb00374.x}
}

@incollection{burgeIntroduction2005,
  title = {Introduction},
  booktitle = {Truth, {{Thought}}, {{Reason}}: {{Essays}} on {{Frege}}},
  author = {Burge, Tyler},
  year = {2005},
  publisher = {Oxford University Press},
  address = {New York}
}

@book{burgeTruthThoughtReason2005,
  title = {Truth, {{Thought}}, {{Reason}}: {{Essays}} on {{Frege}}},
  author = {Burge, Tyler},
  year = {2005},
  publisher = {Oxford University Press},
  address = {New York}
}

@book{burnhamModelSelectionMultimodel2004,
  title = {Model {{Selection}} and {{Multimodel Inference}}},
  editor = {Burnham, Kenneth P. and Anderson, David R.},
  year = {2004},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/b97636},
  urldate = {2024-11-30},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-0-387-95364-9},
  langid = {english},
  keywords = {data analysis,Estimator,Inference,information theory,Likelihood,Model Selection},
  file = {/Users/paultalma/Zotero/storage/G6CHML2W/Burnham and Anderson (2004) - Model Selection and Multimodel Inference.pdf}
}

@article{butlinMachineLearningFunctions2022,
  title = {Machine {{Learning}}, {{Functions}} and {{Goals}}},
  author = {Butlin, Patrick},
  year = {2022},
  month = dec,
  journal = {Croatian Journal of Philosophy},
  volume = {22},
  number = {66},
  pages = {351--370},
  publisher = {Institut za filozofiju},
  issn = {1333-1108, 1847-6139},
  doi = {10.52685/cjp.22.66.5},
  urldate = {2025-01-28},
  abstract = {Machine learning researchers distinguish between reinforcement learning and supervised learning and refer to reinforcement learning systems as ``agents''. This paper vindicates the claim that systems trained by reinforcement learning are agents while t...},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/8S94CP73/Butlin (2022) - Machine Learning, Functions and Goals.pdf}
}

@article{butlinReinforcementLearningArtificial2024,
  title = {Reinforcement Learning and Artificial Agency},
  author = {Butlin, Patrick},
  year = {2024},
  journal = {Mind \& Language},
  volume = {39},
  number = {1},
  pages = {22--38},
  issn = {1468-0017},
  doi = {10.1111/mila.12458},
  urldate = {2025-01-28},
  abstract = {There is an apparent connection between reinforcement learning and agency. Artificial entities controlled by reinforcement learning algorithms are standardly referred to as agents, and the mainstream view in the psychology and neuroscience of agency is that humans and other animals are reinforcement learners. This article examines this connection, focusing on artificial reinforcement learning systems and assuming that there are various forms of agency. Artificial reinforcement learning systems satisfy plausible conditions for minimal agency, and those which use models of the environment to perform forward search are capable of a form of agency which may reasonably be called action for reasons.},
  copyright = {{\copyright} 2023 The Author. Mind \& Language published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {action for reasons,agency,artificial intelligence,minimal agency,reinforcement learning},
  file = {/Users/paultalma/Zotero/storage/3PXDAEV5/Butlin (2024) - Reinforcement learning and artificial agency.pdf}
}

@incollection{buttonMathematicalInternalRealism2022,
  title = {Mathematical {{Internal Realism}}},
  booktitle = {Engaging {{Putnam}}},
  author = {Button, Tim},
  editor = {Chakraborty, Sanjit and Conant, James Ferguson},
  year = {2022},
  pages = {157--182},
  publisher = {De Gruyter}
}

@book{buttonPhilosophyModelTheory2018,
  title = {Philosophy and {{Model Theory}}},
  author = {Button, Tim and Walsh, Sean},
  editor = {Walsh, Sean and Hodges, Wilfrid},
  year = {2018},
  publisher = {Oxford University Press},
  address = {Oxford, UK}
}

@article{buttonStructureCategoricityDeterminacy2016,
  title = {Structure and {{Categoricity}}: {{Determinacy}} of {{Reference}} and {{Truth Value}} in the {{Philosophy}} of {{Mathematics}}},
  author = {Button, Tim and Walsh, Sean},
  year = {2016},
  journal = {Philosophia Mathematica},
  volume = {24},
  number = {3},
  pages = {283--307},
  doi = {10.1093/philmat/nkw007}
}

@article{callawayRationalUseCognitive2022,
  title = {Rational Use of Cognitive Resources in Human Planning},
  author = {Callaway, Frederick and Van Opheusden, Bas and Gul, Sayan and Das, Priyam and Krueger, Paul M. and Griffiths, Thomas L. and Lieder, Falk},
  year = {2022},
  month = apr,
  journal = {Nature Human Behaviour},
  volume = {6},
  number = {8},
  pages = {1112--1125},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01332-8},
  urldate = {2025-01-28},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/XIK5KN9M/Callaway et al. (2022) - Rational use of cognitive resources in human planning.pdf}
}

@article{callawayResourcerationalAnalysisHuman,
  title = {A Resource-Rational Analysis of Human Planning},
  author = {Callaway, Frederick and Lieder, Falk and Das, Priyam and Gul, Sayan and Krueger, Paul M and Griffiths, Thomas L},
  abstract = {People's cognitive strategies are jointly shaped by function and computational constraints. Resource-rational analysis leverages these constraints to derive rational models of people's cognitive strategies from the assumption that people make rational use of limited cognitive resources. We present a resource-rational analysis of planning and evaluate its predictions in a newly developed process tracing paradigm. In Experiment 1, we find that a resource-rational planning strategy predicts the process by which people plan more accurately than previous models of planning. Furthermore, in Experiment 2, we find that it also captures how people's planning strategies adapt to the structure of the environment. In addition, our approach allows us to quantify for the first time how close people's planning strategies are to being resource-rational and to characterize in which ways they conform to and deviate from optimal planning.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/G2JTLGGG/Callaway et al. A resource-rational analysis of human planning.pdf}
}

@article{changConditioningDisintegration1997,
  title = {Conditioning as Disintegration},
  author = {Chang, J. T. and Pollard, D.},
  year = {1997},
  month = nov,
  journal = {Statistica Neerlandica},
  volume = {51},
  number = {3},
  pages = {287--317},
  issn = {0039-0402, 1467-9574},
  doi = {10.1111/1467-9574.00056},
  urldate = {2025-01-22},
  abstract = {Conditional probability distributions seem to have a bad reputation when it comes to rigorous treatment of conditioning. Technical arguments are published as manipulations of Radon--Nikodym derivatives, although we all secretly perform heuristic calculations using elementary definitions of conditional probabilities. In print, measurability and averaging properties substitute for intuitive ideas about random variables behaving like constants given particular conditioning information.             One way to engage in rigorous, guilt-free manipulation of conditional distributions is to treat them as disintegrating measures---families of probability measures concentrating on the level sets of a conditioning statistic. In this paper we present a little theory and a range of examples---from EM algorithms and the Neyman factorization, through Bayes theory and marginalization paradoxes---to suggest that disintegrations have both intuitive appeal and the rigor needed for many problems in mathematical statistics.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/A2HKHYBI/Chang and Pollard (1997) - Conditioning as disintegration.pdf}
}

@article{chelazziAlteringSpatialPriority2014,
  title = {Altering {{Spatial Priority Maps}} via {{Reward-Based Learning}}},
  author = {Chelazzi, Leonardo and E{\v s}to{\v c}inov{\'a}, Jana and Calletti, Riccardo and Gerfo, Emanuele Lo and Sani, Ilaria and Libera, Chiara Della and Santandrea, Elisa},
  year = {2014},
  month = jun,
  journal = {Journal of Neuroscience},
  volume = {34},
  number = {25},
  pages = {8594--8604},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.0277-14.2014},
  urldate = {2025-01-28},
  abstract = {Spatial priority maps are real-time representations of the behavioral salience of locations in the visual field, resulting from the combined influence of stimulus driven activity and top-down signals related to the current goals of the individual. They arbitrate which of a number of (potential) targets in the visual scene will win the competition for attentional resources. As a result, deployment of visual attention to a specific spatial location is determined by the current peak of activation (corresponding to the highest behavioral salience) across the map. Here we report a behavioral study performed on healthy human volunteers, where we demonstrate that spatial priority maps can be shaped via reward-based learning, reflecting long-lasting alterations (biases) in the behavioral salience of specific spatial locations. These biases exert an especially strong influence on performance under conditions where multiple potential targets compete for selection, conferring competitive advantage to targets presented in spatial locations associated with greater reward during learning relative to targets presented in locations associated with lesser reward. Such acquired biases of spatial attention are persistent, are nonstrategic in nature, and generalize across stimuli and task contexts. These results suggest that reward-based attentional learning can induce plastic changes in spatial priority maps, endowing these representations with the ``intelligent'' capacity to learn from experience.},
  chapter = {Articles},
  copyright = {Copyright {\copyright} 2014 the authors 0270-6474/14/348594-11\$15.00/0},
  langid = {english},
  pmid = {24948813},
  keywords = {cross-target competition,reward-based learning,spatial attention,spatial priority maps},
  file = {/Users/paultalma/Zotero/storage/4QQBRXVF/Chelazzi et al. (2014) - Altering Spatial Priority Maps via Reward-Based Learning.pdf}
}

@book{chengExpectedExperiencesPredictive2023,
  title = {Expected {{Experiences}}: {{The Predictive Mind}} in an {{Uncertain World}}},
  shorttitle = {Expected {{Experiences}}},
  author = {Cheng, Tony and Sato, Ryoji and Hohwy, Jakob},
  year = {2023},
  month = dec,
  edition = {1},
  publisher = {Routledge},
  address = {New York},
  doi = {10.4324/9781003084082},
  urldate = {2024-11-07},
  isbn = {978-1-00-308408-2},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/MGKAM6VI/Cheng et al. (2023) - Expected Experiences The Predictive Mind in an Uncertain World.pdf}
}

@incollection{chipmanPracticalImplementationBayesian2001,
  title = {The {{Practical Implementation}} of {{Bayesian Model Selection}}},
  booktitle = {Institute of {{Mathematical Statistics Lecture Notes}} - {{Monograph Series}}},
  author = {Chipman, Hugh and George, Edward I. and McCulloch, Robert E.},
  year = {2001},
  pages = {65--116},
  publisher = {Institute of Mathematical Statistics},
  address = {Beachwood, OH},
  doi = {10.1214/lnms/1215540964},
  urldate = {2024-11-22},
  abstract = {In principle, the Bayesian approach to model selection is straightforward. Prior probability distributions are used to describe the uncertainty surrounding all unknowns. After observing the data, the posterior distribution provides a coherent post data summary of the remaining uncertainty which is relevant for model selection. However, the practical implementation of this approach often requires carefully tailored priors and novel posterior calculation methods. In this article, we illustrate some of the fundamental practical issues that arise for two different model selection problems: the variable selection problem for the linear model and the CART model selection problem.},
  isbn = {978-0-940600-52-2},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/YH78DUI9/Chipman et al. (2001) - The Practical Implementation of Bayesian Model Selection.pdf}
}

@article{cifarelliFinettisContributionProbability1996,
  title = {De {{Finetti}}'s {{Contribution}} to {{Probability}} and {{Statistics}}},
  author = {Cifarelli, Donato Michele and Regazzini, Eugenio},
  year = {1996},
  journal = {Statistical Science},
  volume = {11},
  number = {4},
  eprint = {2246020},
  eprinttype = {jstor},
  pages = {253--282},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237},
  urldate = {2024-11-04},
  abstract = {This paper summarizes the scientific activity of de Finetti in probability and statistics. It falls into three sections: Section 1 includes an essential biography of de Finetti and a survey of the basic features of the scientific milieu in which he took the first steps of his scientific career; Section 2 concerns de Finetti's work in probability: (a) foundations, (b) processes with independent increments, (c) sequences of exchangeable random variables, and (d) contributions which fall within other fields; Section 3 deals with de Finetti's contributions to statistics: (a) description of frequency distributions, (b) induction and statistics, (c) probability and induction, and (d) objectivistic schools and theory of decision. Many recent developments of de Finetti's work are mentioned here and briefly described.},
  file = {/Users/paultalma/Zotero/storage/F8Q6N77K/Cifarelli and Regazzini (1996) - De Finetti's Contribution to Probability and Statistics.pdf}
}

@article{coverNearestNeighborPattern1967,
  title = {Nearest Neighbor Pattern Classification},
  author = {Cover, T. and Hart, P.},
  year = {1967},
  month = jan,
  journal = {IEEE Transactions on Information Theory},
  volume = {13},
  number = {1},
  pages = {21--27},
  issn = {1557-9654},
  doi = {10.1109/TIT.1967.1053964},
  urldate = {2025-01-17},
  abstract = {The nearest neighbor decision rule assigns to an unclassified sample point the classification of the nearest of a set of previously classified points. This rule is independent of the underlying joint distribution on the sample points and their classifications, and hence the probability of errorRof such a rule must be at least as great as the Bayes probability of errorR{\textasciicircum}{\textbackslash}ast--the minimum probability of error over all decision rules taking underlying probability structure into account. However, in a large sample analysis, we will show in theM-category case thatR{\textasciicircum}{\textbackslash}ast {\l}eq R {\l}eq R{\textasciicircum}{\textbackslash}ast(2 --MR{\textasciicircum}{\textbackslash}ast/(M-1)), where these bounds are the tightest possible, for all suitably smooth underlying distributions. Thus for any number of categories, the probability of error of the nearest neighbor rule is bounded above by twice the Bayes probability of error. In this sense, it may be said that half the classification information in an infinite sample set is contained in the nearest neighbor.},
  file = {/Users/paultalma/Zotero/storage/3HZA246A/1053964.html}
}

@article{creelTransparencyComplexComputational2020,
  title = {Transparency in {{Complex Computational Systems}}},
  author = {Creel, Kathleen A.},
  year = {2020},
  month = oct,
  journal = {Philosophy of Science},
  volume = {87},
  number = {4},
  pages = {568--589},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/709729},
  urldate = {2025-01-21},
  abstract = {Scientists depend on complex computational systems that are often ineliminably opaque, to the detriment of our ability to give scientific explanations and detect artifacts. Some philosophers have suggested treating opaque systems instrumentally, but computer scientists developing strategies for increasing transparency are correct in finding this unsatisfying. Instead, I propose an analysis of transparency as having three forms: transparency of the algorithm, the realization of the algorithm in code, and the way that code is run on particular hardware and data. This targets the transparency most useful for a task, avoiding instrumentalism by providing partial transparency when full transparency is impossible.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/CT6ML26N/Creel (2020) - Transparency in Complex Computational Systems.pdf}
}

@article{crupiCritiquePureBayesian2023,
  title = {Critique of Pure {{Bayesian}} Cognitive Science: {{A}} View from the Philosophy of Science},
  shorttitle = {Critique of Pure {{Bayesian}} Cognitive Science},
  author = {Crupi, Vincenzo and Calzavarini, Fabrizio},
  year = {2023},
  month = jun,
  journal = {European Journal for Philosophy of Science},
  volume = {13},
  number = {3},
  pages = {28},
  issn = {1879-4920},
  doi = {10.1007/s13194-023-00533-w},
  urldate = {2025-01-06},
  abstract = {Bayesian approaches to human cognition have been extensively advocated in the last decades, but sharp objections have been raised too within cognitive science. In this paper, we outline a diagnosis of what has gone wrong with the prevalent strand of Bayesian cognitive science (here labelled pure Bayesian cognitive science), relying on selected illustrations from the psychology of reasoning and tools from the philosophy of science. Bayesians' reliance on so-called method of rational analysis is a key point of our discussion. We tentatively conclude on a constructive note, though: an appropriately modified variant of Bayesian cognitive science can still be coherently pursued, as some scholars have noted.},
  langid = {english},
  keywords = {Bayesian cognitive science,Is-ought,Predictivism,Rational analysis},
  file = {/Users/paultalma/Zotero/storage/F36E48DG/Crupi and Calzavarini (2023) - Critique of pure Bayesian cognitive science A view from the philosophy of science.pdf}
}

@article{davidsonComplexStructuredGoals2024,
  title = {Toward {{Complex}} and {{Structured Goals}} in {{Rein-}} Forcement {{Learning}}},
  author = {Davidson, Guy and Gureckis, Todd M},
  year = {2024},
  abstract = {Goals play a central role in the study of agentic behavior. But what is a goal, and how should we best represent them? The traditional reinforcement learning answer is that all goals are expressible as the maximization of future rewards. While parsimonious, such a definition seems insufficient when viewed from both the perspective of humans specifying goals to machines and autotelic agents that self-propose tasks to explore and learn. We offer a critical perspective on the distillation of all goals directly into reward functions. We identify key features we believe goal representations ought to have, and then offer a proposal we believe meets those considerations.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/456UHRM5/Davidson and Gureckis (2024) - Toward Complex and Structured Goals in Rein- forcement Learning.pdf}
}

@article{davidsonGoalsRewardProducingPrograms,
  title = {Goals as {{Reward-Producing Programs}}},
  author = {Davidson, Guy and Todd, Graham and Togelius, Julian and Gureckis, Todd M and Lake, Brenden M},
  abstract = {People are remarkably capable of generating their own goals, beginning with child's play and continuing into adulthood. Despite considerable empirical and computational work on goals and goal-oriented behavior, models are still far from capturing the richness of everyday human goals. Here, we bridge this gap by collecting a dataset of human-generated playful goals, modeling them as reward-producing programs, and generating novel human-like goals through program synthesis. Reward-producing programs capture the rich semantics of goals through symbolic operations that compose, add temporal constraints, and allow for program execution on behavioral traces to evaluate progress. To build a generative model of goals, we learn a fitness function over the infinite set of possible goal programs and sample novel goals with a quality-diversity algorithm. Human evaluators found model generations, when constrained to originate from partitions of program space occupied by human examples, indistinguishable from human-created games. We also discovered that our model's internal fitness scores predict games that are evaluated as more fun to play and more human-like.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/RH5W8E3S/Davidson et al. Goals as Reward-Producing Programs.pdf}
}

@article{davidsonTruthMeaning1967,
  title = {Truth and {{Meaning}}},
  author = {Davidson, Donald},
  year = {1967},
  journal = {Synthese},
  volume = {17},
  number = {1},
  pages = {304--323},
  publisher = {Oxford University Press},
  doi = {10.1007/bf00485035}
}

@article{dawActionsPoliciesValues2005,
  title = {Actions, {{Policies}}, {{Values}}, and the {{Basal Ganglia}}},
  author = {Daw, Nathaniel D and Niv, Yael and Dayan, Peter},
  year = {2005},
  abstract = {The basal ganglia are widely believed to be involved in the learned selection of actions. Building on this idea, reinforcement learning (RL) theories of optimal control have had some success in explaining the responses of their key dopaminergic afferents. While these model-free RL theories offer a compelling account of a range of neurophysiological and behavioural data, they offer only an incomplete picture of action control in the brain. Psychologists and behavioural neuroscientists have long appealed to the existence of at least two separate control systems underlying the learned control of behaviour. The habit system is closely identified with the basal ganglia, and we associate it with the model-free RL theories. The other system, more loosely localised in prefrontal regions and without such a detailed theoretical account, is associated with cognitively more sophisticated goal-directed actions. On the critical issue of which system determines the ultimate output when they disagree, there is a wide range of experimental results and sparse theoretical underpinning.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/7TRUBC6Q/Daw et al. Actions, Policies, Values, and the Basal Ganglia.pdf}
}

@article{dayanFeudalReinforcementLearning1992,
  title = {Feudal Reinforcement Learning},
  author = {Dayan, Peter and Hinton, Geoffrey E.},
  year = {1992},
  journal = {Advances in neural information processing systems},
  volume = {5},
  urldate = {2025-01-28},
  file = {/Users/paultalma/Zotero/storage/Q9T55EHQ/Dayan and Hinton (1992) - Feudal reinforcement learning.pdf}
}

@inproceedings{dayanFeudalReinforcementLearning1992a,
  title = {Feudal {{Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dayan, Peter and Hinton, Geoffrey E},
  year = {1992},
  volume = {5},
  publisher = {Morgan-Kaufmann},
  urldate = {2025-01-28},
  abstract = {One way to speed up reinforcement learning is to enable learning to  happen simultaneously at multiple resolutions in space and time.  This paper shows how to create a Q-Iearning managerial hierarchy  in which high level managers learn how to set tasks to their sub(cid:173) managers who, in turn, learn how to satisfy them.  Sub-managers  need  not initially understand  their managers' commands.  They  simply learn to maximise their reinforcement in the context of the  current command.  We illustrate the system using a simple maze task ..  As the system  learns  how to get around,  satisfying commands at the multiple  levels, it explores more efficiently than standard, flat,  Q-Iearning  and builds a more comprehensive map.},
  file = {/Users/paultalma/Zotero/storage/VL4SPV6W/Dayan and Hinton (1992) - Feudal Reinforcement Learning.pdf}
}

@article{dayanHelmholtzMachine1995,
  title = {The Helmholtz Machine},
  author = {Dayan, Peter and Hinton, Geoffrey E. and Neal, Radford M. and Zemel, Richard S.},
  year = {1995},
  journal = {Neural computation},
  volume = {7},
  number = {5},
  pages = {889--904},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info {\dots}},
  urldate = {2025-01-28},
  file = {/Users/paultalma/Zotero/storage/66I4H4RL/Dayan et al. (1995) - The helmholtz machine.pdf}
}

@article{dayanNeurocomputationalJeremiad2009,
  title = {A Neurocomputational Jeremiad},
  author = {Dayan, Peter},
  year = {2009},
  month = oct,
  journal = {Nature Neuroscience},
  volume = {12},
  number = {10},
  pages = {1207--1207},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/nn1009-1207},
  urldate = {2025-01-28},
  copyright = {2009 Springer Nature America, Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {/Users/paultalma/Zotero/storage/KKSK52PH/Dayan (2009) - A neurocomputational jeremiad.pdf}
}

@article{dayanReinforcementLearningGood2008,
  title = {Reinforcement Learning: {{The Good}}, {{The Bad}} and {{The Ugly}}},
  shorttitle = {Reinforcement Learning},
  author = {Dayan, Peter and Niv, Yael},
  year = {2008},
  month = apr,
  journal = {Current Opinion in Neurobiology},
  series = {Cognitive Neuroscience},
  volume = {18},
  number = {2},
  pages = {185--196},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2008.08.003},
  urldate = {2025-01-28},
  abstract = {Reinforcement learning provides both qualitative and quantitative frameworks for understanding and modeling adaptive decision-making in the face of rewards and punishments. Here we review the latest dispatches from the forefront of this field, and map out some of the territories where lie monsters.}
}

@article{dayanRewardMotivationReinforcement2002,
  title = {Reward, Motivation, and Reinforcement Learning},
  author = {Dayan, Peter and Balleine, Bernard W.},
  year = {2002},
  journal = {Neuron},
  volume = {36},
  number = {2},
  pages = {285--298},
  publisher = {Elsevier},
  urldate = {2025-01-28},
  file = {/Users/paultalma/Zotero/storage/337CJ27Y/S0896-6273(02)00963-7.html}
}

@article{deanMapReduceSimplifiedData2008,
  title = {{{MapReduce}}: Simplified Data Processing on Large Clusters},
  shorttitle = {{{MapReduce}}},
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2008},
  month = jan,
  journal = {Communications of the ACM},
  volume = {51},
  number = {1},
  pages = {107--113},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1327452.1327492},
  urldate = {2024-12-05},
  abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/X73PHHXD/Dean and Ghemawat (2008) - MapReduce simplified data processing on large clusters.pdf}
}

@article{deanWHATALGORITHMSCOULD,
  title = {{{WHAT ALGORITHMS COULD NOT BE}}},
  author = {Dean, Walter},
  langid = {english}
}

@unpublished{deanWhatAlgorithmsCould2007,
  title = {What {{Algorithms Could Not Be}}},
  author = {Dean, Walter H.},
  year = {2007},
  file = {/Users/paultalma/Zotero/storage/54SSX2VS/Dean WHAT ALGORITHMS COULD NOT BE.pdf;/Users/paultalma/Zotero/storage/J25EMQGJ/DEAWAC.html}
}

@article{degraveMagneticControlTokamak2022,
  title = {Magnetic Control of Tokamak Plasmas through Deep Reinforcement Learning},
  author = {Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and {de las Casas}, Diego and Donner, Craig and Fritz, Leslie and Galperti, Cristian and Huber, Andrea and Keeling, James and Tsimpoukelli, Maria and Kay, Jackie and Merle, Antoine and Moret, Jean-Marc and Noury, Seb and Pesamosca, Federico and Pfau, David and Sauter, Olivier and Sommariva, Cristian and Coda, Stefano and Duval, Basil and Fasoli, Ambrogio and Kohli, Pushmeet and Kavukcuoglu, Koray and Hassabis, Demis and Riedmiller, Martin},
  year = {2022},
  month = feb,
  journal = {Nature},
  volume = {602},
  number = {7897},
  pages = {414--419},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04301-9},
  urldate = {2024-12-05},
  abstract = {Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak {\`a} Configuration Variable1,2, including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and `snowflake' configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained `droplets' on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computer science,Magnetically confined plasmas,Nuclear fusion and fission},
  file = {/Users/paultalma/Zotero/storage/DKJQLUSQ/Degrave et al. (2022) - Magnetic control of tokamak plasmas through deep reinforcement learning.pdf}
}

@article{deiganDontTrustFodors2023,
  title = {Don't Trust {{Fodor}}'s Guide in {{Monte Carlo}}: {{Learning}} Concepts by Hypothesis Testing without Circularity},
  shorttitle = {Don't Trust {{Fodor}}'s Guide in {{Monte Carlo}}},
  author = {Deigan, Michael},
  year = {2023},
  journal = {Mind \& Language},
  volume = {38},
  number = {2},
  pages = {355--373},
  issn = {1468-0017},
  doi = {10.1111/mila.12366},
  urldate = {2025-01-06},
  abstract = {Fodor argued that learning a concept by hypothesis testing would involve an impossible circularity. I show that Fodor's argument implicitly relies on the assumption that actually {$\varphi$}-ing entails an ability to {$\varphi$}. But this assumption is false in cases of {$\varphi$}-ing by luck, and just such luck is involved in testing hypotheses with the kinds of generative random sampling methods that many cognitive scientists take our minds to use. Concepts thus can be learned by hypothesis testing without circularity, and it is plausible that this is how humans in fact acquire at least some of their concepts.},
  copyright = {{\copyright} 2022 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {abilities,concept acquisition,concept learning,Fodor,hypothesis testing,sampling}
}

@article{dellaliberaLearningAttendIgnore2009,
  title = {Learning to {{Attend}} and to {{Ignore Is}} a {{Matter}} of {{Gains}} and {{Losses}}},
  author = {Della Libera, Chiara and Chelazzi, Leonardo},
  year = {2009},
  month = jun,
  journal = {Psychological Science},
  volume = {20},
  number = {6},
  pages = {778--784},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.2009.02360.x},
  urldate = {2025-01-28},
  abstract = {Efficient goal-directed behavior in a crowded world is crucially mediated by visual selective attention (VSA), which regulates deployment of cognitive resources toward selected, behaviorally relevant visual objects. Acting as a filter on perceptual representations, VSA allows preferential processing of relevant objects and concurrently inhibits traces of irrelevant items, thus preventing harmful distraction. Recent evidence showed that monetary rewards for performance on VSA tasks strongly affect immediately subsequent deployment of attention; a typical aftereffect of VSA (negative priming) was found only following highly rewarded selections. Here we report a much more striking demonstration that the controlled delivery of monetary rewards also affects attentional processing several days later. Thus, the propensity to select or to ignore specific visual objects appears to be strongly biased by the more or less rewarding consequences of past attentional encounters with the same objects.},
  file = {/Users/paultalma/Zotero/storage/8HZ2NFXZ/Della Libera and Chelazzi (2009) - Learning to Attend and to Ignore Is a Matter of Gains and Losses.pdf}
}

@article{dershowitzNaturalAxiomatizationComputability2008,
  title = {A {{Natural Axiomatization}} of {{Computability}} and {{Proof}} of {{Church}}'s {{Thesis}}},
  author = {Dershowitz, Nachum and Gurevich, Yuri},
  year = {2008},
  journal = {The Bulletin of Symbolic Logic},
  volume = {14},
  number = {3},
  eprint = {20059987},
  eprinttype = {jstor},
  pages = {299--350},
  publisher = {[Association for Symbolic Logic, Cambridge University Press]},
  issn = {1079-8986},
  urldate = {2025-01-30},
  abstract = {Church's Thesis asserts that the only numeric functions that can be calculated by effective means are the recursive ones, which are the same, extensionally, as the Turing-computable numeric functions. The Abstract State Machine Theorem states that every classical algorithm is behaviorally equivalent to an abstract state machine. This theorem presupposes three natural postulates about algorithmic computation. Here, we show that augmenting those postulates with an additional requirement regarding basic operations gives a natural axiomatization of computability and a proof of Church's Thesis, as G{\"o}del and others suggested may be possible. In a similar way, but with a different set of basic operations, one can prove Turing's Thesis, characterizing the effective string functions, and--in particular--the effectively-computable functions on string representations of numbers.},
  file = {/Users/paultalma/Zotero/storage/57S6RCAX/Dershowitz and Gurevich (2008) - A Natural Axiomatization of Computability and Proof of Church's Thesis.pdf}
}

@article{diaconisFiniteExchangeableSequences1980,
  title = {Finite {{Exchangeable Sequences}}},
  author = {Diaconis, P. and Freedman, D.},
  year = {1980},
  month = aug,
  journal = {The Annals of Probability},
  volume = {8},
  number = {4},
  pages = {745--764},
  publisher = {Institute of Mathematical Statistics},
  issn = {0091-1798, 2168-894X},
  doi = {10.1214/aop/1176994663},
  urldate = {2024-11-02},
  abstract = {Let \$X\_1, X\_2,{\textbackslash}cdots, X\_k, X\_\{k+1\},{\textbackslash}cdots, X\_n\$ be exchangeable random variables taking values in the set \$S\$. The variation distance between the distribution of \$X\_1, X\_2,{\textbackslash}cdots, X\_k\$ and the closest mixture of independent, identically distributed random variables is shown to be at most \$2 ck/n\$, where \$c\$ is the cardinality of \$S\$. If \$c\$ is infinite, the bound \$k(k - 1)/n\$ is obtained. These results imply the most general known forms of de Finetti's theorem. Examples are given to show that the rates \$k/n\$ and \$k(k - 1)/n\$ cannot be improved. The main tool is a bound on the variation distance between sampling with and without replacement. For instance, suppose an urn contains \$n\$ balls, each marked with some element of the set \$S\$, whose cardinality \$c\$ is finite. Now \$k\$ draws are made at random from this urn, either with or without replacement. This generates two probability distributions on the set of \$k\$-tuples, and the variation distance between them is at most \$2 ck/n\$.},
  keywords = {60G10,60J05,De Finetti's theorem,Exchangeable,extreme points,presentable,representable,sampling with and without replacement,Symmetric,variation distance},
  file = {/Users/paultalma/Zotero/storage/8MVD57VM/Diaconis and Freedman - 1980 - Finite Exchangeable Sequences.pdf}
}

@article{dingModelSelectionTechniques2018,
  title = {Model {{Selection Techniques}}: {{An Overview}}},
  shorttitle = {Model {{Selection Techniques}}},
  author = {Ding, Jie and Tarokh, Vahid and Yang, Yuhong},
  year = {2018},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {35},
  number = {6},
  pages = {16--34},
  issn = {1558-0792},
  doi = {10.1109/MSP.2018.2867638},
  urldate = {2024-11-17},
  abstract = {In the era of big data, analysts usually explore various statistical models or machine-learning methods for observed data to facilitate scientific discoveries or gain predictive power. Whatever data and fitting procedures are employed, a crucial step is to select the most appropriate model or method from a set of candidates. Model selection is a key ingredient in data analysis for reliable and reproducible statistical inference or prediction, and thus it is central to scientific studies in such fields as ecology, economics, engineering, finance, political science, biology, and epidemiology. There has been a long history of model selection techniques that arise from researches in statistics, information theory, and signal processing. A considerable number of methods has been proposed, following different philosophies and exhibiting varying performances. The purpose of this article is to provide a comprehensive overview of them, in terms of their motivation, large sample performance, and applicability. We provide integrated and practically relevant discussions on theoretical properties of state-of-the-art model selection approaches. We also share our thoughts on some controversial views on the practice of model selection.},
  keywords = {Analytical models,Big Data,Biological system modeling,Computational modeling,Data models,Machine learning,Predictive models},
  file = {/Users/paultalma/Zotero/storage/24WVAYYH/Ding et al. (2018) - Model Selection Techniques An Overview.pdf;/Users/paultalma/Zotero/storage/Y8PP23DA/8498082.html}
}

@incollection{diukDivideConquerHierarchical2013,
  title = {Divide and {{Conquer}}: {{Hierarchical Reinforcement Learning}} and {{Task Decomposition}} in {{Humans}}},
  shorttitle = {Divide and {{Conquer}}},
  booktitle = {Computational and {{Robotic Models}} of the {{Hierarchical Organization}} of {{Behavior}}},
  author = {Diuk, Carlos and Schapiro, Anna and C{\'o}rdova, Natalia and {Ribas-Fernandes}, Jos{\'e} and Niv, Yael and Botvinick, Matthew},
  editor = {Baldassarre, Gianluca and Mirolli, Marco},
  year = {2013},
  pages = {271--291},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-39875-9_12},
  urldate = {2025-01-28},
  abstract = {The field of computational reinforcement learning (RL) has proved extremely useful in research on human and animal behavior and brain function. However, the simple forms of RL considered in most empirical research do not scale well, making their relevance to complex, real-world behavior unclear. In computational RL, one strategy for addressing the scaling problem is to introduce hierarchical structure, an approach that has intriguing parallels with human behavior. We have begun to investigate the potential relevance of hierarchical RL (HRL) to human and animal behavior and brain function. In the present chapter, we first review two results that show the existence of neural correlates to key predictions from HRL. Then, we focus on one aspect of this work, which deals with the question of how action hierarchies are initially established. Work in HRL suggests that hierarchy learning is accomplished by identifying useful subgoal states, and that this might in turn be accomplished through a structural analysis of the given task domain. We review results from a set of behavioral and neuroimaging experiments, in which we have investigated the relevance of these ideas to human learning and decision making.},
  isbn = {978-3-642-39874-2 978-3-642-39875-9},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/NGSF8VLJ/Diuk et al. (2013) - Divide and Conquer Hierarchical Reinforcement Learning and Task Decomposition in Humans.pdf}
}

@article{dolanGoalsHabitsBrain2013,
  title = {Goals and {{Habits}} in the {{Brain}}},
  author = {Dolan, Ray J. and Dayan, Peter},
  year = {2013},
  month = oct,
  journal = {Neuron},
  volume = {80},
  number = {2},
  pages = {312--325},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2013.09.007},
  urldate = {2025-01-28},
  abstract = {An enduring and richly elaborated dichotomy in cognitive neuroscience is that of reflective versus reflexive decision making and choice. Other literatures refer to the two ends of what is likely to be a spectrum with terms such as goal-directed versus habitual, model-based versus model-free or prospective versus retrospective. One of the most rigorous traditions of experimental work in the field started with studies in rodents and graduated via human versions and enrichments of those experiments to a current state in which new paradigms are probing and challenging the very heart of the distinction. We review four generations of work in this tradition and provide pointers to the forefront of the field's fifth generation.},
  file = {/Users/paultalma/Zotero/storage/97AEM8EI/Dolan and Dayan (2013) - Goals and Habits in the Brain.pdf;/Users/paultalma/Zotero/storage/47JPT67S/S0896627313008052.html}
}

@misc{doshi-velezRigorousScienceInterpretable2017,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  year = {2017},
  month = mar,
  number = {arXiv:1702.08608},
  eprint = {1702.08608},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.08608},
  urldate = {2025-01-13},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{doshi-velezRigorousScienceInterpretable2017a,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  year = {2017},
  month = mar,
  number = {arXiv:1702.08608},
  eprint = {1702.08608},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.08608},
  urldate = {2025-01-13},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning}
}

@misc{doshi-velezRigorousScienceInterpretable2017b,
  title = {Towards {{A Rigorous Science}} of {{Interpretable Machine Learning}}},
  author = {{Doshi-Velez}, Finale and Kim, Been},
  year = {2017},
  month = mar,
  number = {arXiv:1702.08608},
  eprint = {1702.08608},
  primaryclass = {stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1702.08608},
  urldate = {2025-01-13},
  abstract = {As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/paultalma/Zotero/storage/I9GQS5BK/Doshi-Velez and Kim (2017) - Towards A Rigorous Science of Interpretable Machine Learning.pdf;/Users/paultalma/Zotero/storage/BJGSLKYG/1702.html}
}

@article{douvenLearnabilityNaturalConcepts,
  title = {The Learnability of Natural Concepts},
  author = {Douven, Igor},
  journal = {Mind \& Language},
  volume = {n/a},
  number = {n/a},
  issn = {1468-0017},
  doi = {10.1111/mila.12523},
  urldate = {2025-01-06},
  abstract = {According to a recent proposal, natural concepts are represented in an optimally designed similarity space, adhering to principles a skilled engineer would use for creatures with our perceptual and cognitive capacities. One key principle is that natural concepts should be easily learnable. While evidence exists for parts of this optimal design proposal, there has been no direct evidence linking naturalness to learning until now. This article presents results from a computational study on perceptual color space, demonstrating that naturalness indeed facilitates learning.},
  copyright = {{\copyright} 2024 John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {concepts,learnability,multi-layer perceptron,naturalness,optimality,similarity spaces},
  file = {/Users/paultalma/Zotero/storage/4RVVMJ59/Douven The learnability of natural concepts.pdf;/Users/paultalma/Zotero/storage/SCNIR4T3/mila.html}
}

@article{drummondModelbasedDecisionMaking,
  title = {Model-Based Decision Making and Model-Free Learning: {{A}} Primer},
  author = {Drummond, Nicole and Niv, Yael},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/Q25TMUUP/Drummond and Niv Model-based decision making and model-free learning A primer.pdf}
}

@article{dubeyPursuitHappinessReinforcement2022,
  title = {The Pursuit of Happiness: {{A}} Reinforcement Learning Perspective on Habituation and Comparisons},
  shorttitle = {The Pursuit of Happiness},
  author = {Dubey, Rachit and Griffiths, Thomas L. and Dayan, Peter},
  editor = {Zhu, Lusha},
  year = {2022},
  month = aug,
  journal = {PLOS Computational Biology},
  volume = {18},
  number = {8},
  pages = {e1010316},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1010316},
  urldate = {2025-01-28},
  abstract = {In evaluating our choices, we often suffer from two tragic relativities. First, when our lives change for the better, we rapidly habituate to the higher standard of living. Second, we cannot escape comparing ourselves to various relative standards. Habituation and comparisons can be very disruptive to decision-making and happiness, and till date, it remains a puzzle why they have come to be a part of cognition in the first place. Here, we present computational evidence that suggests that these features might play an important role in promoting adaptive behavior. Using the framework of reinforcement learning, we explore the benefit of employing a reward function that, in addition to the reward provided by the underlying task, also depends on prior expectations and relative comparisons. We find that while agents equipped with this reward function are less happy, they learn faster and significantly outperform standard reward-based agents in a wide range of environments. Specifically, we find that relative comparisons speed up learning by providing an exploration incentive to the agents, and prior expectations serve as a useful aid to comparisons, especially in sparselyrewarded and non-stationary environments. Our simulations also reveal potential drawbacks of this reward function and show that agents perform sub-optimally when comparisons are left unchecked and when there are too many similar options. Together, our results help explain why we are prone to becoming trapped in a cycle of never-ending wants and desires, and may shed light on psychopathologies such as depression, materialism, and overconsumption.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/UAXRXA2M/Dubey et al. (2022) - The pursuit of happiness A reinforcement learning perspective on habituation and comparisons.pdf}
}

@book{dummettFregePhilosophyMathematics1991,
  title = {Frege: {{Philosophy}} of {{Mathematics}}},
  author = {Dummett, Michael},
  year = {1991},
  edition = {Harvard University Press},
  publisher = {Cambridge University Press},
  address = {Cambridge, MA},
  keywords = {phil}
}

@misc{ecoffetGoExploreNewApproach2021,
  title = {Go-{{Explore}}: A {{New Approach}} for {{Hard-Exploration Problems}}},
  shorttitle = {Go-{{Explore}}},
  author = {Ecoffet, Adrien and Huizinga, Joost and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2021},
  month = feb,
  number = {arXiv:1901.10995},
  eprint = {1901.10995},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1901.10995},
  urldate = {2025-01-28},
  abstract = {A grand challenge in reinforcement learning is intelligent exploration, especially when rewards are sparse or deceptive. Two Atari games serve as benchmarks for such hard-exploration domains: Montezuma's Revenge and Pitfall. On both games, current RL algorithms perform poorly, even those with intrinsic motivation, which is the dominant method to improve performance on hard-exploration domains. To address this shortfall, we introduce a new algorithm called Go-Explore. It exploits the following principles: (1) remember previously visited states, (2) first return to a promising state (without exploration), then explore from it, and (3) solve simulated environments through any available means (including by introducing determinism), then robustify via imitation learning. The combined effect of these principles is a dramatic performance improvement on hard-exploration problems. On Montezuma's Revenge, Go-Explore scores a mean of over 43k points, almost 4 times the previous state of the art. Go-Explore can also harness human-provided domain knowledge and, when augmented with it, scores a mean of over 650k points on Montezuma's Revenge. Its max performance of nearly 18 million surpasses the human world record, meeting even the strictest definition of "superhuman" performance. On Pitfall, Go-Explore with domain knowledge is the first algorithm to score above zero. Its mean score of almost 60k points exceeds expert human performance. Because Go-Explore produces high-performing demonstrations automatically and cheaply, it also outperforms imitation learning work where humans provide solution demonstrations. Go-Explore opens up many new research directions into improving it and weaving its insights into current RL algorithms. It may also enable progress on previously unsolvable hard-exploration problems in many domains, especially those that harness a simulator during training (e.g. robotics).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/paultalma/Zotero/storage/95WDQCP5/Ecoffet et al. (2021) - Go-Explore a New Approach for Hard-Exploration Problems.pdf;/Users/paultalma/Zotero/storage/NNX7UGHR/1901.html}
}

@book{evansVarietiesReference1982,
  title = {The {{Varieties}} of {{Reference}}},
  author = {Evans, Gareth},
  editor = {McDowell, John Henry},
  year = {1982},
  publisher = {Oxford University Press},
  address = {Oxford}
}

@misc{evansWhatDoesProof2013,
  title = {What Does the Proof of {{Birnbaum}}'s Theorem Prove?},
  author = {Evans, Michael},
  year = {2013},
  month = feb,
  number = {arXiv:1302.5468},
  eprint = {1302.5468},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1302.5468},
  urldate = {2025-01-18},
  abstract = {Birnbaum's theorem, that the sufficiency and conditionality principles entail the likelihood principle, has engendered a great deal of controversy and discussion since the publication of the result in 1962. In particular, many have raised doubts as to the validity of this result. Typically these doubts are concerned with the validity of the principles of sufficiency and conditionality as expressed by Birnbaum. Technically it would seem, however, that the proof itself is sound. In this paper we use set theory to formalize the context in which the result is proved and show that in fact Birnbaum's theorem is incorrectly stated as a key hypothesis is left out of the statement. When this hypothesis is added, we see that sufficiency is irrelevant, and that the result is dependent on a well-known flaw in conditionality that renders the result almost vacuous.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Statistics Theory},
  file = {/Users/paultalma/Zotero/storage/YSCZH9UQ/Evans (2013) - What does the proof of Birnbaum's theorem prove.pdf;/Users/paultalma/Zotero/storage/Z5288RG2/1302.html}
}

@book{fineLimitsAbstraction2002,
  title = {The {{Limits}} of {{Abstraction}}},
  author = {Fine, Kit},
  editor = {Schirn, Matthias},
  year = {2002},
  publisher = {Oxford University Press},
  address = {New York}
}

@article{fisherAllModelsAre2019,
  title = {All {{Models}} Are {{Wrong}}, but {{Many}} Are {{Useful}}: {{Learning}} a {{Variable}}'s {{Importance}} by {{Studying}} an {{Entire Class}} of {{Prediction Models Simultaneously}}},
  shorttitle = {All {{Models}} Are {{Wrong}}, but {{Many}} Are {{Useful}}},
  author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
  year = {2019},
  journal = {Journal of machine learning research : JMLR},
  volume = {20},
  pages = {177},
  issn = {1532-4435},
  urldate = {2025-01-17},
  abstract = {Variable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example, a linear model f (x) = xT {$\beta$} with a fixed coefficient vector {$\beta$}) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.},
  pmcid = {PMC8323609},
  pmid = {34335110},
  file = {/Users/paultalma/Zotero/storage/SAAZLU8D/Fisher et al. (2019) - All Models are Wrong, but Many are Useful Learning a Variable’s Importance by Studying an Entire Cl.pdf}
}

@article{fixDiscriminatoryAnalysisNonparametric1989,
  title = {Discriminatory {{Analysis}}. {{Nonparametric Discrimination}}: {{Consistency Properties}}},
  shorttitle = {Discriminatory {{Analysis}}. {{Nonparametric Discrimination}}},
  author = {Fix, Evelyn and Hodges, J. L.},
  year = {1989},
  journal = {International Statistical Review / Revue Internationale de Statistique},
  volume = {57},
  number = {3},
  eprint = {1403797},
  eprinttype = {jstor},
  pages = {238--247},
  publisher = {[Wiley, International Statistical Institute (ISI)]},
  issn = {0306-7734},
  doi = {10.2307/1403797},
  urldate = {2025-01-17},
  file = {/Users/paultalma/Zotero/storage/6P4DTZZX/Fix and Hodges (1989) - Discriminatory Analysis. Nonparametric Discrimination Consistency Properties.pdf}
}

@article{fleisherUnderstandingIdealizationExplainable,
  title = {Understanding, {{Idealization}}, and {{Explainable AI}}},
  author = {Fleisher, Will},
  abstract = {Many AI systems that make important decisions are black boxes: the way they function is opaque even to their developers. This is due to their high complexity and to the fact that they are trained rather than programmed. Explainable AI (XAI) methods aim to provide explanations for black box systems. These post hoc XAI methods typically involve approximating the black box system with a distinct ``explanation'' algorithm. The explanation provided by an XAI method is composed of a different, more easily interpreted kind of algorithm. However, there is debate about what it means for an AI system to be interpretable, and what it takes to give an adequate explanation. Computer scientists and philosophers have objected to post hoc XAI methods on the grounds that: (a) the explanation system functions in a very different way than the black box system to be explained; and (b) the explanation system is imperfectly faithful, sometimes making a different prediction than the black box system. I argue that we should understand interpretability and explainability in terms of understanding. I appeal to the epistemology of understanding for a better account of the goals of XAI methods. Moreover, I suggest that XAI explanation models are akin to idealized scientific models. Idealized scientific models (e.g., the ideal gas law, or evolutionary game theory) deliberately employ false representations of their target phenomena. While there is disagreement over the import of these ``felicitous falsehoods'' for debates about epistemic value and factivity, the common view in epistemology is that idealized models can provide a high-degree of genuine understanding. If XAI methods are like idealized models, then neither (a) nor (b) offer reason to doubt that XAI methods can provide adequate understanding.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/TT4CZ4VQ/Fleisher Understanding, Idealization, and Explainable AI.pdf}
}

@article{fleisherUnderstandingIdealizationExplainable2022,
  title = {Understanding, {{Idealization}}, and {{Explainable AI}}},
  author = {Fleisher, Will},
  year = {2022},
  month = dec,
  journal = {Episteme},
  volume = {19},
  number = {4},
  pages = {534--560},
  issn = {1742-3600, 1750-0117},
  doi = {10.1017/epi.2022.39},
  urldate = {2025-01-16},
  abstract = {Many AI systems that make important decisions are black boxes: how they function is opaque even to their developers. This is due to their high complexity and to the fact that they are trained rather than programmed. Efforts to alleviate the opacity of black box systems are typically discussed in terms of transparency, interpretability, and explainability. However, there is little agreement about what these key concepts mean, which makes it difficult to adjudicate the success or promise of opacity alleviation methods. I argue for a unified account of these key concepts that treats the concept of understanding as fundamental. This allows resources from the philosophy of science and the epistemology of understanding to help guide opacity alleviation efforts. A first significant benefit of this understanding account is that it defuses one of the primary, in-principle objections to post hoc explainable AI (XAI) methods. This ``rationalization objection'' argues that XAI methods provide mere rationalizations rather than genuine explanations. This is because XAI methods involve using a separate ``explanation'' system to approximate the original black box system. These explanation systems function in a completely different way than the original system, yet XAI methods make inferences about the original system based on the behavior of the explanation system. I argue that, if we conceive of XAI methods as idealized scientific models, this rationalization worry is dissolved. Idealized scientific models misrepresent their target phenomena, yet are capable of providing significant and genuine understanding of their targets.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/CA2JUUKX/Fleisher (2022) - Understanding, Idealization, and Explainable AI.pdf}
}

@article{forsterHowTellWhen1994,
  title = {How to {{Tell When Simpler}}, {{More Unified}}, or {{Less}} {{{\emph{Ad Hoc}}}} {{Theories}} Will {{Provide More Accurate Predictions}}},
  author = {Forster, Malcolm and Sober, Elliott},
  year = {1994},
  month = mar,
  journal = {The British Journal for the Philosophy of Science},
  volume = {45},
  number = {1},
  pages = {1--35},
  issn = {0007-0882, 1464-3537},
  doi = {10.1093/bjps/45.1.1},
  urldate = {2024-11-24},
  abstract = {Traditional analyses of the curve fitting problem maintain that the data do not indicate what form the fitted curve should take. Rather, this issue is said to be settled by prior probabilities, by simplicity, or by a background theory. In this paper, we describe a result due to Akaike [1973], which shows how the data can underwrite an inference concerning the curve{\'i}s form based on an estimate of how predictively accurate it will be. We argue that this approach throws light on the theoretical virtues of parsimoniousness, unification, and non ad hocness, on the dispute about Bayesianism, and on empiricism and scientific realism.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/ZA96KE64/Forster and Sober (1994) - How to Tell When Simpler, More Unified, or Less Ad Hoc Theories will Provide More Accurate Pr.pdf}
}

@article{forsterKeyConceptsModel2000,
  title = {Key {{Concepts}} in {{Model Selection}}: {{Performance}} and {{Generalizability}}},
  shorttitle = {Key {{Concepts}} in {{Model Selection}}},
  author = {Forster, Malcolm R},
  year = {2000},
  month = mar,
  journal = {Journal of Mathematical Psychology},
  volume = {44},
  number = {1},
  pages = {205--231},
  issn = {00222496},
  doi = {10.1006/jmps.1999.1284},
  urldate = {2024-11-22},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/VKKIR3IU/Forster (2000) - Key Concepts in Model Selection Performance and Generalizability.pdf}
}

@incollection{forsterWhyLikelihood2004,
  title = {Why {{Likelihood}}?},
  booktitle = {The {{Nature}} of {{Scientific Evidence}}: {{Statistical}}, {{Philosophical}}, and {{Empirical Considerations}}},
  author = {Forster, Malcolm and Sober, Elliott},
  editor = {Taper, Mark L. and Lele, Subhash R.},
  year = {2004},
  month = oct,
  pages = {0},
  publisher = {University of Chicago Press},
  doi = {10.7208/chicago/9780226789583.003.0006},
  urldate = {2024-11-30},
  abstract = {The likelihood principle has been defended on Bayesian grounds, with proponents insisting that it coincides with and systematizes intuitive judgments about example problems, and that it generalizes what is true when hypotheses have deductive consequences about observations. Richard Royall offers three kinds of justification. He points out, first, that the likelihood principle makes intuitive sense when probabilities are all 1s and 0s. His second argument is that the likelihood ratio is precisely the factor that transforms a ratio of prior probabilities into a ratio of posteriors. His third line of defense of the likelihood principle is to show that it coincides with intuitive judgments about evidence when the principle is applied to specific cases. This chapter divides the principle into two parts---one qualitative, the other quantitative---and evaluates each in the light of the Akaike information criterion (AIC). Both turn out to be correct in a special case (when the competing hypotheses have the same number of adjustable parameters), but not otherwise.},
  isbn = {978-0-226-78955-2},
  file = {/Users/paultalma/Zotero/storage/IM63CCDJ/Forster and Sober (2004) - Why Likelihood.pdf;/Users/paultalma/Zotero/storage/UG2B659G/239485421.html}
}

@article{fortuinPriorsBayesianDeep2022,
  title = {Priors in {{Bayesian Deep Learning}}: {{A Review}}},
  shorttitle = {Priors in {{Bayesian Deep Learning}}},
  author = {Fortuin, Vincent},
  year = {2022},
  journal = {International Statistical Review},
  volume = {90},
  number = {3},
  pages = {563--591},
  issn = {1751-5823},
  doi = {10.1111/insr.12502},
  urldate = {2024-11-11},
  abstract = {While the choice of prior is one of the most critical parts of the Bayesian inference workflow, recent Bayesian deep learning models have often fallen back on vague priors, such as standard Gaussians. In this review, we highlight the importance of prior choices for Bayesian deep learning and present an overview of different priors that have been proposed for (deep) Gaussian processes, variational autoencoders and Bayesian neural networks. We also outline different methods of learning priors for these models from data. We hope to motivate practitioners in Bayesian deep learning to think more carefully about the prior specification for their models and to provide them with some inspiration in this regard.},
  copyright = {{\copyright} 2022 The Authors. International Statistical Review published by John Wiley \& Sons Ltd on behalf of International Statistical Institute.},
  langid = {english},
  keywords = {Bayesian deep learning,Bayesian learning,deep learning,priors},
  file = {/Users/paultalma/Zotero/storage/36CP8HN5/Fortuin (2022) - Priors in Bayesian Deep Learning A Review.pdf;/Users/paultalma/Zotero/storage/84VVAJ47/insr.html}
}

@inproceedings{fournierDeepReinforcementLearning2022,
  title = {A {{Deep Reinforcement Learning Heuristic}} for {{SAT}} Based on {{Antagonist Graph Neural Networks}}},
  booktitle = {2022 {{IEEE}} 34th {{International Conference}} on {{Tools}} with {{Artificial Intelligence}} ({{ICTAI}})},
  author = {Fournier, Thomas and Lallouet, Arnaud and Cropsal, T{\'e}lio and Glorian, Ga{\"e}l and Papadopoulos, Alexandre and Petitet, Antoine and Perez, Guillaume and Sekar, Suruthy and Suijlen, Wijnand},
  year = {2022},
  month = oct,
  pages = {1218--1222},
  issn = {2375-0197},
  doi = {10.1109/ICTAI56018.2022.00185},
  urldate = {2024-12-05},
  abstract = {Heuristics are one of the most important tools to guide search to solve combinatorial problems. They are often specifically designed for one single problem and require both expertise and implementation work. Generic frameworks like SAT or CSP have developed heuristics that obey general principles like first fail or are able to learn and adapt from the exploration of the search tree like Dom/wDeg. In SAT, the classic VSIDS heuristic falls into both categories. The question of whether it is possible to learn from solving existing problems has been addressed for a long time by portfolio solvers where the best heuristic is chosen by Machine Learning from hand-crafted features, and more recently with Deep Learning by embedding this knowledge into a Graph Neural Network (GNN). In this paper, we build upon the latter category by proposing a new heuristic based on Deep Reinforcement Learning using two GNNs with adversarial rewards. We show that our method reduces the number of fails to get the first solution by more than 50\% compared to MiniSat. This work shows the advantages of this type of techniques to extract structural and contextual knowledge from past solving experience.},
  keywords = {Constraint Programming,Deep learning,Graph neural networks,Knowledge engineering,Learning (artificial intelligence),Machine Learning,Portfolios,Reinforcement learning,Search problems},
  file = {/Users/paultalma/Zotero/storage/ZHKJKLFD/Fournier et al. (2022) - A Deep Reinforcement Learning Heuristic for SAT based on Antagonist Graph Neural Networks.pdf;/Users/paultalma/Zotero/storage/72ZQDLH3/10098018.html}
}

@book{fregeBasicLawsArithmetic2013,
  title = {Basic {{Laws}} of {{Arithmetic}}},
  author = {Frege, F. L. Gottlob},
  editor = {Ebert, Philip A. and Rossberg, Marcus},
  year = {2013},
  publisher = {Oxford University Press UK}
}

@book{fregeFoundationsArithmeticLogicoMathematical1950,
  title = {The {{Foundations}} of {{Arithmetic}}: {{A Logico-Mathematical Enquiry Into}} the {{Concept}} of {{Number}}},
  author = {Frege, F. L. Gottlob},
  editor = {Austin, J. L.},
  year = {1950},
  publisher = {Northwestern University Press},
  address = {New York, NY, USA}
}

@incollection{fregeFunctionConcept1997,
  title = {Function and {{Concept}}},
  booktitle = {Properties},
  author = {Frege, F. L. Gottlob},
  editor = {Mellor, David Hugh and Oliver, Alex},
  year = {1997},
  pages = {130--149},
  publisher = {Oxford University Press}
}

@article{fregeThoughtLogicalInquiry1956,
  title = {The {{Thought}}: {{A Logical Inquiry}}},
  author = {Frege, F. L. Gottlob},
  year = {1956},
  journal = {Mind},
  volume = {65},
  number = {259},
  pages = {289--311}
}

@incollection{fregeUberSinnUnd1892,
  title = {{\"U}ber {{Sinn}} Und {{Bedeutung}}},
  booktitle = {Translations {{From}} the {{Philosophical Writings}} of {{Gottlob Frege}}},
  author = {Frege, F. L. Gottlob},
  year = {1892},
  publisher = {Blackwell},
  address = {Oxford, UK}
}

@article{freieslebenGeneralizationTheoryRobustness2023,
  title = {Beyond Generalization: A Theory of Robustness in Machine Learning},
  shorttitle = {Beyond Generalization},
  author = {Freiesleben, Timo and Grote, Thomas},
  year = {2023},
  month = sep,
  journal = {Synthese},
  volume = {202},
  number = {4},
  pages = {109},
  issn = {1573-0964},
  doi = {10.1007/s11229-023-04334-9},
  urldate = {2025-01-17},
  abstract = {The term robustness is ubiquitous in modern Machine Learning (ML). However, its meaning varies depending on context and community. Researchers either focus on narrow technical definitions, such as adversarial robustness, natural distribution shifts, and performativity, or they simply leave open what exactly they mean by robustness. In this paper, we provide a conceptual analysis of the term robustness, with the aim to develop a common language, that allows us to weave together different strands of robustness research. We define robustness as the relative stability of a robustness target with respect to specific interventions on a modifier. Our account captures the various sub-types of robustness that are discussed in the research literature, including robustness to distribution shifts, prediction robustness, or the robustness of algorithmic explanations. Finally, we delineate robustness from adjacent key concepts in ML, such as extrapolation, generalization, and uncertainty, and establish it as an independent epistemic concept.},
  langid = {english},
  keywords = {Extrapolation,Generalization,Machine Learning,Models in Science,Robustness,Uncertainty},
  file = {/Users/paultalma/Zotero/storage/GZ6PBAJH/Freiesleben and Grote (2023) - Beyond generalization a theory of robustness in machine learning.pdf}
}

@book{geachTranslationsPhilosophicalWritings1952,
  title = {Translations {{From}} the {{Philosophical Writings}} of {{Gottlob Frege}}},
  author = {Geach, Peter and Black, Max},
  year = {1952},
  publisher = {Blackwell},
  address = {Oxford, UK}
}

@article{geirhosShortcutLearningDeep2020,
  title = {Shortcut Learning in Deep Neural Networks},
  author = {Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
  year = {2020},
  month = nov,
  journal = {Nature Machine Intelligence},
  volume = {2},
  number = {11},
  pages = {665--673},
  publisher = {Nature Publishing Group},
  issn = {2522-5839},
  doi = {10.1038/s42256-020-00257-z},
  urldate = {2025-01-17},
  abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this Perspective we seek to distil how many of deep learning's failures can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in comparative psychology, education and linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
  copyright = {2020 Springer Nature Limited},
  langid = {english},
  keywords = {Computational science,Human behaviour,Information technology,Network models},
  file = {/Users/paultalma/Zotero/storage/FX6THIF3/Geirhos et al. (2020) - Shortcut learning in deep neural networks.pdf}
}

@article{gelmanPhilosophyPracticeBayesian2013,
  title = {Philosophy and the Practice of {{Bayesian}} Statistics},
  author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
  year = {2013},
  journal = {British Journal of Mathematical and Statistical Psychology},
  volume = {66},
  number = {1},
  pages = {8--38},
  issn = {2044-8317},
  doi = {10.1111/j.2044-8317.2011.02037.x},
  urldate = {2024-11-12},
  abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/KBBCR5FQ/Gelman and Shalizi (2013) - Philosophy and the practice of Bayesian statistics.pdf;/Users/paultalma/Zotero/storage/Z5B7CBSQ/j.2044-8317.2011.02037.html}
}

@article{gershmanDiscoveringLatentCauses2015,
  title = {Discovering Latent Causes in Reinforcement Learning},
  author = {Gershman, Samuel J and Norman, Kenneth A and Niv, Yael},
  year = {2015},
  month = oct,
  journal = {Current Opinion in Behavioral Sciences},
  series = {Neuroeconomics},
  volume = {5},
  pages = {43--50},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2015.07.007},
  urldate = {2025-01-28},
  abstract = {Effective reinforcement learning hinges on having an appropriate state representation. But where does this representation come from? We argue that the brain discovers state representations by trying to infer the latent causal structure of the task at hand, and assigning each latent cause to a separate state. In this paper, we review several implications of this latent cause framework, with a focus on Pavlovian conditioning. The framework suggests that conditioning is not the acquisition of associations between cues and outcomes, but rather the acquisition of associations between latent causes and observable stimuli. A latent cause interpretation of conditioning enables us to begin answering questions that have frustrated classical theories: Why do extinguished responses sometimes return? Why do stimuli presented in compound sometimes summate and sometimes do not? Beyond conditioning, the principles of latent causal inference may provide a general theory of structure learning across cognitive domains.},
  file = {/Users/paultalma/Zotero/storage/EJD242XW/Gershman et al. (2015) - Discovering latent causes in reinforcement learning.pdf;/Users/paultalma/Zotero/storage/89EMY7MD/S2352154615001059.html}
}

@article{gershmanDiscoveringLatentCauses2015a,
  title = {Discovering Latent Causes in Reinforcement Learning},
  author = {Gershman, Samuel J and Norman, Kenneth A and Niv, Yael},
  year = {2015},
  month = oct,
  journal = {Current Opinion in Behavioral Sciences},
  volume = {5},
  pages = {43--50},
  issn = {23521546},
  doi = {10.1016/j.cobeha.2015.07.007},
  urldate = {2025-01-28},
  langid = {english}
}

@article{gershmanNoveltyInductiveGeneralization2015,
  title = {Novelty and {{Inductive Generalization}} in {{Human Reinforcement Learning}}},
  author = {Gershman, Samuel J. and Niv, Yael},
  year = {2015},
  journal = {Topics in Cognitive Science},
  volume = {7},
  number = {3},
  pages = {391--415},
  issn = {1756-8765},
  doi = {10.1111/tops.12138},
  urldate = {2025-01-28},
  abstract = {In reinforcement learning (RL), a decision maker searching for the most rewarding option is often faced with the question: What is the value of an option that has never been tried before? One way to frame this question is as an inductive problem: How can I generalize my previous experience with one set of options to a novel option? We show how hierarchical Bayesian inference can be used to solve this problem, and we describe an equivalence between the Bayesian model and temporal difference learning algorithms that have been proposed as models of RL in humans and animals. According to our view, the search for the best option is guided by abstract knowledge about the relationships between different options in an environment, resulting in greater search efficiency compared to traditional RL algorithms previously applied to human cognition. In two behavioral experiments, we test several predictions of our model, providing evidence that humans learn and exploit structured inductive knowledge to make predictions about novel options. In light of this model, we suggest a new interpretation of dopaminergic responses to novelty.},
  copyright = {Copyright {\copyright} 2015 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Bayesian inference,Exploration-exploitation dilemma,Neophilia,Neophobia,Reinforcement learning},
  file = {/Users/paultalma/Zotero/storage/89SMIKRY/Gershman and Niv (2015) - Novelty and Inductive Generalization in Human Reinforcement Learning.pdf;/Users/paultalma/Zotero/storage/QDBEHH2X/tops.html}
}

@article{gershmanReinforcementLearningEpisodic2017,
  title = {Reinforcement Learning and Episodic Memory in Humans and Animals: An Integrative Framework},
  shorttitle = {Reinforcement Learning and Episodic Memory in Humans and Animals},
  author = {Gershman, Samuel J. and Daw, Nathaniel D.},
  year = {2017},
  month = jan,
  journal = {Annual review of psychology},
  volume = {68},
  pages = {101--128},
  issn = {0066-4308},
  doi = {10.1146/annurev-psych-122414-033625},
  urldate = {2025-01-28},
  abstract = {We review the psychology and neuroscience of reinforcement learning (RL), which has witnessed significant progress in the last two decades, enabled by the comprehensive experimental study of simple learning and decision-making tasks. However, the simplicity of these tasks misses important aspects of reinforcement learning in the real world: (i) State spaces are high-dimensional, continuous, and partially observable; this implies that (ii) data are relatively sparse: indeed precisely the same situation may never be encountered twice; and also that (iii) rewards depend on long-term consequences of actions in ways that violate the classical assumptions that make RL tractable., A seemingly distinct challenge is that, cognitively, these theories have largely connected with procedural and semantic memory: how knowledge about action values or world models extracted gradually from many experiences can drive choice. This misses many aspects of memory related to traces of individual events, such as episodic memory. We suggest that these two gaps are related. In particular, the computational challenges can be dealt with, in part, by endowing RL systems with episodic memory, allowing them to (i) efficiently approximate value functions over complex state spaces, (ii) learn with very little data, and (iii) bridge long-term dependencies between actions and rewards. We review the computational theory underlying this proposal and the empirical evidence to support it. Our proposal suggests that the ubiquitous and diverse roles of memory in RL may function as part of an integrated learning system.},
  pmcid = {PMC5953519},
  pmid = {27618944},
  file = {/Users/paultalma/Zotero/storage/QQ5U99VY/annurev-psych-122414-033625.pdf}
}

@article{gershmanReinforcementLearningEpisodic2017a,
  title = {Reinforcement {{Learning}} and {{Episodic Memory}} in {{Humans}} and {{Animals}}: {{An Integrative Framework}}},
  shorttitle = {Reinforcement {{Learning}} and {{Episodic Memory}} in {{Humans}} and {{Animals}}},
  author = {Gershman, Samuel J. and Daw, Nathaniel D.},
  year = {2017},
  month = jan,
  journal = {Annual Review of Psychology},
  volume = {68},
  number = {Volume 68, 2017},
  pages = {101--128},
  publisher = {Annual Reviews},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev-psych-122414-033625},
  urldate = {2025-01-30},
  abstract = {We review the psychology and neuroscience of reinforcement learning (RL), which has experienced significant progress in the past two decades, enabled by the comprehensive experimental study of simple learning and decision-making tasks. However, one challenge in the study of RL is computational: The simplicity of these tasks ignores important aspects of reinforcement learning in the real world: (a) State spaces are high-dimensional, continuous, and partially observable; this implies that (b) data are relatively sparse and, indeed, precisely the same situation may never be encountered twice; furthermore, (c) rewards depend on the long-term consequences of actions in ways that violate the classical assumptions that make RL tractable. A seemingly distinct challenge is that, cognitively, theories of RL have largely involved procedural and semantic memory, the way in which knowledge about action values or world models extracted gradually from many experiences can drive choice. This focus on semantic memory leaves out many aspects of memory, such as episodic memory, related to the traces of individual events. We suggest that these two challenges are related. The computational challenge can be dealt with, in part, by endowing RL systems with episodic memory, allowing them to (a) efficiently approximate value functions over complex state spaces, (b) learn with very little data, and (c) bridge long-term dependencies between actions and rewards. We review the computational theory underlying this proposal and the empirical evidence to support it. Our proposal suggests that the ubiquitous and diverse roles of memory in RL may function as part of an integrated learning system.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/L2BNEV3N/Gershman and Daw (2017) - Reinforcement Learning and Episodic Memory in Humans and Animals An Integrative Framework.pdf;/Users/paultalma/Zotero/storage/QVUHV6P7/annurev-psych-122414-033625.html}
}

@book{goldmanEpistemologyCognition1986,
  title = {Epistemology and Cognition},
  author = {Goldman, Alvin I.},
  year = {1986},
  publisher = {Harvard University Press},
  address = {Cambridge, Mass},
  isbn = {978-0-674-25895-2},
  langid = {english},
  lccn = {BF311 .G582 1986},
  keywords = {Cognition,Knowledge Theory of,Philosophy},
  file = {/Users/paultalma/Zotero/storage/9W9KIED7/Goldman (1986) - Epistemology and cognition.pdf}
}

@article{griffithsRationalUseCognitive2015,
  title = {Rational {{Use}} of {{Cognitive Resources}}: {{Levels}} of {{Analysis Between}} the {{Computational}} and the {{Algorithmic}}},
  shorttitle = {Rational {{Use}} of {{Cognitive Resources}}},
  author = {Griffiths, Thomas L. and Lieder, Falk and Goodman, Noah D.},
  year = {2015},
  month = apr,
  journal = {Topics in Cognitive Science},
  volume = {7},
  number = {2},
  pages = {217--229},
  issn = {1756-8757, 1756-8765},
  doi = {10.1111/tops.12142},
  urldate = {2024-12-01},
  abstract = {Abstract             Marr's levels of analysis---computational, algorithmic, and implementation---have served cognitive science well over the last 30~years. But the recent increase in the popularity of the computational level raises a new challenge: How do we begin to relate models at different levels of analysis? We propose that it is possible to define levels of analysis that lie between the computational and the algorithmic, providing a way to build a bridge between computational- and algorithmic-level models. The key idea is to push the notion of rationality, often used in defining computational-level models, deeper toward the algorithmic level. We offer a simple recipe for reverse-engineering the mind's cognitive strategies by deriving optimal algorithms for a series of increasingly more realistic abstract computational architectures, which we call ``resource-rational analysis.''},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/QDDDEYM9/Griffiths et al. (2015) - Rational Use of Cognitive Resources Levels of Analysis Between the Computational and the Algorithmi.pdf}
}

@article{groteReliabilityMachineLearning2024,
  title = {Reliability in {{Machine Learning}}},
  author = {Grote, Thomas and Genin, Konstantin and Sullivan, Emily},
  year = {2024},
  journal = {Philosophy Compass},
  volume = {19},
  number = {5},
  pages = {e12974},
  issn = {1747-9991},
  doi = {10.1111/phc3.12974},
  urldate = {2025-01-16},
  abstract = {Issues of reliability are claiming center-stage in the epistemology of machine learning. This paper unifies different branches in the literature and points to promising research directions, whilst also providing an accessible introduction to key concepts in statistics and machine learning -- as far as they are concerned with reliability.},
  copyright = {{\copyright} 2024 The Authors. Philosophy Compass published by John Wiley \& Sons Ltd.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/NQVUX7VH/Grote et al. (2024) - Reliability in Machine Learning.pdf}
}

@misc{gruberSourcesUncertaintyMachine2023,
  title = {Sources of {{Uncertainty}} in {{Machine Learning}} -- {{A Statisticians}}' {{View}}},
  author = {Gruber, Cornelia and Schenk, Patrick Oliver and Schierholz, Malte and Kreuter, Frauke and Kauermann, G{\"o}ran},
  year = {2023},
  month = may,
  number = {arXiv:2305.16703},
  eprint = {2305.16703},
  publisher = {arXiv},
  urldate = {2024-11-06},
  abstract = {Machine Learning and Deep Learning have achieved an impressive standard today, enabling us to answer questions that were inconceivable a few years ago. Besides these successes, it becomes clear, that beyond pure prediction, which is the primary strength of most supervised machine learning algorithms, the quantification of uncertainty is relevant and necessary as well. While first concepts and ideas in this direction have emerged in recent years, this paper adopts a conceptual perspective and examines possible sources of uncertainty. By adopting the viewpoint of a statistician, we discuss the concepts of aleatoric and epistemic uncertainty, which are more commonly associated with machine learning. The paper aims to formalize the two types of uncertainty and demonstrates that sources of uncertainty are miscellaneous and can not always be decomposed into aleatoric and epistemic. Drawing parallels between statistical concepts and uncertainty in machine learning, we also demonstrate the role of data and their influence on uncertainty.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/paultalma/Zotero/storage/RRZMLWV9/Gruber et al. (2023) - Sources of Uncertainty in Machine Learning -- A Statisticians' View.pdf;/Users/paultalma/Zotero/storage/952M2ZRC/2305.html}
}

@misc{guestLevelsRepresentationDeep2019,
  title = {Levels of {{Representation}} in a {{Deep Learning Model}} of {{Categorization}}},
  author = {Guest, Olivia and Love, Bradley C.},
  year = {2019},
  month = may,
  primaryclass = {New Results},
  pages = {626374},
  publisher = {bioRxiv},
  doi = {10.1101/626374},
  urldate = {2025-01-28},
  abstract = {Deep convolutional neural networks (DCNNs) rival humans in object recognition. The layers (or levels of representation) in DCNNs have been successfully aligned with processing stages along the ventral stream for visual processing. Here, we propose a model of concept learning that uses visual representations from these networks to build memory representations of novel categories, which may rely on the medial temporal lobe (MTL) and medial prefrontal cortex (mPFC). Our approach opens up two possibilities: a) formal investigations can involve photographic stimuli as opposed to stimuli handcrafted and coded by the experimenter; b) model comparison can determine which level of representation within a DCNN a learner is using during categorization decisions. Pursuing the latter point, DCNNs suggest that the shape bias in children relies on representations at more advanced network layers whereas a learner that relied on lower network layers would display a color bias. These results confirm the role of natural statistics in the shape bias (i.e., shape is predictive of category membership) while highlighting that the type of statistics matter, i.e., those from lower or higher levels of representation. We use the same approach to provide evidence that pigeons performing seemingly sophisticated categorization of complex imagery may in fact be relying on representations that are very low-level (i.e., retinotopic). Although complex features, such as shape, relatively predominate at more advanced network layers, even simple features, such as spatial frequency and orientation, are better represented at the more advanced layers, contrary to a standard hierarchical view.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/8ZB4LYM7/Guest and Love (2019) - Levels of Representation in a Deep Learning Model of Categorization.pdf}
}

@article{gureckisUnifiedAccountSupervised2003,
  title = {Towards a Unified Account of Supervised and Unsupervised Category Learning},
  author = {Gureckis, Todd M. and Love, Bradley C.},
  year = {2003},
  month = jan,
  journal = {Journal of Experimental \& Theoretical Artificial Intelligence},
  volume = {15},
  number = {1},
  pages = {1--24},
  publisher = {Taylor \& Francis},
  issn = {0952-813X},
  doi = {10.1080/09528130210166097},
  urldate = {2024-11-05},
  abstract = {(Supervised and Unsupervised STratified Adaptive IncrementalNetwork) is a network model of human category learning. SUSTAIN initially assumes a simple category structure. If simple solutions prove inadequate and SUSTAIN is confronted with a surprising event (e.g. it is told that a bat is a mammal instead of a bird), SUSTAIN recruits an additional cluster to represent the surprising event. Newly recruited clusters are available to explain future events and can themselves evolve into prototypes/attractors/rules. SUSTAIN has expanded the scope of findings that models of human category learning can address. This paper extends SUSTAIN so that it can be used to account for both supervised and unsupervised learning data through a common mechanism. A modified recruitment rule is introduced that creates new conceptual clusters in response to surprising events during learning. The new formulation of the model is called uSUSTAIN for `unified SUSTAIN.' The implications of using a unified recruitment method for both supervised and unsupervised learning are discussed.},
  file = {/Users/paultalma/Zotero/storage/XBDE8VAR/Gureckis and Love (2003) - Towards a unified account of supervised and unsupervised category learning.pdf}
}

@article{gureckisWhenThingsGet2009,
  title = {When {{Things Get Worse}} before They {{Get Better}}: {{Regulatory Fit}} and {{Average-Reward Learning}} in a {{Dynamic Decision-Making Environment}}},
  shorttitle = {When {{Things Get Worse}} before They {{Get Better}}},
  author = {Gureckis, Todd and Love, Bradley and Markman, Arthur and Otto, A. Ross},
  year = {2009},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {31},
  number = {31},
  urldate = {2025-01-28},
  abstract = {Author(s): Gureckis, Todd; Love, Bradley; Markman, Arthur; Otto, A. Ross},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/5N8F2E8H/Gureckis et al. (2009) - When Things Get Worse before they Get Better Regulatory Fit and Average-Reward Learning in a Dynami.pdf}
}

@incollection{haasEvaluativeMind2024,
  title = {The {{Evaluative Mind}}},
  booktitle = {Mind {{Design III}}},
  author = {Haas, Julia},
  year = {2024},
  urldate = {2025-01-28},
  file = {/Users/paultalma/Zotero/storage/N2GD8H3H/go.pdf}
}

@article{haasReinforcementLearningBrief2022,
  title = {Reinforcement Learning: {{A}} Brief Guide for Philosophers of Mind},
  shorttitle = {Reinforcement Learning},
  author = {Haas, Julia},
  year = {2022},
  journal = {Philosophy Compass},
  volume = {17},
  number = {9},
  pages = {e12865},
  issn = {1747-9991},
  doi = {10.1111/phc3.12865},
  urldate = {2024-11-17},
  abstract = {In this opinionated review, I draw attention to some of the contributions reinforcement learning can make to questions in the philosophy of mind. In particular, I highlight reinforcement learning's foundational emphasis on the role of reward in agent learning, and canvass two ways in which the framework may advance our understanding of perception and motivation.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/QXV8PANX/Haas (2022) - Reinforcement learning A brief guide for philosophers of mind.pdf;/Users/paultalma/Zotero/storage/EJPFZXQ5/phc3.html}
}

@book{hajekOxfordHandbookProbability2016,
  title = {The {{Oxford Handbook}} of {{Probability}} and {{Philosophy}}},
  author = {Hajek, Alan and Hitchcock, Christopher},
  year = {2016},
  keywords = {Statistics},
  file = {/Users/paultalma/Zotero/storage/32XQWGM3/Hajek and Hitchcock (eds.) (2016) - The Oxford Handbook of Probability and Philosophy.pdf}
}

@incollection{haleBuryCaesarDots2001,
  title = {To {{Bury Caesar}} {\textbackslash}dots},
  booktitle = {The {{Reason}}'s {{Proper Study}}: {{Essays Towards}} a {{Neo-Fregean Philosophy}} of {{Mathematics}}},
  author = {Hale, Bob and Wright, Crispin},
  editor = {Hale, Bob and Wright, Crispin},
  year = {2001},
  publisher = {Clarendon Press}
}

@incollection{haleRealsAbstraction2001,
  title = {Reals by {{Abstraction}}},
  booktitle = {The {{Reason}}'s {{Proper Study}}: {{Essays Towards}} a {{Neo-Fregean Philosophy}} of {{Mathematics}}},
  author = {Hale, Bob},
  editor = {Hale, Bob and Wright, Crispin},
  year = {2001},
  publisher = {Clarendon Press}
}

@book{haleReasonsProperStudy2001,
  title = {The {{Reason}}'s {{Proper Study}}: {{Essays Towards}} a {{Neo-Fregean Philosophy}} of {{Mathematics}}},
  author = {Hale, Bob and Wright, Crispin},
  year = {2001},
  publisher = {Clarendon Press},
  address = {Oxford}
}

@article{hayhoeEyeMovementsNatural2005,
  title = {Eye Movements in Natural Behavior},
  author = {Hayhoe, Mary and Ballard, Dana},
  year = {2005},
  month = apr,
  journal = {Trends in Cognitive Sciences},
  volume = {9},
  number = {4},
  pages = {188--194},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2005.02.009},
  urldate = {2025-01-28},
  langid = {english},
  pmid = {15808501},
  file = {/Users/paultalma/Zotero/storage/3V48VVCM/Hayhoe and Ballard (2005) - Eye movements in natural behavior.pdf}
}

@article{heckCardinalityCountingEquinumerosity2000,
  title = {Cardinality, {{Counting}}, and {{Equinumerosity}}},
  author = {Heck, Richard Kimberly},
  year = {2000},
  journal = {Notre Dame J. Formal Log.},
  volume = {41},
  pages = {187--209}
}

@book{heckFregesTheorem2011,
  title = {Frege's {{Theorem}}},
  author = {Heck, Richard Kimberly},
  year = {2011},
  publisher = {Clarendon Press},
  address = {New York}
}

@article{heckFregesTheoremIntroduction1999,
  title = {Frege's {{Theorem}}: {{An Introduction}}},
  author = {Heck, Richard Kimberly},
  year = {1999},
  journal = {The Harvard Review of Philosophy},
  volume = {7},
  number = {1},
  pages = {56--73},
  publisher = {The Harvard Review of Philosophy},
  doi = {10.5840/harvardreview1999717}
}

@incollection{heckFregesTheoremOverview2011,
  title = {Frege's {{Theorem}}: {{An Overview}}},
  booktitle = {Frege's {{Theorem}}},
  author = {Heck, Richard Kimberly},
  year = {2011},
  publisher = {Oxford University Press. Originally published under the name Richard G. Heck},
  address = {Oxford, UK.}
}

@incollection{heckJuliusCaesarBasic2005,
  title = {Julius {{Caesar}} and {{Basic Law V}}},
  booktitle = {Frege's {{Theorem}}},
  author = {Heck, Richard Kimberly},
  year = {2005},
  publisher = {Clarendon Press},
  address = {Oxford}
}

@misc{hickeyRewardChangesSalience2010,
  title = {Reward {{Changes Salience}} in {{Human Vision}} via the {{Anterior Cingulate}} {\textbar} {{Journal}} of {{Neuroscience}}},
  author = {Hickey, Clayton and Chelazzi, Leonardo and Theeuwes, Jan},
  year = {2010},
  urldate = {2025-01-28},
  howpublished = {https://www.jneurosci.org/content/30/33/11096},
  file = {/Users/paultalma/Zotero/storage/IEEYLY78/Reward Changes Salience in Human Vision via the Anterior Cingulate  Journal of Neuroscience.pdf;/Users/paultalma/Zotero/storage/SLDD8Q3V/11096.html}
}

@misc{hikosakaBasalGangliaOrient2006,
  title = {Basal {{Ganglia Orient Eyes}} to {{Reward}}},
  author = {Hikosaka, Okihide and Nakamura, Kae and Nakahara, Hiroyuki},
  year = {2006},
  doi = {10.1152/jn.00458.2005},
  urldate = {2025-01-28},
  howpublished = {https://journals.physiology.org/doi/epdf/10.1152/jn.00458.2005},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/L6XNLCMN/Basal Ganglia Orient Eyes to Reward.pdf;/Users/paultalma/Zotero/storage/S9XUEJNB/jn.00458.html}
}

@article{hillWhatAlgorithm2016,
  title = {What an {{Algorithm Is}}},
  author = {Hill, Robin K.},
  year = {2016},
  month = mar,
  journal = {Philosophy \& Technology},
  volume = {29},
  number = {1},
  pages = {35--59},
  issn = {2210-5441},
  doi = {10.1007/s13347-014-0184-5},
  urldate = {2025-01-30},
  abstract = {The algorithm, a building block of computer science, is defined from an intuitive and pragmatic point of view, through a methodological lens of philosophy rather than that of formal computation. The treatment extracts properties of abstraction, control, structure, finiteness, effective mechanism, and imperativity, and intentional aspects of goal and preconditions. The focus on the algorithm as a robust conceptual object obviates issues of correctness and minimality. Neither the articulation of an algorithm nor the dynamic process constitute the algorithm itself. Analysis for implications in computer science and philosophy reveals unexpected results, new questions, and new perspectives on current questions, including the relationship between our informally construed algorithms and Turing machines. Exploration in terms of current computational and philosophical thinking invites further developments.},
  langid = {english},
  keywords = {Algorithm,Church-Turing thesis,Mathematical ontology,Philosophy of computer science},
  file = {/Users/paultalma/Zotero/storage/2VFLY8M6/Hill (2016) - What an Algorithm Is.pdf}
}

@article{holroydNeuralBasisHuman2002,
  title = {The Neural Basis of Human Error Processing: {{Reinforcement}} Learning, Dopamine, and the Error-Related Negativity.},
  shorttitle = {The Neural Basis of Human Error Processing},
  author = {Holroyd, Clay B. and Coles, Michael G. H.},
  year = {2002},
  month = oct,
  journal = {Psychological Review},
  volume = {109},
  number = {4},
  pages = {679--709},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.109.4.679},
  urldate = {2025-01-28},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/56GG6EHW/Holroyd and Coles (2002) - The neural basis of human error processing Reinforcement learning, dopamine, and the error-related.pdf}
}

@article{holroydNeuralBasisHuman2002a,
  title = {The {{Neural Basis}} of {{Human Error Processing}}: {{Reinforcement Learning}}, {{Dopamine}}, and the {{Error-Related Negativity}}},
  shorttitle = {The {{Neural Basis}} of {{Human Error Processing}}},
  author = {Holroyd, Clay B. and Coles, Michael G. H.},
  year = {2002},
  journal = {Psychological Review},
  volume = {109},
  number = {4},
  pages = {679--709},
  doi = {10.1037/0033-295x.109.4.679},
  file = {/Users/paultalma/Zotero/storage/MZUS9SCE/Holroyd and Coles (2002) - The Neural Basis of Human Error Processing Reinforcement Learning, Dopamine, and the Error-Related.pdf;/Users/paultalma/Zotero/storage/B63QC4IF/HOLTNB.html}
}

@article{hoPeopleConstructSimplified2022,
  title = {People Construct Simplified Mental Representations to Plan},
  author = {Ho, Mark K. and Abel, David and Correa, Carlos G. and Littman, Michael L. and Cohen, Jonathan D. and Griffiths, Thomas L.},
  year = {2022},
  month = jun,
  journal = {Nature},
  volume = {606},
  number = {7912},
  pages = {129--136},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-022-04743-9},
  urldate = {2025-01-28},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/YX8RVPBR/Ho et al. (2022) - People construct simplified mental representations to plan.pdf}
}

@article{hoValueAbstraction2019,
  title = {The Value of Abstraction},
  author = {Ho, Mark K and Abel, David and Griffiths, Thomas L and Littman, Michael L},
  year = {2019},
  month = oct,
  journal = {Current Opinion in Behavioral Sciences},
  volume = {29},
  pages = {111--116},
  issn = {23521546},
  doi = {10.1016/j.cobeha.2019.05.001},
  urldate = {2025-01-28},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/TYH6PFLI/Ho et al. (2019) - The value of abstraction.pdf}
}

@article{hoValueAbstraction2019a,
  title = {The Value of Abstraction},
  author = {Ho, Mark K and Abel, David and Griffiths, Thomas L and Littman, Michael L},
  year = {2019},
  month = oct,
  journal = {Current Opinion in Behavioral Sciences},
  series = {Artificial {{Intelligence}}},
  volume = {29},
  pages = {111--116},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2019.05.001},
  urldate = {2025-01-28},
  file = {/Users/paultalma/Zotero/storage/IFXQBCR9/Ho et al. (2019) - The value of abstraction.pdf;/Users/paultalma/Zotero/storage/YDBCJ6WN/S2352154619300026.html}
}

@book{Huttegger2017-HUTTPF,
  title = {The Probabilistic Foundations of Rational Learning},
  author = {Huttegger, Simon M.},
  year = {2017},
  publisher = {Cambridge University Press}
}

@article{hutteggerDefenseReflection2013,
  title = {In {{Defense}} of {{Reflection}}},
  author = {Huttegger, Simon M.},
  year = {2013},
  month = jul,
  journal = {Philosophy of Science},
  volume = {80},
  number = {3},
  pages = {413--433},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/671427},
  urldate = {2024-11-03},
  abstract = {I discuss two ways of justifying reflection principles. First, I propose that an undogmatic reading of dynamic Dutch book arguments provides them with a sound foundation. Second, I show also that minimizing expected inaccuracy leads to a novel argument for reflection principles. The required inaccuracy measures comprise a natural class of functions that can be derived from a generalization of a condition known as propriety or immodesty. This shows that reflection principles are an essential feature not just of consistent degrees of belief but also of degrees of belief that approximate truth.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/QXAAPWTV/Huttegger - 2013 - In Defense of Reflection.pdf}
}

@article{hutteggerSuperconditioning2024,
  title = {Superconditioning},
  author = {Huttegger, Simon M.},
  year = {2024},
  month = apr,
  journal = {Philosophical Studies},
  volume = {181},
  number = {4},
  pages = {811--833},
  issn = {0031-8116, 1573-0883},
  doi = {10.1007/s11098-024-02117-7},
  urldate = {2024-11-03},
  abstract = {When can a shift from a prior to a posterior be represented by conditionalization? A wellknown result, known as ``superconditioning'' and going back to work by Diaconis and Zabell, gives a sharp answer. This paper extends the result and connects it to the reflection principle and common priors. I show that a shift from a prior to a set of posteriors can be represented within a conditioning model if and only if the prior and the posteriors are connected via a general form of the reflection principle. Common priors can be characterized by principles that require a certain kind of coherence between distinct sets of posteriors. I discuss the implications these results have for diachronic and synchronic modes of updating, learning experiences, the common prior assumption of game theory, and time-slice epistemology.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/SKTPTFCZ/Huttegger - 2024 - Superconditioning.pdf}
}

@article{icardBayesBoundsRational2018,
  title = {Bayes, {{Bounds}}, and {{Rational Analysis}}},
  author = {Icard, Thomas F.},
  year = {2018},
  month = jan,
  journal = {Philosophy of Science},
  volume = {85},
  number = {1},
  pages = {79--101},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/694837},
  urldate = {2025-01-06},
  abstract = {While Bayesian models have been applied to an impressive range of cognitive phenomena, methodological challenges have been leveled concerning their role in the program of rational analysis. The focus of the current article is on computational impediments to probabilistic inference and related puzzles about empirical confirmation of these models. The proposal is to rethink the role of Bayesian methods in rational analysis, to adopt an independently motivated notion of rationality appropriate for computationally bounded agents, and to explore broad conditions under which (approximately) Bayesian agents would be rational. The proposal is illustrated with a characterization of costs inspired by thermodynamics.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/DKF7FDWQ/Icard (2018) - Bayes, Bounds, and Rational Analysis.pdf}
}

@article{icardWhyBeRandom2021,
  title = {Why {{Be Random}}?},
  author = {Icard, Thomas},
  year = {2021},
  month = jan,
  journal = {Mind},
  volume = {130},
  number = {517},
  pages = {111--139},
  issn = {0026-4423},
  doi = {10.1093/mind/fzz065},
  urldate = {2024-12-04},
  abstract = {When does it make sense to act randomly? A persuasive argument from Bayesian decision theory legitimizes randomization essentially only in tie-breaking situations. Rational behaviour in humans, non-human animals, and artificial agents, however, often seems indeterminate, even random. Moreover, rationales for randomized acts have been offered in a number of disciplines, including game theory, experimental design, and machine learning. A common way of accommodating some of these observations is by appeal to a decision-maker's bounded computational resources. Making this suggestion both precise and compelling is surprisingly difficult. Toward this end, I propose two fundamental rationales for randomization, drawing upon diverse ideas and results from the wider theory of computation. The first unifies common intuitions in favour of randomization from the aforementioned disciplines. The second introduces a deep connection between randomization and memory: access to a randomizing device is provably helpful for an agent burdened with a finite memory. Aside from fit with ordinary intuitions about rational action, the two rationales also make sense of empirical observations in the biological world. Indeed, random behaviour emerges more or less where it should, according to the proposal.},
  file = {/Users/paultalma/Zotero/storage/Q5G7MG5N/Icard (2021) - Why Be Random.pdf;/Users/paultalma/Zotero/storage/CVF8D5SM/5611345.html}
}

@article{icardWhyBeRandom2021a,
  title = {Why {{Be Random}}?},
  author = {Icard, Thomas},
  year = {2021},
  month = jan,
  journal = {Mind},
  volume = {130},
  number = {517},
  pages = {111--139},
  issn = {0026-4423},
  doi = {10.1093/mind/fzz065},
  urldate = {2025-01-06},
  abstract = {When does it make sense to act randomly? A persuasive argument from Bayesian decision theory legitimizes randomization essentially only in tie-breaking situations. Rational behaviour in humans, non-human animals, and artificial agents, however, often seems indeterminate, even random. Moreover, rationales for randomized acts have been offered in a number of disciplines, including game theory, experimental design, and machine learning. A common way of accommodating some of these observations is by appeal to a decision-maker's bounded computational resources. Making this suggestion both precise and compelling is surprisingly difficult. Toward this end, I propose two fundamental rationales for randomization, drawing upon diverse ideas and results from the wider theory of computation. The first unifies common intuitions in favour of randomization from the aforementioned disciplines. The second introduces a deep connection between randomization and memory: access to a randomizing device is provably helpful for an agent burdened with a finite memory. Aside from fit with ordinary intuitions about rational action, the two rationales also make sense of empirical observations in the biological world. Indeed, random behaviour emerges more or less where it should, according to the proposal.}
}

@article{jefferysOckhamsRazorBayesian1992,
  title = {Ockham's {{Razor}} and {{Bayesian Analysis}}},
  author = {Jefferys, William H. and Berger, James O.},
  year = {1992},
  journal = {American Scientist},
  volume = {80},
  number = {1},
  eprint = {29774559},
  eprinttype = {jstor},
  pages = {64--72},
  publisher = {Sigma Xi, The Scientific Research Society},
  issn = {0003-0996},
  urldate = {2024-11-22},
  file = {/Users/paultalma/Zotero/storage/PEK2QIHA/Jefferys and Berger (1992) - Ockham's Razor and Bayesian Analysis.pdf}
}

@article{jeffreyFinettisProbabilism1984,
  title = {De {{Finetti}}'s {{Probabilism}}},
  author = {Jeffrey, Richard},
  year = {1984},
  journal = {Synthese},
  volume = {60},
  number = {1},
  eprint = {20116017},
  eprinttype = {jstor},
  pages = {73--90},
  publisher = {Springer},
  issn = {0039-7857},
  urldate = {2024-11-05},
  file = {/Users/paultalma/Zotero/storage/G95GYJ74/Jeffrey (1984) - De Finetti's Probabilism.pdf}
}

@article{jonesIntegratingReinforcementLearning2010,
  title = {Integrating {{Reinforcement Learning}} with {{Models}} of {{Representation Learning}}},
  author = {Jones, Matt and Canas, Fabian},
  year = {2010},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {32},
  number = {32},
  urldate = {2025-01-31},
  abstract = {Author(s): Jones, Matt; Canas, Fabian},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/AEG4X4XC/Jones and Canas (2010) - Integrating Reinforcement Learning with Models of Representation Learning.pdf}
}

@article{kangasraasioParameterInferenceComputational2019,
  title = {Parameter {{Inference}} for {{Computational Cognitive Models}} with {{Approximate Bayesian Computation}}},
  author = {Kangasr{\"a}{\"a}si{\"o}, Antti and Jokinen, Jussi P. P. and Oulasvirta, Antti and Howes, Andrew and Kaski, Samuel},
  year = {2019},
  journal = {Cognitive Science},
  volume = {43},
  number = {6},
  pages = {e12738},
  issn = {1551-6709},
  doi = {10.1111/cogs.12738},
  urldate = {2025-01-06},
  abstract = {This paper addresses a common challenge with computational cognitive models: identifying parameter values that are both theoretically plausible and generate predictions that match well with empirical data. While computational models can offer deep explanations of cognition, they are computationally complex and often out of reach of traditional parameter fitting methods. Weak methodology may lead to premature rejection of valid models or to acceptance of models that might otherwise be falsified. Mathematically robust fitting methods are, therefore, essential to the progress of computational modeling in cognitive science. In this article, we investigate the capability and role of modern fitting methods---including Bayesian optimization and approximate Bayesian computation---and contrast them to some more commonly used methods: grid search and Nelder--Mead optimization. Our investigation consists of a reanalysis of the fitting of two previous computational models: an Adaptive Control of Thought---Rational model of skill acquisition and a computational rationality model of visual search. The results contrast the efficiency and informativeness of the methods. A key advantage of the Bayesian methods is the ability to estimate the uncertainty of fitted parameter values. We conclude that approximate Bayesian computation is (a) efficient, (b) informative, and (c) offers a path to reproducible results.},
  copyright = {{\copyright} 2019 The Authors. Cognitive Science published by Wiley Periodicals, Inc. on behalf of Cognitive Science Society. All rights reserved},
  langid = {english},
  keywords = {Approximate Bayesian computation,Bayesian optimization,Cognitive models,Computational statistics,Inference,Machine learning,Parameter estimation}
}

@article{kangasraasioParameterInferenceComputational2019a,
  title = {Parameter {{Inference}} for {{Computational Cognitive Models}} with {{Approximate Bayesian Computation}}},
  author = {Kangasr{\"a}{\"a}si{\"o}, Antti and Jokinen, Jussi P. P. and Oulasvirta, Antti and Howes, Andrew and Kaski, Samuel},
  year = {2019},
  journal = {Cognitive Science},
  volume = {43},
  number = {6},
  pages = {e12738},
  issn = {1551-6709},
  doi = {10.1111/cogs.12738},
  urldate = {2025-01-06},
  abstract = {This paper addresses a common challenge with computational cognitive models: identifying parameter values that are both theoretically plausible and generate predictions that match well with empirical data. While computational models can offer deep explanations of cognition, they are computationally complex and often out of reach of traditional parameter fitting methods. Weak methodology may lead to premature rejection of valid models or to acceptance of models that might otherwise be falsified. Mathematically robust fitting methods are, therefore, essential to the progress of computational modeling in cognitive science. In this article, we investigate the capability and role of modern fitting methods---including Bayesian optimization and approximate Bayesian computation---and contrast them to some more commonly used methods: grid search and Nelder--Mead optimization. Our investigation consists of a reanalysis of the fitting of two previous computational models: an Adaptive Control of Thought---Rational model of skill acquisition and a computational rationality model of visual search. The results contrast the efficiency and informativeness of the methods. A key advantage of the Bayesian methods is the ability to estimate the uncertainty of fitted parameter values. We conclude that approximate Bayesian computation is (a) efficient, (b) informative, and (c) offers a path to reproducible results.},
  copyright = {{\copyright} 2019 The Authors. Cognitive Science published by Wiley Periodicals, Inc. on behalf of Cognitive Science Society. All rights reserved},
  langid = {english},
  keywords = {Approximate Bayesian computation,Bayesian optimization,Cognitive models,Computational statistics,Inference,Machine learning,Parameter estimation}
}

@article{kangasraasioParameterInferenceComputational2019b,
  title = {Parameter {{Inference}} for {{Computational Cognitive Models}} with {{Approximate Bayesian Computation}}},
  author = {Kangasr{\"a}{\"a}si{\"o}, Antti and Jokinen, Jussi P. P. and Oulasvirta, Antti and Howes, Andrew and Kaski, Samuel},
  year = {2019},
  journal = {Cognitive Science},
  volume = {43},
  number = {6},
  pages = {e12738},
  issn = {1551-6709},
  doi = {10.1111/cogs.12738},
  urldate = {2025-01-06},
  abstract = {This paper addresses a common challenge with computational cognitive models: identifying parameter values that are both theoretically plausible and generate predictions that match well with empirical data. While computational models can offer deep explanations of cognition, they are computationally complex and often out of reach of traditional parameter fitting methods. Weak methodology may lead to premature rejection of valid models or to acceptance of models that might otherwise be falsified. Mathematically robust fitting methods are, therefore, essential to the progress of computational modeling in cognitive science. In this article, we investigate the capability and role of modern fitting methods---including Bayesian optimization and approximate Bayesian computation---and contrast them to some more commonly used methods: grid search and Nelder--Mead optimization. Our investigation consists of a reanalysis of the fitting of two previous computational models: an Adaptive Control of Thought---Rational model of skill acquisition and a computational rationality model of visual search. The results contrast the efficiency and informativeness of the methods. A key advantage of the Bayesian methods is the ability to estimate the uncertainty of fitted parameter values. We conclude that approximate Bayesian computation is (a) efficient, (b) informative, and (c) offers a path to reproducible results.},
  copyright = {{\copyright} 2019 The Authors. Cognitive Science published by Wiley Periodicals, Inc. on behalf of Cognitive Science Society. All rights reserved},
  langid = {english},
  keywords = {Approximate Bayesian computation,Bayesian optimization,Cognitive models,Computational statistics,Inference,Machine learning,Parameter estimation}
}

@article{kangasraasioParameterInferenceComputational2019c,
  title = {Parameter {{Inference}} for {{Computational Cognitive Models}} with {{Approximate Bayesian Computation}}},
  author = {Kangasr{\"a}{\"a}si{\"o}, Antti and Jokinen, Jussi P. P. and Oulasvirta, Antti and Howes, Andrew and Kaski, Samuel},
  year = {2019},
  journal = {Cognitive Science},
  volume = {43},
  number = {6},
  pages = {e12738},
  issn = {1551-6709},
  doi = {10.1111/cogs.12738},
  urldate = {2025-01-06},
  abstract = {This paper addresses a common challenge with computational cognitive models: identifying parameter values that are both theoretically plausible and generate predictions that match well with empirical data. While computational models can offer deep explanations of cognition, they are computationally complex and often out of reach of traditional parameter fitting methods. Weak methodology may lead to premature rejection of valid models or to acceptance of models that might otherwise be falsified. Mathematically robust fitting methods are, therefore, essential to the progress of computational modeling in cognitive science. In this article, we investigate the capability and role of modern fitting methods---including Bayesian optimization and approximate Bayesian computation---and contrast them to some more commonly used methods: grid search and Nelder--Mead optimization. Our investigation consists of a reanalysis of the fitting of two previous computational models: an Adaptive Control of Thought---Rational model of skill acquisition and a computational rationality model of visual search. The results contrast the efficiency and informativeness of the methods. A key advantage of the Bayesian methods is the ability to estimate the uncertainty of fitted parameter values. We conclude that approximate Bayesian computation is (a) efficient, (b) informative, and (c) offers a path to reproducible results.},
  langid = {english},
  keywords = {Approximate Bayesian computation,Bayesian optimization,Cognitive models,Computational statistics,Inference,Machine learning,Parameter estimation}
}

@book{kantCritiquePureReason1998,
  title = {Critique of {{Pure Reason}}},
  author = {Kant, Immanuel},
  editor = {Guyer, Paul and Wood, Allen W.},
  year = {1998},
  publisher = {Cambridge University Press}
}

@misc{kingmaIntroductionVariationalAutoencoders2019,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2019},
  month = dec,
  number = {arXiv:1906.02691},
  eprint = {1906.02691},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1906.02691},
  urldate = {2024-11-27},
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/paultalma/Zotero/storage/Y6QWAQFY/Kingma and Welling (2019) - An Introduction to Variational Autoencoders.pdf;/Users/paultalma/Zotero/storage/CM6C7AWB/1906.html}
}

@article{kleinbergStochasticDiscrimination1990,
  title = {Stochastic Discrimination},
  author = {Kleinberg, E. M.},
  year = {1990},
  month = sep,
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {1},
  number = {1},
  pages = {207--239},
  issn = {1573-7470},
  doi = {10.1007/BF01531079},
  urldate = {2025-01-17},
  abstract = {A general method is introduced for separating points in multidimensional spaces through the use of stochastic processes. This technique is called stochastic discrimination.},
  langid = {english},
  keywords = {Artificial Intelligence,complexity theory,Pattern recognition},
  file = {/Users/paultalma/Zotero/storage/ZLFUHBH4/Kleinberg (1990) - Stochastic discrimination.pdf}
}

@article{knoxNatureBeliefDirectedExploratory2012,
  title = {The {{Nature}} of {{Belief-Directed Exploratory Choice}} in {{Human Decision-Making}}},
  author = {Knox, W. Bradley and Otto, A. Ross and Stone, Peter and Love, Bradley},
  year = {2012},
  month = jan,
  journal = {Frontiers in Psychology},
  volume = {2},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2011.00398},
  urldate = {2025-01-28},
  abstract = {{$<$}p{$>$}In non-stationary environments, there is a conflict between exploiting currently favored options and gaining information by exploring lesser-known options that in the past have proven less rewarding. Optimal decision-making in such tasks requires considering future states of the environment (i.e., planning) and properly updating beliefs about the state of the environment after observing outcomes associated with choices. Optimal belief-updating is {$<$}italic{$>$}reflective{$<$}/italic{$>$} in that beliefs can change without directly observing environmental change. For example, after 10 s elapse, one might correctly believe that a traffic light last observed to be red is now more likely to be green. To understand human decision-making when rewards associated with choice options change over time, we develop a variant of the classic ``bandit'' task that is both rich enough to encompass relevant phenomena and sufficiently tractable to allow for ideal actor analysis of sequential choice behavior. We evaluate whether people update beliefs about the state of environment in a {$<$}italic{$>$}reflexive{$<$}/italic{$>$} (i.e., only in response to observed changes in reward structure) or reflective manner. In contrast to purely ``random'' accounts of exploratory behavior, model-based analyses of the subjects' choices and latencies indicate that people are reflective belief updaters. However, unlike the Ideal Actor model, our analyses indicate that people's choice behavior does not reflect consideration of future environmental states. Thus, although people update beliefs in a reflective manner consistent with the Ideal Actor, they do not engage in optimal long-term planning, but instead myopically choose the option on every trial that is believed to have the highest immediate payoff.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Decision Making,exploitation,exploration,ideal actor,ideal observer,planning,POMDP,reinforcement learning},
  file = {/Users/paultalma/Zotero/storage/RGCQXALR/Knox et al. (2012) - The Nature of Belief-Directed Exploratory Choice in Human Decision-Making.pdf}
}

@incollection{kripkeChurchTuringThesisSpecial2013,
  title = {The {{Church-Turing}} ?{{Thesis}}? As a {{Special Corollary}} of {{G{\"o}del}}?S {{Completeness Theorem}}},
  shorttitle = {The {{Church-Turing}} ?},
  booktitle = {Computability: {{G{\"o}del}}, {{Turing}}, {{Church}}, and Beyond},
  author = {Kripke, Saul A.},
  editor = {Copeland, B. J. and Posy, C. and Shagrir, O.},
  year = {2013},
  publisher = {MIT Press},
  file = {/Users/paultalma/Zotero/storage/BP8VK3DG/KRIQAT.html}
}

@book{kripkeNamingNecessityLectures1980,
  title = {Naming and {{Necessity}}: {{Lectures Given}} to the {{Princeton University Philosophy Colloquium}}},
  author = {Kripke, Saul A.},
  editor = {Byrne, Darragh and K{\"o}lbel, Max},
  year = {1980},
  publisher = {Harvard University Press},
  address = {Cambridge, MA}
}

@misc{kurinCan$Q$LearningGraph2020,
  title = {Can \${{Q}}\$-{{Learning}} with {{Graph Networks Learn}} a {{Generalizable Branching Heuristic}} for a {{SAT Solver}}?},
  author = {Kurin, Vitaly and Godil, Saad and Whiteson, Shimon and Catanzaro, Bryan},
  year = {2020},
  month = nov,
  number = {arXiv:1909.11830},
  eprint = {1909.11830},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1909.11830},
  urldate = {2024-12-05},
  abstract = {We present Graph-\$Q\$-SAT, a branching heuristic for a Boolean SAT solver trained with value-based reinforcement learning (RL) using Graph Neural Networks for function approximation. Solvers using Graph-\$Q\$-SAT are complete SAT solvers that either provide a satisfying assignment or proof of unsatisfiability, which is required for many SAT applications. The branching heuristics commonly used in SAT solvers make poor decisions during their warm-up period, whereas Graph-\$Q\$-SAT is trained to examine the structure of the particular problem instance to make better decisions early in the search. Training Graph-\$Q\$-SAT is data efficient and does not require elaborate dataset preparation or feature engineering. We train Graph-\$Q\$-SAT using RL interfacing with MiniSat solver and show that Graph-\$Q\$-SAT can reduce the number of iterations required to solve SAT problems by 2-3X. Furthermore, it generalizes to unsatisfiable SAT instances, as well as to problems with 5X more variables than it was trained on. We show that for larger problems, reductions in the number of iterations lead to wall clock time reductions, the ultimate goal when designing heuristics. We also show positive zero-shot transfer behavior when testing Graph-\$Q\$-SAT on a task family different from that used for training. While more work is needed to apply Graph-\$Q\$-SAT to reduce wall clock time in modern SAT solving settings, it is a compelling proof-of-concept showing that RL equipped with Graph Neural Networks can learn a generalizable branching heuristic for SAT search.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/paultalma/Zotero/storage/ZEQF9S7F/Kurin et al. (2020) - Can $Q$-Learning with Graph Networks Learn a Generalizable Branching Heuristic for a SAT Solver.pdf;/Users/paultalma/Zotero/storage/WJ9Q8QXR/1909.html}
}

@article{kwisthoutBayesianIntractabilityNot2011,
  title = {Bayesian {{Intractability Is Not}} an {{Ailment That Approximation Can Cure}}},
  author = {Kwisthout, Johan and Wareham, Todd and {van Rooij}, Iris},
  year = {2011},
  journal = {Cognitive Science},
  volume = {35},
  number = {5},
  pages = {779--784},
  issn = {1551-6709},
  doi = {10.1111/j.1551-6709.2011.01182.x},
  urldate = {2024-11-08},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/AUJUYPPW/Kwisthout et al. (2011) - Bayesian Intractability Is Not an Ailment That Approximation Can Cure.pdf}
}

@article{lawlerModelExplanationModelInduced2021,
  title = {Model {{Explanation Versus Model-Induced Explanation}}},
  author = {Lawler, Insa and Sullivan, Emily},
  year = {2021},
  month = dec,
  journal = {Foundations of Science},
  volume = {26},
  number = {4},
  pages = {1049--1074},
  issn = {1572-8471},
  doi = {10.1007/s10699-020-09649-1},
  urldate = {2025-01-17},
  abstract = {Scientists appeal to models when explaining phenomena. Such explanations are often dubbed model explanations or model-based explanations (short: ME). But what are the precise conditions for ME? Are ME special explanations? In our paper, we first rebut two definitions of ME and specify a more promising one. Based on this analysis, we single out a related conception that is concerned with explanations that are induced from working with a model. We call them `model-induced explanations' (MIE). Second, we study three paradigmatic cases of alleged ME. We argue that all of them are MIE, upon closer examination. Third, we argue that this undermines the building consensus that model explanations are special explanations that, e.g., challenge the factivity of explanation. Instead, it suggests that what is special about models in science is the epistemology behind how models induce explanations.},
  langid = {english},
  keywords = {Explanation,Idealization,Model,Model-based Explanation},
  file = {/Users/paultalma/Zotero/storage/NL8VRQPZ/Lawler and Sullivan (2021) - Model Explanation Versus Model-Induced Explanation.pdf}
}

@article{lecunBackpropagationAppliedHandwritten1989,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year = {1989},
  month = dec,
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1989.1.4.541},
  urldate = {2024-11-04},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/IULESKZ2/LeCun et al. - 1989 - Backpropagation Applied to Handwritten Zip Code Recognition.pdf}
}

@article{lecunBackpropagationAppliedHandwritten1989b,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year = {1989},
  month = dec,
  journal = {Neural Computation},
  volume = {1},
  number = {4},
  pages = {541--551},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1989.1.4.541},
  urldate = {2024-11-06},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/RNWMSR9H/LeCun et al. (1989) - Backpropagation Applied to Handwritten Zip Code Recognition.pdf}
}

@inproceedings{lecunHandwrittenDigitRecognition1989,
  title = {Handwritten {{Digit Recognition}} with a {{Back-Propagation Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {LeCun, Yann and Boser, Bernhard and Denker, John and Henderson, Donnie and Howard, R. and Hubbard, Wayne and Jackel, Lawrence},
  year = {1989},
  volume = {2},
  publisher = {Morgan-Kaufmann},
  urldate = {2024-11-06},
  abstract = {We present an application of back-propagation networks to hand(cid:173) written digit recognition. Minimal preprocessing of the data was  required, but architecture of the network was highly constrained  and specifically designed for the task. The input of the network  consists of normalized images of isolated digits. The method has  1 \% error rate and about a 9\% reject rate on zipcode digits provided  by the U.S. Postal Service.},
  file = {/Users/paultalma/Zotero/storage/JJQJLQWD/LeCun et al. (1989) - Handwritten Digit Recognition with a Back-Propagation Network.pdf}
}

@article{leitgebStructuralJustificationProbabilism2021,
  title = {A {{Structural Justification}} of {{Probabilism}}: {{From Partition Invariance}} to {{Subjective Probability}}},
  shorttitle = {A {{Structural Justification}} of {{Probabilism}}},
  author = {Leitgeb, Hannes},
  year = {2021},
  month = apr,
  journal = {Philosophy of Science},
  volume = {88},
  number = {2},
  pages = {341--365},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/711570},
  urldate = {2024-11-18},
  abstract = {A new justification of probabilism is developed that pays close attention to the structure of the underlying space of possibilities. Its central assumption is that rational numerical degrees of belief ought to be partition invariant. By means of a representation theorem, one can prove that if graded belief satisfies the resulting set of postulates, rational degrees of belief may be identified with probabilities.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/ALE4PAI2/Leitgeb (2021) - A Structural Justification of Probabilism From Partition Invariance to Subjective Probability.pdf}
}

@article{leongDynamicInteractionReinforcement2017,
  title = {Dynamic {{Interaction}} between {{Reinforcement Learning}} and {{Attention}} in {{Multidimensional Environments}}},
  author = {Leong, Yuan Chang and Radulescu, Angela and Daniel, Reka and DeWoskin, Vivian and Niv, Yael},
  year = {2017},
  month = jan,
  journal = {Neuron},
  volume = {93},
  number = {2},
  pages = {451--463},
  publisher = {Elsevier},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2016.12.040},
  urldate = {2025-01-28},
  langid = {english},
  pmid = {28103483},
  keywords = {attention,computational modeling,decision making,fMRI,MVPA,prediction error,reinforcement learning,striatum,value,vmPFC},
  file = {/Users/paultalma/Zotero/storage/QG9PSQIG/Leong et al. (2017) - Dynamic Interaction between Reinforcement Learning and Attention in Multidimensional Environments.pdf}
}

@book{lewis2013counterfactuals,
  title = {Counterfactuals},
  author = {Lewis, David},
  year = {2013},
  publisher = {John Wiley \& Sons}
}

@article{liaoRewardLearningBiases2020,
  title = {Reward Learning Biases the Direction of Saccades},
  author = {Liao, Ming-Ray and Anderson, Brian A.},
  year = {2020},
  month = mar,
  journal = {Cognition},
  volume = {196},
  pages = {104145},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2019.104145},
  urldate = {2025-01-28},
  abstract = {The role of associative reward learning in guiding feature-based attention and spatial attention is well established. However, no studies have looked at the extent to which reward learning can modulate the direction of saccades during visual search. Here, we introduced a novel reward learning paradigm to examine whether reward-associated directions of eye movements can modulate performance in different visual search tasks. Participants had to fixate a peripheral target before fixating one of four disks that subsequently appeared in each cardinal position. This was followed by reward feedback contingent upon the direction chosen, where one direction consistently yielded a high reward. Thus, reward was tied to the direction of saccades rather than the absolute location of the stimulus fixated. Participants selected the target in the high-value direction on the majority of trials, demonstrating robust learning of the task contingencies. In an untimed visual foraging task that followed, which was performed in extinction, initial saccades were reliably biased in the previously rewarded-associated direction. In a second experiment, following the same training procedure, eye movements in the previously high-value direction were facilitated in a saccade-to-target task. Our findings suggest that rewarding directional eye movements biases oculomotor search patterns in a manner that is robust to extinction and generalizes across stimuli and task.},
  keywords = {Eye movements,Foraging,Overt attention,Reinforcement,Reward learning,Visual search},
  file = {/Users/paultalma/Zotero/storage/QTCETHRB/1-s2.0-S0010027719303191-main.pdf;/Users/paultalma/Zotero/storage/Q3886YSZ/S0010027719303191.html}
}

@article{liederResourcerationalAnalysisUnderstanding2020,
  title = {Resource-Rational Analysis: {{Understanding}} Human Cognition as the Optimal Use of Limited Computational Resources},
  shorttitle = {Resource-Rational Analysis},
  author = {Lieder, Falk and Griffiths, Thomas L.},
  year = {2020},
  journal = {Behavioral and Brain Sciences},
  volume = {43},
  pages = {e1},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X1900061X},
  urldate = {2024-12-06},
  abstract = {Modeling human cognition is challenging because there are infinitely many mechanisms that can generate any given observation. Some researchers address this by constraining the hypothesis space through assumptions about what the human mind can and cannot do, while others constrain it through principles of rationality and adaptation. Recent work in economics, psychology, neuroscience, and linguistics has begun to integrate both approaches by augmenting rational models with cognitive constraints, incorporating rational principles into cognitive architectures, and applying optimality principles to understanding neural representations. We identify the rational use of limited resources as a unifying principle underlying these diverse approaches, expressing it in a new cognitive modeling paradigm called resourcerational analysis. The integration of rational principles with realistic cognitive constraints makes resource-rational analysis a promising framework for reverse-engineering cognitive mechanisms and representations. It has already shed new light on the debate about human rationality and can be leveraged to revisit classic questions of cognitive psychology within a principled computational framework. We demonstrate that resource-rational models can reconcile the mind's most impressive cognitive skills with people's ostensive irrationality. Resource-rational analysis also provides a new way to connect psychological theory more deeply with artificial intelligence, economics, neuroscience, and linguistics.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/B3M8WH6U/Lieder and Griffiths (2020) - Resource-rational analysis Understanding human cognition as the optimal use of limited computationa.pdf}
}

@article{liLearningLearnFunctions2023,
  title = {Learning to {{Learn Functions}}},
  author = {Li, Michael Y. and Callaway, Fred and Thompson, William D. and Adams, Ryan P. and Griffiths, Thomas L.},
  year = {2023},
  journal = {Cognitive Science},
  volume = {47},
  number = {4},
  pages = {e13262},
  issn = {1551-6709},
  doi = {10.1111/cogs.13262},
  urldate = {2025-01-06},
  abstract = {Humans can learn complex functional relationships between variables from small amounts of data. In doing so, they draw on prior expectations about the form of these relationships. In three experiments, we show that people learn to adjust these expectations through experience, learning about the likely forms of the functions they will encounter. Previous work has used Gaussian processes---a statistical framework that extends Bayesian nonparametric approaches to regression---to model human function learning. We build on this work, modeling the process of learning to learn functions as a form of hierarchical Bayesian inference about the Gaussian process hyperparameters.},
  copyright = {{\copyright} 2023 The Authors. Cognitive Science published by Wiley Periodicals LLC on behalf of Cognitive Science Society (CSS).},
  langid = {english},
  keywords = {Bayesian nonparametrics,Function learning,Gaussian process,Hierarchical Bayesian models,Learning-to-learn}
}

@book{linneboThinObjectsAbstractionist2018,
  title = {Thin {{Objects}}: {{An Abstractionist Account}}},
  author = {Linnebo, {\O}ystein},
  year = {2018},
  publisher = {Oxford University Press},
  address = {Oxford}
}

@article{liuHowBeIndifferent,
  title = {How to Be Indifferent},
  author = {Liu, Sebastian},
  journal = {No{\^u}s},
  volume = {n/a},
  number = {n/a},
  issn = {1468-0068},
  doi = {10.1111/nous.12512},
  urldate = {2025-01-06},
  abstract = {According to the principle of indifference, when a set of possibilities is evidentially symmetric for you -- when your evidence no more supports any one of the possibilities over any other -- you're required to distribute your credences uniformly among them. Despite its intuitive appeal, the principle of indifference is often thought to be unsustainable due to the problem of multiple partitions: Depending on how a set of possibilities is divided, it seems that sometimes, applying indifference reasoning can require you to assign incompatible credences to equivalent possibilities. This paper defends the principle of indifference from the problem of multiple partitions by offering two guides for how to respond. The first is for permissivists about rationality, and is modeled on permissivists' arguments for the claim that a body of evidence sometimes does not uniquely determine a fully rational credence function. The second is for impermissivists about rationality, and is modeled on impermissivists' arguments for the claim that a body of evidence does always uniquely determine a fully rational credence function. What appears to be a decisive objection against the principle of indifference is in fact an instance of a general challenge taking different forms familiar to both permissivists and impermissivists.},
  copyright = {{\copyright} 2024 The Author(s). No{\^u}s published by Wiley Periodicals LLC.},
  langid = {english}
}

@article{luceChoiceAxiomTwenty1977a,
  title = {The Choice Axiom after Twenty Years},
  author = {Luce, R.Duncan},
  year = {1977},
  month = jun,
  journal = {Journal of Mathematical Psychology},
  volume = {15},
  number = {3},
  pages = {215--233},
  issn = {00222496},
  doi = {10.1016/0022-2496(77)90032-3},
  urldate = {2024-11-25},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/AXJ7MICK/Luce (1977) - The choice axiom after twenty years.pdf}
}

@incollection{macbrideJuliusCaesarObjection2006,
  title = {The {{Julius Caesar Objection}} : {{More Problematic Than Ever}}},
  booktitle = {Identity and {{Modality}}},
  author = {MacBride, Fraser},
  editor = {MacBride, Fraser},
  year = {2006},
  pages = {174},
  publisher = {Oxford University Press}
}

@article{mackBuildingConceptsOne2018,
  title = {Building Concepts One Episode at a Time: {{The}} Hippocampus and Concept Formation},
  shorttitle = {Building Concepts One Episode at a Time},
  author = {Mack, Michael L. and Love, Bradley C. and Preston, Alison R.},
  year = {2018},
  month = jul,
  journal = {Neuroscience Letters},
  series = {New {{Perspectives}} on the {{Hippocampus}} and {{Memory}}},
  volume = {680},
  pages = {31--38},
  issn = {0304-3940},
  doi = {10.1016/j.neulet.2017.07.061},
  urldate = {2025-01-28},
  abstract = {Concepts organize our experiences and allow for meaningful inferences in novel situations. Acquiring new concepts requires extracting regularities across multiple learning experiences, a process formalized in mathematical models of learning. These models posit a computational framework that has increasingly aligned with the expanding repertoire of functions associated with the hippocampus. Here, we propose the Episodes-to-Concepts (EpCon) theoretical model of hippocampal function in concept learning and review evidence for the hippocampal computations that support concept formation including memory integration, attentional biasing, and memory-based prediction error. We focus on recent studies that have directly assessed the hippocampal role in concept learning with an innovative approach that combines computational modeling and sophisticated neuroimaging measures. Collectively, this work suggests that the hippocampus does much more than encode individual episodes; rather, it adaptively transforms initially-encoded episodic memories into organized conceptual knowledge that drives novel behavior.},
  keywords = {Attention,Computational modeling,Concept learning,Episodic memory,Hippocampus,Prediction error},
  file = {/Users/paultalma/Zotero/storage/NZJ3KRW2/Mack et al. (2018) - Building concepts one episode at a time The hippocampus and concept formation.pdf;/Users/paultalma/Zotero/storage/BWQUUVIK/S030439401730647X.html}
}

@article{mameliEvaluationConceptInnateness2011,
  title = {An Evaluation of the Concept of Innateness},
  author = {Mameli, Matteo and Bateson, Patrick},
  year = {2011},
  month = feb,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {366},
  number = {1563},
  pages = {436--443},
  publisher = {Royal Society},
  doi = {10.1098/rstb.2010.0174},
  urldate = {2024-11-26},
  abstract = {The concept of innateness is often used in explanations and classifications of biological and cognitive traits. But does this concept have a legitimate role to play in contemporary scientific discourse? Empirical studies and theoretical developments have revealed that simple and intuitively appealing ways of classifying traits (e.g. genetically specified versus owing to the environment) are inadequate. They have also revealed a variety of scientifically interesting ways of classifying traits each of which captures some aspect of the innate/non-innate distinction. These include things such as whether a trait is canalized, whether it has a history of natural selection, whether it developed without learning or without a specific set of environmental triggers, whether it is causally correlated with the action of certain specific genes, etc. We offer an analogy: the term `jade' was once thought to refer to a single natural kind; it was then discovered that it refers to two different chemical compounds, jadeite and nephrite. In the same way, we argue, researchers should recognize that `innateness' refers not to a single natural kind but to a set of (possibly related) natural kinds. When this happens, it will be easier to progress in the field of biological and cognitive sciences.},
  keywords = {canalization,genetic information,innateness,learning,nativism,natural selection},
  file = {/Users/paultalma/Zotero/storage/HYRR4JE9/Mameli and Bateson (2011) - An evaluation of the concept of innateness.pdf}
}

@article{margolisDefenseNativism2013,
  title = {In Defense of Nativism},
  author = {Margolis, Eric and Laurence, Stephen},
  year = {2013},
  journal = {Philosophical Studies: An International Journal for Philosophy in the Analytic Tradition},
  volume = {165},
  number = {2},
  eprint = {42920527},
  eprinttype = {jstor},
  pages = {693--718},
  publisher = {Springer},
  issn = {0031-8116},
  urldate = {2024-11-26},
  abstract = {This paper takes a fresh look at the nativism-empiricism debate, presenting and defending a nativist perspective on the mind. Empiricism is often taken to be the default view both in philosophy and in cognitive science. This paper argues, on the contrary, that there should be no presumption in favor of empiricism (or nativism), but that the existing evidence suggests that nativism is the most promising framework for the scientific study of the mind. Our case on behalf of nativism has four parts. (1) We characterize nativism's core commitments relative to the contemporary debate between empiricists and nativists, (2) we present the positive case for nativism in terms of two central nativist arguments (the poverty of the stimulus argument and the argument from animals), (3) we respond to a number of influential objections to nativist theories, and (4) we explain the nativist approach to the conceptual system.},
  file = {/Users/paultalma/Zotero/storage/FR6JRS4Y/Margolis and Laurence (2013) - In defense of nativism.pdf}
}

@article{millerExplanationArtificialIntelligence2019a,
  title = {Explanation in Artificial Intelligence: {{Insights}} from the Social Sciences},
  shorttitle = {Explanation in Artificial Intelligence},
  author = {Miller, Tim},
  year = {2019},
  month = feb,
  journal = {Artificial Intelligence},
  volume = {267},
  pages = {1--38},
  issn = {00043702},
  doi = {10.1016/j.artint.2018.07.007},
  urldate = {2025-01-13},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/PPSGU6PP/Miller (2019) - Explanation in artificial intelligence Insights from the social sciences.pdf}
}

@article{mitchellExplanationBasedGeneralizationUnifying1986,
  title = {Explanation-{{Based Generalization}}: {{A Unifying View}}},
  shorttitle = {Explanation-{{Based Generalization}}},
  author = {Mitchell, Tom M. and Keller, Richard M. and {Kedar-Cabelli}, Smadar T.},
  year = {1986},
  month = mar,
  journal = {Machine Learning},
  volume = {1},
  number = {1},
  pages = {47--80},
  issn = {1573-0565},
  doi = {10.1023/A:1022691120807},
  urldate = {2025-01-16},
  abstract = {The problem of formulating general concepts from specific training examples has long been a major focus of machine learning research. While most previous research has focused on empirical methods for generalizing from a large number of training examples using no domain-specific knowledge, in the past few years new methods have been developed for applying domain-specific knowledge to formulate valid generalizations from single training examples. The characteristic common to these methods is that their ability to generalize from a single example follows from their ability to explain why the training example is a member of the concept being learned. This paper proposes a general, domain-independent mechanism, called EBG, that unifies previous approaches to explanation-based generalization. The EBG method is illustrated in the context of several example problems, and used to contrast several existing systems for explanation-based generalization. The perspective on explanation-based generalization afforded by this general method is also used to identify open research problems in this area.},
  langid = {english},
  keywords = {Artificial Intelligence,constraint back-propagation,explanation-based generalization,explanation-based learning,goal regression,operationalization,similarity-based generalization},
  file = {/Users/paultalma/Zotero/storage/TI5U4GTT/Mitchell et al. (1986) - Explanation-Based Generalization A Unifying View.pdf}
}

@article{mnihHumanlevelControlDeep2015,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  urldate = {2024-12-05},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science},
  file = {/Users/paultalma/Zotero/storage/BTU428N4/Mnih et al. (2015) - Human-level control through deep reinforcement learning.pdf}
}

@article{mnihHumanlevelControlDeep2015a,
  title = {Human-Level Control through Deep Reinforcement Learning},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  year = {2015},
  month = feb,
  journal = {Nature},
  volume = {518},
  number = {7540},
  pages = {529--533},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature14236},
  urldate = {2025-01-28},
  abstract = {An artificial agent is developed that learns to play~a diverse range of classic Atari 2600 computer games directly from sensory experience, achieving a~performance comparable to that of an expert human player; this work paves the way to building general-purpose learning algorithms that bridge the divide between perception and action.},
  copyright = {2015 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science},
  file = {/Users/paultalma/Zotero/storage/B3XZG54C/Mnih et al. (2015) - Human-level control through deep reinforcement learning.pdf}
}

@misc{morrisSteinsParadoxStatistics1977,
  title = {Stein's {{Paradox}} in {{Statistics}}},
  author = {Morris, Carl and Efron, Bradley},
  year = {1977},
  month = may,
  journal = {Scientific American},
  urldate = {2024-11-30},
  abstract = {The best guess about the future is usually obtained by computing the average of past events. Stein's paradox defines circumstances in which there are estimators better than the arithmetic average},
  howpublished = {https://www.scientificamerican.com/article/steins-paradox-in-statistics/},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/FHUVLRNN/Morris (1977) - Stein's Paradox in Statistics.pdf;/Users/paultalma/Zotero/storage/3DZQWGGM/steins-paradox-in-statistics.html}
}

@article{myungTutorialMaximumLikelihood2003,
  title = {Tutorial on Maximum Likelihood Estimation},
  author = {Myung, In Jae},
  year = {2003},
  month = feb,
  journal = {Journal of Mathematical Psychology},
  volume = {47},
  number = {1},
  pages = {90--100},
  issn = {0022-2496},
  doi = {10.1016/S0022-2496(02)00028-7},
  urldate = {2024-12-01},
  abstract = {In this paper, I provide a tutorial exposition on maximum likelihood estimation (MLE). The intended audience of this tutorial are researchers who practice mathematical modeling of cognition but are unfamiliar with the estimation method. Unlike least-squares estimation which is primarily a descriptive tool, MLE is a preferred method of parameter estimation in statistics and is an indispensable tool for many statistical modeling techniques, in particular in non-linear modeling with non-normal data. The purpose of this paper is to provide a good conceptual explanation of the method with illustrative examples so the reader can have a grasp of some of the basic principles.},
  file = {/Users/paultalma/Zotero/storage/H68KGYT3/S0022249602000287.html}
}

@article{nivEvolutionReinforcementLearning2002,
  title = {Evolution of Reinforcement Learning in Foraging Bees: A Simple Explanation for Risk Averse Behavior},
  shorttitle = {Evolution of Reinforcement Learning in Foraging Bees},
  author = {Niv, Yael and Joel, Daphna and Meilijson, Isaac and Ruppin, Eytan},
  year = {2002},
  month = jun,
  journal = {Neurocomputing},
  series = {Computational {{Neuroscience Trends}} in {{Research}} 2002},
  volume = {44--46},
  pages = {951--956},
  issn = {0925-2312},
  doi = {10.1016/S0925-2312(02)00496-4},
  urldate = {2025-01-28},
  abstract = {Reinforcement learning is a fundamental process by which organisms learn to achieve goals from their interactions with the environment. We use evolutionary computation techniques to derive (near-)optimal neuronal learning rules in a simple neural network model of decision-making in simulated bumblebees foraging for nectar. The resulting bees exhibit efficient reinforcement learning. The evolved synaptic plasticity dynamics give rise to varying exploration/exploitation levels and to the well-documented foraging strategy of risk aversion. This behavior is shown to emerge directly from optimal reinforcement learning, providing a biologically founded, parsimonious and novel explanation of risk-averse behavior.},
  keywords = {Bumble bees,Evolutionary computation,Exploration/exploitation tradeoff,Reinforcement learning,Risk aversion},
  file = {/Users/paultalma/Zotero/storage/LXNY6BXY/S0925231202004964.html}
}

@article{nivLearningTaskstateRepresentations2019,
  title = {Learning Task-State Representations},
  author = {Niv, Yael},
  year = {2019},
  month = oct,
  journal = {Nature Neuroscience},
  volume = {22},
  number = {10},
  pages = {1544--1553},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/s41593-019-0470-8},
  urldate = {2025-01-28},
  abstract = {Arguably, the most difficult part of learning is deciding what to learn about. Should I associate the positive outcome of safely completing a street-crossing with the situation `the car approaching the crosswalk was red' or with `the approaching car was slowing down'? In this Perspective, we summarize our recent research into the computational and neural underpinnings of `representation learning'---how humans (and other animals) construct task representations that allow efficient learning and decision-making. We first discuss the problem of learning what to ignore when confronted with too much information, so that experience can properly generalize across situations. We then turn to the problem of augmenting perceptual information with inferred latent causes that embody unobservable task-relevant information, such as contextual knowledge. Finally, we discuss recent findings regarding the neural substrates of task representations that suggest the orbitofrontal cortex represents `task states', deploying them for decision-making and learning elsewhere in the brain.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  langid = {english},
  keywords = {Attention,Classical conditioning,Decision,Operant learning,Psychology},
  file = {/Users/paultalma/Zotero/storage/FJNZINM8/Niv (2019) - Learning task-state representations.pdf}
}

@article{nivNeuralPredictionErrors2012,
  title = {Neural {{Prediction Errors Reveal}} a {{Risk-Sensitive Reinforcement-Learning Process}} in the {{Human Brain}}},
  author = {Niv, Yael and Edlund, Jeffrey A. and Dayan, Peter and O'Doherty, John P.},
  year = {2012},
  month = jan,
  journal = {Journal of Neuroscience},
  volume = {32},
  number = {2},
  pages = {551--562},
  publisher = {Society for Neuroscience},
  issn = {0270-6474, 1529-2401},
  doi = {10.1523/JNEUROSCI.5498-10.2012},
  urldate = {2025-01-28},
  abstract = {Humans and animals are exquisitely, though idiosyncratically, sensitive to risk or variance in the outcomes of their actions. Economic, psychological, and neural aspects of this are well studied when information about risk is provided explicitly. However, we must normally learn about outcomes from experience, through trial and error. Traditional models of such reinforcement learning focus on learning about the mean reward value of cues and ignore higher order moments such as variance. We used fMRI to test whether the neural correlates of human reinforcement learning are sensitive to experienced risk. Our analysis focused on anatomically delineated regions of a priori interest in the nucleus accumbens, where blood oxygenation level-dependent (BOLD) signals have been suggested as correlating with quantities derived from reinforcement learning. We first provide unbiased evidence that the raw BOLD signal in these regions corresponds closely to a reward prediction error. We then derive from this signal the learned values of cues that predict rewards of equal mean but different variance and show that these values are indeed modulated by experienced risk. Moreover, a close neurometric--psychometric coupling exists between the fluctuations of the experience-based evaluations of risky options that we measured neurally and the fluctuations in behavioral risk aversion. This suggests that risk sensitivity is integral to human learning, illuminating economic models of choice, neuroscientific models of affective learning, and the workings of the underlying neural mechanisms.},
  chapter = {Articles},
  copyright = {Copyright {\copyright} 2012 the authors 0270-6474/12/320551-12\$15.00/0},
  langid = {english},
  pmid = {22238090},
  file = {/Users/paultalma/Zotero/storage/MLWU2YKQ/Niv et al. (2012) - Neural Prediction Errors Reveal a Risk-Sensitive Reinforcement-Learning Process in the Human Brain.pdf}
}

@article{nivReinforcementLearningBrain,
  title = {Reinforcement Learning in the Brain},
  author = {Niv, Yael},
  abstract = {A wealth of research focuses on the decision-making processes that animals and humans employ when selecting actions in the face of reward and punishment. Initially such work stemmed from psychological investigations of conditioned behavior, and explanations of these in terms of computational models. Increasingly, analysis at the computational level has drawn on ideas from reinforcement learning, which provide a normative framework within which decision-making can be analyzed. More recently, the fruits of these extensive lines of research have made contact with investigations into the neural basis of decision making. Converging evidence now links reinforcement learning to specific neural substrates, assigning them precise computational roles. Specifically, electrophysiological recordings in behaving animals and functional imaging of human decision-making have revealed in the brain the existence of a key reinforcement learning signal, the temporal difference reward prediction error. Here, we first introduce the formal reinforcement learning framework. We then review the multiple lines of evidence linking reinforcement learning to the function of dopaminergic neurons in the mammalian midbrain and to more recent data from human imaging experiments. We further extend the discussion to aspects of learning not associated with phasic dopamine signals, such as learning of goal-directed responding that may not be dopamine-dependent, and learning about the vigor (or rate) with which actions should be performed that has been linked to tonic aspects of dopaminergic signaling. We end with a brief discussion of some of the limitations of the reinforcement learning framework, highlighting questions for future research.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/HMES3VLB/Niv Reinforcement learning in the brain.pdf}
}

@article{nivReinforcementLearningBrain2009,
  title = {Reinforcement Learning in the Brain},
  author = {Niv, Yael},
  year = {2009},
  month = jun,
  journal = {Journal of Mathematical Psychology},
  series = {Special {{Issue}}: {{Dynamic Decision Making}}},
  volume = {53},
  number = {3},
  pages = {139--154},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2008.12.005},
  urldate = {2025-01-28},
  abstract = {A wealth of research focuses on the decision-making processes that animals and humans employ when selecting actions in the face of reward and punishment. Initially such work stemmed from psychological investigations of conditioned behavior, and explanations of these in terms of computational models. Increasingly, analysis at the computational level has drawn on ideas from reinforcement learning, which provide a normative framework within which decision-making can be analyzed. More recently, the fruits of these extensive lines of research have made contact with investigations into the neural basis of decision making. Converging evidence now links reinforcement learning to specific neural substrates, assigning them precise computational roles. Specifically, electrophysiological recordings in behaving animals and functional imaging of human decision-making have revealed in the brain the existence of a key reinforcement learning signal, the temporal difference reward prediction error. Here, we first introduce the formal reinforcement learning framework. We then review the multiple lines of evidence linking reinforcement learning to the function of dopaminergic neurons in the mammalian midbrain and to more recent data from human imaging experiments. We further extend the discussion to aspects of learning not associated with phasic dopamine signals, such as learning of goal-directed responding that may not be dopamine-dependent, and learning about the vigor (or rate) with which actions should be performed that has been linked to tonic aspects of dopaminergic signaling. We end with a brief discussion of some of the limitations of the reinforcement learning framework, highlighting questions for future research.},
  file = {/Users/paultalma/Zotero/storage/SRGP254Q/Niv (2009) - Reinforcement learning in the brain.pdf;/Users/paultalma/Zotero/storage/GS3SFERX/S0022249608001181.html}
}

@article{nivReinforcementLearningMarr2016,
  title = {Reinforcement Learning with {{Marr}}},
  author = {Niv, Yael and Langdon, Angela},
  year = {2016},
  month = oct,
  journal = {Current Opinion in Behavioral Sciences},
  series = {Computational Modeling},
  volume = {11},
  pages = {67--73},
  issn = {2352-1546},
  doi = {10.1016/j.cobeha.2016.04.005},
  urldate = {2025-01-28},
  abstract = {To many, the poster child for David Marr's famous three levels of scientific inquiry is reinforcement learning --- a computational theory of reward optimization, which readily prescribes algorithmic solutions that evidence striking resemblance to signals found in the brain, suggesting a straightforward neural implementation. Here we review questions that remain open at each level of analysis, concluding that the path forward to their resolution calls for inspiration across levels, rather than a focus on mutual constraints.},
  file = {/Users/paultalma/Zotero/storage/VPAQQKF7/rl with marr.pdf;/Users/paultalma/Zotero/storage/IGIMA234/S2352154616300821.html}
}

@article{odohertyTemporalDifferenceModels2003,
  title = {Temporal Difference Models and Reward-Related Learning in the Human Brain},
  author = {O'Doherty, John P. and Dayan, Peter and Friston, Karl and Critchley, Hugo and Dolan, Raymond J.},
  year = {2003},
  journal = {Neuron},
  volume = {38},
  number = {2},
  pages = {329--337},
  publisher = {Elsevier},
  urldate = {2025-01-28},
  file = {/Users/paultalma/Zotero/storage/GL97QEHQ/O'Doherty et al. (2003) - Temporal difference models and reward-related learning in the human brain.pdf;/Users/paultalma/Zotero/storage/72MPTDYN/S0896-6273(03)00169-7.html}
}

@article{oneillExchangeabilityCorrelationBayes2009,
  title = {Exchangeability, {{Correlation}}, and {{Bayes}}' {{Effect}}},
  author = {O'Neill, Ben},
  year = {2009},
  month = aug,
  journal = {International Statistical Review},
  volume = {77},
  number = {2},
  pages = {241--250},
  issn = {0306-7734, 1751-5823},
  doi = {10.1111/j.1751-5823.2008.00059.x},
  urldate = {2024-10-24},
  abstract = {We examine the difference between Bayesian and frequentist statistics in making statements about the relationship between observable values. We show how standard models under both paradigms can be based on an assumption of exchangeability and we derive useful covariance and correlation results for values from an exchangeable sequence. We find that such values are never negatively correlated, and are generally positively correlated under the models used in Bayesian statistics. We discuss the significance of this result as well as a phenomenon which often follows from the differing methodologies and practical applications of these paradigms -- a phenomenon we call Bayes' effect.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/KQPZIQK7/O'Neill - 2009 - Exchangeability, Correlation, and Bayes' Effect.pdf}
}

@article{papayannopoulosAlgorithmsEffectiveProcedures2023,
  title = {On {{Algorithms}}, {{Effective Procedures}}, and {{Their Definitions}}},
  author = {Papayannopoulos, Philippos},
  year = {2023},
  month = oct,
  journal = {Philosophia Mathematica},
  volume = {31},
  number = {3},
  pages = {291--329},
  issn = {1744-6406},
  doi = {10.1093/philmat/nkad011},
  urldate = {2025-01-30},
  abstract = {I examine the classical idea of `algorithm' as a sequential, step-by-step, deterministic procedure (i.e., the idea of `algorithm' that was already in use by the 1930s), with respect to three themes, its relation to the notion of an `effective procedure', its different roles and uses in logic, computer science, and mathematics (focused on numerical analysis), and its different formal definitions proposed by practitioners in these areas. I argue that `algorithm' has been conceptualized and used in contrasting ways in the above areas, and discuss challenges and prospects for adopting a final foundational theory of (classical) `algorithms'.},
  file = {/Users/paultalma/Zotero/storage/ZBJEAPVI/Papayannopoulos (2023) - On Algorithms, Effective Procedures, and Their Definitions.pdf;/Users/paultalma/Zotero/storage/D4FU7V43/7233491.html}
}

@book{parsonsMathematicalThoughtIts2008,
  title = {Mathematical {{Thought}} and Its {{Objects}}},
  author = {Parsons, Charles},
  year = {2008},
  publisher = {Cambridge University Press},
  address = {New York}
}

@article{parsonsObjectsLogic1982,
  title = {Objects and {{Logic}}},
  author = {Parsons, Charles},
  year = {1982},
  journal = {The Monist},
  volume = {65},
  number = {4},
  pages = {491--516},
  publisher = {The Hegeler Institute},
  doi = {10.5840/monist198265437}
}

@book{peacockeStudyConcepts1992,
  title = {A {{Study}} of {{Concepts}}},
  author = {Peacocke, Christopher},
  year = {1992},
  publisher = {MIT Press}
}

@book{perryPersonalIdentity1975,
  title = {Personal {{Identity}}},
  editor = {Perry, John},
  year = {1975},
  publisher = {University of California Press},
  address = {Berkeley}
}

@article{pittWhenGoodFit2002,
  title = {When a Good Fit Can Be Bad},
  author = {Pitt, Mark A. and Myung, In Jae},
  year = {2002},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {6},
  number = {10},
  pages = {421--425},
  issn = {1364-6613},
  doi = {10.1016/S1364-6613(02)01964-2},
  urldate = {2024-12-01},
  abstract = {How should we select among computational models of cognition? Although it is commonplace to measure how well each model fits the data, this is insufficient. Good fits can be misleading because they can result from properties of the model that have nothing to do with it being a close approximation to the cognitive process of interest (e.g. overfitting). Selection methods are introduced that factor in these properties when measuring fit. Their success in outperforming standard goodness-of-fit measures stems from a focus on measuring the generalizability of a model's data-fitting abilities, which should be the goal of model selection.},
  keywords = {generalizability,goodness of fit,mathematical modeling,minimum description length,model complexity,model selection},
  file = {/Users/paultalma/Zotero/storage/A7G8L988/S1364661302019642.html}
}

@article{putnamMeaningMeaning1975,
  title = {The {{Meaning}} of '{{Meaning}}'},
  author = {Putnam, Hilary},
  year = {1975},
  journal = {Minnesota Studies in the Philosophy of Science},
  volume = {7},
  pages = {131--193},
  publisher = {University of Minnesota Press}
}

@article{putnamModelsReality1980,
  title = {Models and {{Reality}}},
  author = {Putnam, Hilary},
  year = {1980},
  journal = {Journal of Symbolic Logic},
  volume = {45},
  number = {3},
  pages = {464--482},
  publisher = {Cambridge University Press},
  doi = {10.2307/2273415}
}

@incollection{putnamModelsReality1983,
  title = {Models and {{Reality}}},
  booktitle = {Realism and Reason},
  author = {Putnam, Hilary},
  editor = {Putnam, Hilary},
  year = {1983},
  pages = {1--25},
  publisher = {Cambridge University Press}
}

@article{putnamRealismReason1977,
  title = {Realism and {{Reason}}},
  author = {Putnam, Hilary},
  year = {1977},
  journal = {Proceedings and Addresses of the American Philosophical Association},
  volume = {50},
  number = {6},
  pages = {483--498},
  publisher = {Monograph Collection (Matt - Pseudo)},
  doi = {10.2307/3129784}
}

@article{radulescuHolisticReinforcementLearning2019,
  title = {Holistic {{Reinforcement Learning}}: {{The Role}} of {{Structure}} and {{Attention}}},
  shorttitle = {Holistic {{Reinforcement Learning}}},
  author = {Radulescu, Angela and Niv, Yael and Ballard, Ian},
  year = {2019},
  month = apr,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {4},
  pages = {278--292},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2019.01.010},
  urldate = {2025-01-28},
  langid = {english},
  pmid = {30824227},
  keywords = {approximate inference,Bayesian inference,category learning,corticostriatal circuits,dopamine,representation learning,rule learning,striatum},
  file = {/Users/paultalma/Zotero/storage/C2PF42BK/Radulescu et al. (2019) - Holistic Reinforcement Learning The Role of Structure and Attention.pdf}
}

@article{radulescuHumanRepresentationLearning2021,
  title = {Human {{Representation Learning}}},
  author = {Radulescu, Angela and Shin, Yeon Soon and Niv, Yael},
  year = {2021},
  month = jul,
  journal = {Annual Review of Neuroscience},
  volume = {44},
  number = {1},
  pages = {253--273},
  issn = {0147-006X, 1545-4126},
  doi = {10.1146/annurev-neuro-092920-120559},
  urldate = {2025-01-28},
  abstract = {The central theme of this review is the dynamic interaction between information selection and learning. We pose a fundamental question about this interaction: How do we learn what features of our experiences are worth learning about? In humans, this process depends on attention and memory, two cognitive functions that together constrain representations of the world to features that are relevant for goal attainment. Recent evidence suggests that the representations shaped by attention and memory are themselves inferred from experience with each task. We review this evidence and place it in the context of work that has explicitly characterized representation learning as statistical inference. We discuss how inference can be scaled to real-world decisions by approximating beliefs based on a small number of experiences. Finally, we highlight some implications of this inference process for human decision-making in social environments.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/FF8IFNJD/Radulescu et al. (2021) - Human Representation Learning.pdf}
}

@incollection{rayoNeoFregeanismReconsidered2016,
  title = {Neo-{{Fregeanism Reconsidered}}},
  booktitle = {Abstractionism},
  author = {Rayo, August{\'i}n},
  editor = {Ebert, Philip A. and Rossberg, Marcus},
  year = {2016},
  publisher = {Oxford University Press}
}

@article{razUnderstandingDeepLearning2022,
  title = {Understanding {{Deep Learning}} with {{Statistical Relevance}}},
  author = {R{\"a}z, Tim},
  year = {2022},
  month = jan,
  journal = {Philosophy of Science},
  volume = {89},
  number = {1},
  pages = {20--41},
  issn = {0031-8248, 1539-767X},
  doi = {10.1017/psa.2021.12},
  urldate = {2025-01-17},
  abstract = {This paper argues that a notion of statistical explanation, based on Salmon's statistical relevance model, can help us better understand deep neural networks. It is proved that homogeneous partitions, the core notion of Salmon's model, are equivalent to minimal sufficient statistics, an important notion from statistical inference. This establishes a link to deep neural networks via the so-called Information Bottleneck method, an information-theoretic framework, according to which deep neural networks implicitly solve an optimization problem that generalizes minimal sufficient statistics. The resulting notion of statistical explanation is general, mathematical, and subcausal.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/LNXV3PDH/Räz (2022) - Understanding Deep Learning with Statistical Relevance.pdf}
}

@article{reichmanMachineLearningModeling2024,
  title = {Machine Learning for Modeling Human Decisions},
  author = {Reichman, Daniel and Peterson, Joshua C. and Griffiths, Thomas L.},
  year = {2024},
  journal = {Decision},
  volume = {11},
  number = {4},
  pages = {619--632},
  publisher = {Educational Publishing Foundation},
  address = {US},
  issn = {2325-9973},
  doi = {10.1037/dec0000242},
  abstract = {The rapid development of machine learning has led to new opportunities for applying these methods to the study of human decision making. We highlight some of these opportunities and discuss some of the issues that arise when using machine learning to model the decisions people make. We first elaborate on the relationship between predicting decisions and explaining them, leveraging findings from computational learning theory to argue that, in some cases, the conversion of predictive models to interpretable ones with comparable accuracy is an intractable problem. We then identify an important bottleneck in using machine learning to study human cognition---data scarcity---and highlight active learning and optimal experimental design as a way to move forward. Finally, we touch on additional topics such as machine learning methods for combining multiple predictors arising from known theories and specific machine learning architectures that could prove useful for the study of judgment and decision making. In doing so, we point out connections to behavioral economics, computer science, cognitive science, and psychology. (PsycInfo Database Record (c) 2024 APA, all rights reserved)},
  keywords = {Decision Making,Machine Learning,Simulation},
  file = {/Users/paultalma/Zotero/storage/GBVFCD8G/2025-38026-003.html}
}

@inbook{rescorlaNeuralImplementationApproximate2023,
  title = {Neural {{Implementation}} of ({{Approximate}}) {{Bayesian Inference}}},
  booktitle = {Expected {{Experiences}}},
  author = {Rescorla, Michael},
  year = {2023},
  month = dec,
  edition = {1},
  pages = {197--239},
  publisher = {Routledge},
  address = {New York},
  doi = {10.4324/9781003084082-11},
  urldate = {2024-11-08},
  collaborator = {Cheng, Tony and Sato, Ryoji and Hohwy, Jakob},
  isbn = {978-1-00-308408-2},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/KAA9F4II/Rescorla (2023) - Neural Implementation of (Approximate) Bayesian Inference.pdf}
}

@incollection{rescorlaReifyingRepresentations2020,
  title = {Reifying {{Representations}}},
  booktitle = {What Are {{Mental Representations}}?},
  author = {Rescorla, Michael},
  year = {2020},
  publisher = {Oxford University Press},
  doi = {10.1093/oso/9780190686673.003.0006},
  abstract = {The representational theory of mind (RTM) holds that the mind is stocked with mental representations: mental items that represent. They can be stored in memory, manipulated during mental activity, and combined to form complex representations. RTM is widely presupposed within cognitive science, which offers many successful theories that cite mental representations. Nevertheless, mental representations are still viewed warily in some scientific and philosophical circles. This chapter develops a novel version of RTM: the capacities-based representational theory of mind (C-RTM). According to C-RTM, a mental representation is an abstract type that marks the exercise of a representational capacity. Talk about mental representations embodies an ontologically loaded way of classifying mental states through representational capacities that the states deploy. Complex mental representations mark the appropriate joint exercise of multiple representational capacities. The chapter supports C-RTM with examples drawn from cognitive science, including perceptual representations and cognitive maps, and applies C-RTM to long-standing debates over the existence, nature, individuation, structure, and explanatory role of mental representations.},
  isbn = {978-0-19-068667-3}
}

@article{richHowIntractabilitySpans2020,
  title = {How {{Intractability Spans}} the {{Cognitive}} and {{Evolutionary Levels}} of {{Explanation}}},
  author = {Rich, Patricia and Blokpoel, Mark and {de Haan}, Ronald and {van Rooij}, Iris},
  year = {2020},
  journal = {Topics in Cognitive Science},
  volume = {12},
  number = {4},
  pages = {1382--1402},
  issn = {1756-8765},
  doi = {10.1111/tops.12506},
  urldate = {2025-01-06},
  abstract = {The challenge of explaining how cognition can be tractably realized is widely recognized. Classical rationality is thought to be intractable due to its assumptions of optimization and/or domain generality, and proposed solutions therefore drop one or both of these assumptions. We consider three such proposals: Resource-Rationality, the Adaptive Toolbox theory, and Massive Modularity. All three seek to ensure the tractability of cognition by shifting part of the explanation from the cognitive to the evolutionary level: Evolution is responsible for producing the tractable architecture. We consider the three proposals and show that, in each case, the intractability challenge is not thereby resolved, but only relocated from the cognitive level to the evolutionary level. We explain how non-classical accounts do not currently have the upper hand on the new playing field.},
  copyright = {{\copyright} 2020 The Authors. Topics in Cognitive Science published by Wiley Periodicals LLC on behalf of Cognitive Science Society},
  langid = {english},
  keywords = {Evolution,Heuristics,Intractability,Levels of explanation,Modularity,Rationality},
  file = {/Users/paultalma/Zotero/storage/2D9EYTHZ/Rich et al. (2020) - How Intractability Spans the Cognitive and Evolutionary Levels of Explanation.pdf;/Users/paultalma/Zotero/storage/CWVYJBI7/tops.html}
}

@article{roadsDimensionsDimensionality2024,
  title = {The {{Dimensions}} of Dimensionality},
  author = {Roads, Brett D. and Love, Bradley C.},
  year = {2024},
  month = dec,
  journal = {Trends in Cognitive Sciences},
  volume = {28},
  number = {12},
  pages = {1118--1131},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2024.07.005},
  urldate = {2025-01-28},
  abstract = {Cognitive scientists often infer multidimensional representations from data. Whether the data involve text, neuroimaging, neural networks, or human judgments, researchers frequently infer and analyze latent representational spaces (i.e., embeddings). However, the properties of a latent representation (e.g., prediction performance, interpretability, compactness) depend on the inference procedure, which can vary widely across endeavors. For example, dimensions are not always globally interpretable and the dimensionality of different embeddings may not be readily comparable. Moreover, the dichotomy between multidimensional spaces and purportedly richer representational formats, such as graph representations, is misleading. We review what the different notions of dimension in cognitive science imply for how these latent representations should be used and interpreted.},
  keywords = {dimensionality,dimensions,embedding,latent representations,manifold}
}

@book{RoutledgeHandbookPhilosophy,
  title = {The {{Routledge Handbook}} of {{Philosophy}} of {{Skill}} and {{Expertise}}},
  file = {/Users/paultalma/Zotero/storage/IGSGD9JK/_.pdf}
}

@book{russellPrinciplesMathematics1903,
  title = {Principles of {{Mathematics}}},
  author = {Russell, Bertrand},
  year = {1903},
  publisher = {Routledge},
  address = {New York,}
}

@article{russellProvablyBoundedoptimalAgents1995,
  title = {Provably Bounded-Optimal Agents},
  author = {Russell, Stuart J. and Subramanian, Devika},
  year = {1995},
  month = jan,
  journal = {J. Artif. Int. Res.},
  volume = {2},
  number = {1},
  pages = {575--609},
  issn = {1076-9757},
  abstract = {Since its inception, artificial intelligence has relied upon a theoretical foundation centred around perfect rationality as the desired property of intelligent systems. We argue, as others have done, that this foundation is inadequate because it imposes fundamentally unsatisfiable requirements. As a result, there has arisen a wide gap between theory and practice in AI, hindering progress in the field. We propose instead a property called bounded optimality. Roughly speaking, an agent is bounded-optimal if its program is a solution to the constrained optimization problem presented by its architecture and the task environment. We show how to construct agents with this property for a simple class of machine architectures in a broad class of real-time environments. We illustrate these results using a simple model of an automated mail sorting facility. We also define a weaker property, asymptotic bounded optimality (ABO), that generalizes the notion of optimality in classical complexity theory. We then construct universal ABO programs, i.e., programs that are ABO no matter what real-time constraints are applied. Universal ABO programs can be used as building blocks for more complex systems. We conclude with a discussion of the prospects for bounded optimality as a theoretical basis for AI, and relate it to similar trends in philosophy, economics, and game theory.},
  file = {/Users/paultalma/Zotero/storage/XZDPFE9B/Russell and Subramanian (1995) - Provably Bounded-Optimal Agents.pdf}
}

@misc{russellProvablyBoundedOptimalAgents1995a,
  title = {Provably {{Bounded-Optimal Agents}}},
  author = {Russell, S. J. and Subramanian, D.},
  year = {1995},
  month = may,
  number = {arXiv:cs/9505103},
  eprint = {cs/9505103},
  publisher = {arXiv},
  doi = {10.48550/arXiv.cs/9505103},
  urldate = {2025-01-31},
  abstract = {Since its inception, artificial intelligence has relied upon a theoretical foundation centered around perfect rationality as the desired property of intelligent systems. We argue, as others have done, that this foundation is inadequate because it imposes fundamentally unsatisfiable requirements. As a result, there has arisen a wide gap between theory and practice in AI, hindering progress in the field. We propose instead a property called bounded optimality. Roughly speaking, an agent is bounded-optimal if its program is a solution to the constrained optimization problem presented by its architecture and the task environment. We show how to construct agents with this property for a simple class of machine architectures in a broad class of real-time environments. We illustrate these results using a simple model of an automated mail sorting facility. We also define a weaker property, asymptotic bounded optimality (ABO), that generalizes the notion of optimality in classical complexity theory. We then construct universal ABO programs, i.e., programs that are ABO no matter what real-time constraints are applied. Universal ABO programs can be used as building blocks for more complex systems. We conclude with a discussion of the prospects for bounded optimality as a theoretical basis for AI, and relate it to similar trends in philosophy, economics, and game theory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/paultalma/Zotero/storage/ZKJJTUN5/9505103.html}
}

@inproceedings{sanbornRepresentationalEfficiencyOutweighs2018,
  title = {{Representational efficiency outweighs action efficiency in human program induction}},
  booktitle = {{Proceedings of the 40th Annual Meeting of the Cognitive Science Society, CogSci 2018}},
  author = {Sanborn, Sophia and Bourgin, David D. and Chang, Michael and Griffiths, Thomas L.},
  year = {2018},
  pages = {2400--2405},
  publisher = {The Cognitive Science Society},
  urldate = {2025-01-28},
  langid = {English (US)},
  file = {/Users/paultalma/Zotero/storage/Z63RR5SK/Sanborn et al. Representational efﬁciency outweighs action efﬁciency in human program induction.pdf;/Users/paultalma/Zotero/storage/SXW69Q86/representational-efficiency-outweighs-action-efficiency-in-human-.html}
}

@article{sanbornRepresentationalEfficiencyOutweighs2018a,
  title = {Representational Efficiency Outweighs Action Efficiency in Human Program Induction},
  author = {Sanborn, Sophia and Bourgin, David D and Chang, Michael and Griffiths, Thomas L},
  year = {2018},
  abstract = {The importance of hierarchically structured representations for tractable planning has long been acknowledged. However, the questions of how people discover such abstractions and how to define a set of optimal abstractions remain open. This problem has been explored in cognitive science in the problem solving literature and in computer science in hierarchical reinforcement learning. Here, we emphasize an algorithmic perspective on learning hierarchical representations in which the objective is to efficiently encode the structure of the problem, or, equivalently, to learn an algorithm with minimal length. We introduce a novel problem-solving paradigm that links problem solving and program induction under the Markov Decision Process (MDP) framework. Using this task, we target the question of whether humans discover hierarchical solutions by maximizing efficiency in number of actions they generate or by minimizing the complexity of the resulting representation and find evidence for the primacy of representational efficiency.},
  langid = {english}
}

@article{schrittwieserMasteringAtariGo2020,
  title = {Mastering {{Atari}}, {{Go}}, Chess and Shogi by Planning with a Learned Model},
  author = {Schrittwieser, Julian and Antonoglou, Ioannis and Hubert, Thomas and Simonyan, Karen and Sifre, Laurent and Schmitt, Simon and Guez, Arthur and Lockhart, Edward and Hassabis, Demis and Graepel, Thore and Lillicrap, Timothy and Silver, David},
  year = {2020},
  month = dec,
  journal = {Nature},
  volume = {588},
  number = {7839},
  pages = {604--609},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-03051-4},
  urldate = {2025-01-28},
  abstract = {Constructing agents with planning capabilities has long been one of the main challenges in the pursuit of artificial intelligence. Tree-based planning methods have enjoyed huge success in challenging domains, such as chess1 and Go2, where a perfect simulator is available. However, in real-world problems, the dynamics governing the environment are often complex and unknown. Here we present the MuZero algorithm, which, by combining a tree-based search with a learned model, achieves superhuman performance in a range of challenging and visually complex domains, without any knowledge of their underlying dynamics. The MuZero algorithm learns an iterable model that produces predictions relevant to planning: the action-selection policy, the value function and the reward. When evaluated on 57 different Atari games3---the canonical video game environment for testing artificial intelligence techniques, in which model-based planning approaches have historically struggled4---the MuZero algorithm achieved state-of-the-art performance. When evaluated on Go, chess and shogi---canonical environments for high-performance planning---the MuZero algorithm matched, without any knowledge of the game dynamics, the superhuman performance of the AlphaZero algorithm5 that was supplied with the rules of the game.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational science,Computer science},
  file = {/Users/paultalma/Zotero/storage/KCN2N9MK/Schrittwieser et al. (2020) - Mastering Atari, Go, chess and shogi by planning with a learned model.pdf}
}

@article{schultzNeuralSubstratePrediction1997,
  title = {A {{Neural Substrate}} of {{Prediction}} and {{Reward}}},
  author = {Schultz, Wolfram and Dayan, Peter and Montague, P. Read},
  year = {1997},
  month = mar,
  journal = {Science},
  volume = {275},
  number = {5306},
  pages = {1593--1599},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.275.5306.1593},
  urldate = {2025-01-28},
  abstract = {The capacity to predict future events permits a creature to detect, model, and manipulate the causal structure of its interactions with its environment. Behavioral experiments suggest that learning is driven by changes in the expectations about future salient events such as rewards and punishments. Physiological work has recently complemented these studies by identifying dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events. Taken together, these findings can be understood through quantitative theories of adaptive optimizing control.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/HUFDKMJ2/Schultz et al. (1997) - A Neural Substrate of Prediction and Reward.pdf}
}

@article{shapiroFregeMeetsDedekind2000,
  title = {Frege {{Meets Dedekind}}: {{A Neologicist Treatment}} of {{Real Analysis}}},
  author = {Shapiro, Stewart},
  year = {2000},
  journal = {Notre Dame Journal of Formal Logic},
  volume = {41},
  number = {4},
  pages = {335--364},
  publisher = {Duke University Press},
  doi = {10.1305/ndjfl/1038336880}
}

@article{silverMasteringGameGo2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  urldate = {2025-01-28},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  copyright = {2016 Springer Nature Limited},
  langid = {english},
  keywords = {Computational science,Computer science,Reward},
  file = {/Users/paultalma/Zotero/storage/TH3IB7VY/Silver et al. (2016) - Mastering the game of Go with deep neural networks and tree search.pdf}
}

@book{sober2015ockham,
  title = {Ockham's Razors: {{A User}}'s {{Manual}}},
  author = {Sober, Elliott},
  year = {2015},
  publisher = {Cambridge University Press}
}

@article{soberLikelihoodModelSelection2004,
  title = {Likelihood, {{Model Selection}}, and the {{Duhem-Quine Problem}}},
  author = {Sober, Elliott},
  year = {2004},
  journal = {The Journal of Philosophy},
  volume = {101},
  number = {5},
  eprint = {3655662},
  eprinttype = {jstor},
  pages = {221--241},
  publisher = {Journal of Philosophy, Inc.},
  issn = {0022-362X},
  urldate = {2024-11-25},
  file = {/Users/paultalma/Zotero/storage/S485I36Y/Sober (2004) - Likelihood, Model Selection, and the Duhem-Quine Problem.pdf}
}

@incollection{soberWhyLikelihood2010,
  title = {Why {{Likelihood}}?},
  booktitle = {The {{Nature}} of {{Scientific Evidence}}: {{Statistical}}, {{Philosophical}}, and {{Empirical Considerations}}},
  author = {Sober, Elliott and Forster, Malcolm},
  year = {2010},
  urldate = {2024-11-22},
  keywords = {Statistics},
  file = {/Users/paultalma/Zotero/storage/7CHKDQTT/Forster and Sober 2010 - Why Likelihood (1).pdf.pdf}
}

@article{sprengerScienceParametricModels2011,
  title = {Science without (Parametric) Models: The Case of Bootstrap Resampling},
  shorttitle = {Science without (Parametric) Models},
  author = {Sprenger, Jan},
  year = {2011},
  month = may,
  journal = {Synthese},
  volume = {180},
  number = {1},
  pages = {65--76},
  issn = {1573-0964},
  doi = {10.1007/s11229-009-9567-z},
  urldate = {2024-11-30},
  abstract = {Scientific and statistical inferences build heavily on explicit, parametric models, and often with good reasons. However, the limited scope of parametric models and the increasing complexity of the studied systems in modern science raise the risk of model misspecification. Therefore, I examine alternative, data-based inference techniques, such as bootstrap resampling. I argue that their neglect in the philosophical literature is unjustified: they suit some contexts of inquiry much better and use a more direct approach to scientific inference. Moreover, they make more parsimonious assumptions and often replace theoretical understanding and knowledge about mechanisms by careful experimental design. Thus, it is worthwhile to study in detail how nonparametric models serve as inferential engines in science.},
  langid = {english},
  keywords = {Bootstrap resampling,Data,Inductive inference,Models,Nonparametric statistics},
  file = {/Users/paultalma/Zotero/storage/6N9TKXCQ/Sprenger (2011) - Science without (parametric) models the case of bootstrap resampling.pdf}
}

@book{stalnakerInquiry1984,
  title = {Inquiry},
  author = {Stalnaker, Robert C.},
  year = {1984},
  publisher = {Cambridge University Press}
}

@incollection{stalnakerPropositions1976,
  title = {Propositions},
  booktitle = {Issues in the Philosophy of Language: Proceedings of the 1972 {{Oberlin Colloquium}} in {{Philosophy}}},
  author = {Stalnaker, Robert C.},
  editor = {Mackay, Alfred F. and Merrill, Daniel Davy},
  year = {1976},
  pages = {79--91},
  publisher = {Yale University Press}
}

@article{stojnicJustWordsIntentions2021,
  title = {Just {{Words}}: {{Intentions}}, {{Tolerance}} and {{Lexical Selection}}},
  author = {Stojni{\'c}, Una},
  year = {2021},
  journal = {Philosophy and Phenomenological Research},
  volume = {105},
  number = {1},
  pages = {3--17},
  doi = {10.1111/phpr.12781}
}

@article{stroblWhatFormalLanguages2024,
  title = {What {{Formal Languages Can Transformers Express}}? {{A Survey}}},
  shorttitle = {What {{Formal Languages Can Transformers Express}}?},
  author = {Strobl, Lena and Merrill, William and Weiss, Gail and Chiang, David and Angluin, Dana},
  year = {2024},
  month = may,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {12},
  pages = {543--561},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00663},
  urldate = {2024-11-11},
  abstract = {As transformers have gained prominence in natural language processing, some researchers have investigated theoretically what problems they can and cannot solve, by treating problems as formal languages. Exploring such questions can help clarify the power of transformers relative to other models of computation, their fundamental capabilities and limits, and the impact of architectural choices. Work in this subarea has made considerable progress in recent years. Here, we undertake a comprehensive survey of this work, documenting the diverse assumptions that underlie different results and providing a unified framework for harmonizing seemingly contradictory findings.},
  file = {/Users/paultalma/Zotero/storage/7MXZ64PY/Strobl et al. (2024) - What Formal Languages Can Transformers Express A Survey.pdf;/Users/paultalma/Zotero/storage/L4QGX3BF/What-Formal-Languages-Can-Transformers-Express-A.html}
}

@misc{sucholutskyGettingAlignedRepresentational2024,
  title = {Getting Aligned on Representational Alignment},
  author = {Sucholutsky, Ilia and Muttenthaler, Lukas and Weller, Adrian and Peng, Andi and Bobu, Andreea and Kim, Been and Love, Bradley C. and Cueva, Christopher J. and Grant, Erin and Groen, Iris and Achterberg, Jascha and Tenenbaum, Joshua B. and Collins, Katherine M. and Hermann, Katherine L. and Oktar, Kerem and Greff, Klaus and Hebart, Martin N. and Cloos, Nathan and Kriegeskorte, Nikolaus and Jacoby, Nori and Zhang, Qiuyi and Marjieh, Raja and Geirhos, Robert and Chen, Sherol and Kornblith, Simon and Rane, Sunayana and Konkle, Talia and O'Connell, Thomas P. and Unterthiner, Thomas and Lampinen, Andrew K. and M{\"u}ller, Klaus-Robert and Toneva, Mariya and Griffiths, Thomas L.},
  year = {2024},
  month = nov,
  number = {arXiv:2310.13018},
  eprint = {2310.13018},
  primaryclass = {q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.13018},
  urldate = {2025-01-28},
  abstract = {Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. How can we measure the similarity between the representations formed by these diverse systems? Do similarities in representations then translate into similar behavior? If so, then how can a system's representations be modified to better match those of another system? These questions pertaining to the study of representational alignment are at the heart of some of the most promising research areas in contemporary cognitive science, neuroscience, and machine learning. In this Perspective, we survey the exciting recent developments in representational alignment research in the fields of cognitive science, neuroscience, and machine learning. Despite their overlapping interests, there is limited knowledge transfer between these fields, so work in one field ends up duplicated in another, and useful innovations are not shared effectively. To improve communication, we propose a unifying framework that can serve as a common language for research on representational alignment, and map several streams of existing work across fields within our framework. We also lay out open problems in representational alignment where progress can benefit all three of these fields. We hope that this paper will catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/paultalma/Zotero/storage/E9JYH6JT/Sucholutsky et al. (2024) - Getting aligned on representational alignment.pdf;/Users/paultalma/Zotero/storage/CIG5JE28/2310.html}
}

@article{sullivanMachineLearningModels2024,
  title = {Do {{Machine Learning Models Represent Their Targets}}?},
  author = {Sullivan, Emily},
  year = {2024},
  month = dec,
  journal = {Philosophy of Science},
  volume = {91},
  number = {5},
  pages = {1445--1455},
  issn = {0031-8248, 1539-767X},
  doi = {10.1017/psa.2023.151},
  urldate = {2025-01-16},
  abstract = {I argue that machine learning (ML) models used in science function as highly idealized toy models. If we treat ML models as a type of highly idealized toy model, then we can deploy standard representational and epistemic strategies from the toy model literature to explain why ML models can still provide epistemic success despite their lack of similarity to their targets.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/4SLW3G82/Sullivan (2024) - Do Machine Learning Models Represent Their Targets.pdf}
}

@article{sullivanUnderstandingMachineLearning2022,
  title = {Understanding from {{Machine Learning Models}}},
  author = {Sullivan, Emily},
  year = {2022},
  month = mar,
  journal = {The British Journal for the Philosophy of Science},
  volume = {73},
  number = {1},
  pages = {109--133},
  publisher = {The University of Chicago Press},
  issn = {0007-0882},
  doi = {10.1093/bjps/axz035},
  urldate = {2025-01-16},
  abstract = {Simple idealized models seem to provide more understanding than opaque, complex, and hyper-realistic models. However, an increasing number of scientists are going in the opposite direction by utilizing opaque machine learning models to make predictions and draw inferences, suggesting that scientists are opting for models that have less potential for understanding. Are scientists trading understanding for some other epistemic or pragmatic good when they choose a machine learning model? Or are the assumptions behind why minimal models provide understanding misguided? In this article, using the case of deep neural networks, I argue that it is not the complexity or black box nature of a model that limits how much understanding the model provides. Instead, it is a lack of scientific and empirical evidence supporting the link that connects a model to the target phenomenon that primarily prohibits understanding.},
  file = {/Users/paultalma/Zotero/storage/AGT85TKS/Sullivan (2022) - Understanding from Machine Learning Models.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  lccn = {Q325.6 .R45 2018},
  keywords = {Reinforcement learning}
}

@article{talagrandNewLookIndependence1996,
  title = {A New Look at Independence},
  author = {Talagrand, Michel},
  year = {1996},
  month = jan,
  journal = {The Annals of Probability},
  volume = {24},
  number = {1},
  pages = {1--34},
  publisher = {Institute of Mathematical Statistics},
  issn = {0091-1798, 2168-894X},
  doi = {10.1214/aop/1042644705},
  urldate = {2024-11-26},
  abstract = {The concentration of measure phenomenon is product spaces is a far-reaching abstract generalization of the classical exponential inequalities for sums of independent random variables. We attempt to explain in the simplest possible terms the basic concepts underlying this phenomenon, the basic method to prove concentration inequalities and the meaning of several of the most useful inequalities.},
  file = {/Users/paultalma/Zotero/storage/J7GEQY56/Talagrand (1996) - A new look at independence.pdf}
}

@article{thagardPhilosophyMachineLearning1990,
  title = {Philosophy and {{Machine Learning}}},
  author = {Thagard, Paul},
  year = {1990},
  month = jun,
  journal = {Canadian Journal of Philosophy},
  volume = {20},
  number = {2},
  pages = {261--276},
  issn = {0045-5091, 1911-0820},
  doi = {10.1080/00455091.1990.10717218},
  urldate = {2024-11-04},
  abstract = {Philosophers since the ancient Greeks have investigated the nature of different kinds of inference. Although deductive inference in the form of Aristotelian syllogisms and Fregean formal logic has predominated, much attention has also been paid to               induction,               inference where the conclusion does not follow necessarily from the premises.},
  copyright = {https://www.cambridge.org/core/terms},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/XNWEGRMJ/Thagard (1990) - Philosophy and Machine Learning.pdf}
}

@article{thorstadTwoParadoxesBounded2022,
  title = {Two Paradoxes of Bounded Rationality},
  author = {Thorstad, David},
  year = {2022},
  month = nov,
  journal = {Philosophers' Imprint},
  volume = {22},
  number = {0},
  publisher = {Michigan Publishing Services},
  issn = {1533-628X},
  doi = {10.3998/phimp.1198},
  urldate = {2025-01-06},
  abstract = {My aim in this paper is to develop a unified solution to two paradoxes of bounded rationality. The first is the regress problem that incorporating cognitive bounds into models of rational decisionmaking generates a regress of higher-order decision problems. The second is the problem of rational irrationality: it sometimes seems rational for bounded agents to act irrationally on the basis of rational deliberation. I review two strategies which have been brought to bear on these problems: the way of weakening which responds by weakening rational norms, and the way of indirection which responds by letting the rationality of behavior be determined by the rationality of the deliberative processes which produced it. Then I propose and defend a third way to confront the paradoxes: the way of level separation.},
  copyright = {{\copyright} 2022 David Thorstad. CC BY-NC-ND 4.0This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 3.0 License {$<$}www.philosophersimprint.org/022015/{$>$}},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/L4MXJ3WX/Thorstad (2022) - Two paradoxes of bounded rationality.pdf}
}

@article{thorstadWhyBoundedRationality2024,
  title = {Why Bounded Rationality (in Epistemology)?},
  author = {Thorstad, David},
  year = {2024},
  journal = {Philosophy and Phenomenological Research},
  volume = {108},
  number = {2},
  pages = {396--413},
  issn = {1933-1592},
  doi = {10.1111/phpr.12978},
  urldate = {2025-01-06},
  abstract = {Bounded rationality gets a bad rap in epistemology. It is argued that theories of bounded rationality are overly context-sensitive; conventionalist; or dependent on ordinary language (Carr, 2022; Pasnau, 2013). In this paper, I have three aims. The first is to set out and motivate an approach to bounded rationality in epistemology inspired by traditional theories of bounded rationality in cognitive science. My second aim is to show how this approach can answer recent challenges raised for theories of bounded rationality. My third aim is to clarify the role of rational ideals in bounded rationality.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/2BFYFPRD/Thorstad (2024) - Why bounded rationality (in epistemology).pdf;/Users/paultalma/Zotero/storage/WMKBH9JW/phpr.html}
}

@misc{Top6Machine10:20:26+00:00,
  title = {Top 6 {{Machine Learning Classification Algorithms}}},
  year = {10:20:26+00:00},
  journal = {GeeksforGeeks},
  urldate = {2025-01-17},
  abstract = {A Computer Science portal for geeks. It contains well written, well thought and well explained computer science and programming articles, quizzes and practice/competitive programming/company interview Questions.},
  chapter = {Machine Learning},
  howpublished = {https://www.geeksforgeeks.org/top-6-machine-learning-algorithms-for-classification/},
  langid = {american}
}

@article{valiantTheoryLearnable1984,
  title = {A Theory of the Learnable},
  author = {Valiant, L. G.},
  year = {1984},
  month = nov,
  journal = {Communications of the ACM},
  volume = {27},
  number = {11},
  pages = {1134--1142},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/1968.1972},
  urldate = {2024-11-11},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/8KLWQCX6/Valiant (1984) - A theory of the learnable.pdf}
}

@article{vanrooijCognitionIntractabilityGuide,
  title = {Cognition and {{Intractability}}: {{A Guide}} to {{Classical}} and {{Parameterized Complexity Analysis}}},
  author = {{van Rooij}, Iris and Blokpoel, Mark and Kwisthout, Johan and Wareham, Todd},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/2KE5ZH7N/Cognition and Intractability A Guide to Classical and Parameterized Complexity Analysis.pdf}
}

@article{vassendPhilosophicalSignificanceSteins2017,
  title = {The Philosophical Significance of {{Stein}}'s Paradox},
  author = {Vassend, Olav and Sober, Elliott and Fitelson, Branden},
  year = {2017},
  journal = {European Journal for Philosophy of Science},
  volume = {7},
  number = {3},
  issn = {1879-4912},
  doi = {10.1007/s13194-016-0168-7},
  urldate = {2024-11-16},
  abstract = {Charles Stein discovered a paradox in 1955 that many statisticians think is of fundamental importance. Here we explore its philosophical implications. We outline the nature of Stein's result and of subsequent work on shrinkage estimators; then we describe how these results are related to Bayesianism and to model selection criteria like AIC. We also discuss their bearing on scientific realism and instrumentalism. We argue that results concerning shrinkage estimators underwrite a surprising form of holistic pragmatism.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/XZH6GYCR/Vassend et al. (2017) - The philosophical significance of Stein’s paradox.pdf;/Users/paultalma/Zotero/storage/J36ACHVD/10.html}
}

@article{walshComparingPeanoArithmetic2012,
  title = {Comparing {{Peano}} Arithmetic, {{Basic Law V}}, and {{Hume}}'s {{Principle}}},
  author = {Walsh, Sean},
  year = {2012},
  journal = {Annals of Pure and Applied Logic},
  volume = {163},
  number = {11},
  pages = {1679--1709},
  issn = {0168-0072},
  doi = {10.1016/j.apal.2011.12.016},
  abstract = {This paper presents new constructions of models of Hume's Principle and Basic Law V with restricted amounts of comprehension. The techniques used in these constructions are drawn from hyperarithmetic theory and the model theory of fields, and formalizing these techniques within various subsystems of second-order Peano arithmetic allows one to put upper and lower bounds on the interpretability strength of these theories and hence to compare these theories to the canonical subsystems of second-order arithmetic. The main results of this paper are: (i) there is a consistent extension of the hypearithmetic fragment of Basic Law V which interprets the hyperarithmetic fragment of second-order Peano arithmetic, and (ii) the hyperarithmetic fragment of Hume's Principle does not interpret the hyperarithmetic fragment of second-order Peano arithmetic, so that in this specific sense there is no predicative version of Frege's Theorem.},
  keywords = {Basic Law V,Hume's Principle,Hyperarithmetic,Interpretability,Recursively saturated,Second-order arithmetic}
}

@article{walshFragmentsFregesGrundgesetze2016,
  title = {Fragments of {{Frege}}'s {{Grundgesetze}} and {{G{\"o}del}}'s {{Constructible Universer}}},
  author = {Walsh, Sean},
  year = {2016},
  journal = {The Journal of Symbolic Logic},
  volume = {81},
  number = {2},
  pages = {605--628},
  doi = {10.1017/jsl.2015.32}
}

@article{walshLogicismInterpretabilityKnowledge2014,
  title = {Logicism, {{Interpretability}}, and {{Knowledge}} of {{Arithmetic}}},
  author = {Walsh, Sean},
  year = {2014},
  journal = {Review of Symbolic Logic},
  volume = {7},
  number = {1},
  pages = {84--119},
  doi = {10.1017/s1755020313000397}
}

@article{walshOccamsRazorPrinciple1979,
  title = {Occam's {{Razor}}: {{A Principle}} of {{Intellectual Elegance}}},
  shorttitle = {Occam's {{Razor}}},
  author = {Walsh, Dorothy},
  year = {1979},
  journal = {American Philosophical Quarterly},
  volume = {16},
  number = {3},
  eprint = {20009764},
  eprinttype = {jstor},
  pages = {241--244},
  publisher = {[North American Philosophical Publications, University of Illinois Press]},
  issn = {0003-0481},
  urldate = {2024-11-22},
  file = {/Users/paultalma/Zotero/storage/LSU4UNYZ/Walsh (1979) - Occam's Razor A Principle of Intellectual Elegance.pdf}
}

@article{walshRelativeCategoricityAbstraction2015,
  title = {Relative {{Categoricity}} and {{Abstraction Principles}}},
  author = {Walsh, Sean and {Ebels-Duggan}, Sean},
  year = {2015},
  journal = {Review of Symbolic Logic},
  volume = {8},
  number = {3},
  pages = {572--606},
  doi = {10.1017/s1755020315000052}
}

@article{warrenMetasemanticChallengeMathematical2020,
  title = {A {{Metasemantic Challenge}} for {{Mathematical Determinacy}}},
  author = {Warren, Jared and Waxman, Daniel},
  year = {2020},
  journal = {Synthese},
  volume = {197},
  number = {2},
  pages = {477--495},
  publisher = {Springer Verlag},
  doi = {10.1007/s11229-016-1266-y}
}

@book{warrenShadowsSyntaxRevitalizing2020,
  title = {Shadows of {{Syntax}}: {{Revitalizing Logical}} and {{Mathematical Conventionalism}}},
  author = {Warren, Jared},
  year = {2020},
  publisher = {Oxford University Press},
  address = {New York, USA}
}

@article{watkinsQlearning1992,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  year = {1992},
  month = may,
  journal = {Machine Learning},
  volume = {8},
  number = {3-4},
  pages = {279--292},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00992698},
  urldate = {2025-01-28},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/JCKCASSV/Watkins and Dayan (1992) - Q-learning.pdf}
}

@misc{weissThinkingTransformers2021,
  title = {Thinking {{Like Transformers}}},
  author = {Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  year = {2021},
  month = jul,
  number = {arXiv:2106.06981},
  eprint = {2106.06981},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2106.06981},
  urldate = {2024-11-24},
  abstract = {What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder -- attention and feed-forward computation -- into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/paultalma/Zotero/storage/IVGEF6NH/Weiss et al. (2021) - Thinking Like Transformers.pdf;/Users/paultalma/Zotero/storage/6HUYXYCW/2106.html}
}

@incollection{westerstahlCompositionalityContext2021,
  title = {Compositionality in {{Context}}},
  booktitle = {Outstanding {{Contributions}} to {{Logic}}: {{Samson Abramsky}}},
  author = {Westerst{\aa}hl, Dag and Baltag, Alexandru and van Benthem, Johan},
  editor = {Palmigiano, A. and Zadrzadeh, M.},
  year = {2021},
  publisher = {Springer}
}

@book{wetzelTypesTokensAbstract2009,
  title = {Types and {{Tokens}}: {{On Abstract Objects}}},
  author = {Wetzel, Linda},
  year = {2009},
  publisher = {MIT Press},
  address = {Cambridge, Mass.}
}

@misc{wijmansEmergenceMapsMemories2023,
  title = {Emergence of {{Maps}} in the {{Memories}} of {{Blind Navigation Agents}}},
  author = {Wijmans, Erik and Savva, Manolis and Essa, Irfan and Lee, Stefan and Morcos, Ari S. and Batra, Dhruv},
  year = {2023},
  month = jan,
  number = {arXiv:2301.13261},
  eprint = {2301.13261},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2301.13261},
  urldate = {2024-11-26},
  abstract = {Animal navigation research posits that organisms build and maintain internal spatial representations, or maps, of their environment. We ask if machines -- specifically, artificial intelligence (AI) navigation agents -- also build implicit (or 'mental') maps. A positive answer to this question would (a) explain the surprising phenomenon in recent literature of ostensibly map-free neural-networks achieving strong performance, and (b) strengthen the evidence of mapping as a fundamental mechanism for navigation by intelligent embodied agents, whether they be biological or artificial. Unlike animal navigation, we can judiciously design the agent's perceptual system and control the learning paradigm to nullify alternative navigation mechanisms. Specifically, we train 'blind' agents -- with sensing limited to only egomotion and no other sensing of any kind -- to perform PointGoal navigation ('go to \${\textbackslash}Delta\$ x, \${\textbackslash}Delta\$ y') via reinforcement learning. Our agents are composed of navigation-agnostic components (fully-connected and recurrent neural networks), and our experimental setup provides no inductive bias towards mapping. Despite these harsh conditions, we find that blind agents are (1) surprisingly effective navigators in new environments ({\textasciitilde}95\% success); (2) they utilize memory over long horizons (remembering {\textasciitilde}1,000 steps of past experience in an episode); (3) this memory enables them to exhibit intelligent behavior (following walls, detecting collisions, taking shortcuts); (4) there is emergence of maps and collision detection neurons in the representations of the environment built by a blind agent as it navigates; and (5) the emergent maps are selective and task dependent (e.g. the agent 'forgets' exploratory detours). Overall, this paper presents no new techniques for the AI audience, but a surprising finding, an insight, and an explanation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  file = {/Users/paultalma/Zotero/storage/SKUY5ZJ9/Wijmans et al. (2023) - Emergence of Maps in the Memories of Blind Navigation Agents.pdf;/Users/paultalma/Zotero/storage/76VAG6PI/2301.html}
}

@article{wilkenfeldUnderstandingCompression2019,
  title = {Understanding as Compression},
  author = {Wilkenfeld, Daniel A.},
  year = {2019},
  month = oct,
  journal = {Philosophical Studies},
  volume = {176},
  number = {10},
  pages = {2807--2831},
  issn = {1573-0883},
  doi = {10.1007/s11098-018-1152-1},
  urldate = {2025-01-13},
  abstract = {What is understanding? My goal in this paper is to lay out a new approach to this question and clarify how that approach deals with certain issues. The claim is that understanding is a matter of compressing information about the understood so that it can be mentally useful. On this account, understanding amounts to having a representational kernel and the ability to use it to generate the information one needs regarding the target phenomenon. I argue that this ambitious new account can accommodate much of the data that has motivated theories of understanding in philosophy of science, and can also be generally applicable in epistemology and daily life as well.},
  langid = {english},
  keywords = {Compression,Epistemology,Explanation,Philosophy of science,Understanding},
  file = {/Users/paultalma/Zotero/storage/43SLGWHC/Wilkenfeld (2019) - Understanding as compression.pdf}
}

@article{williamsonMotivatingObjectiveBayesianism,
  title = {Motivating {{Objective Bayesianism}}: {{From Empirical Constraints}} to {{Objective Probabilities}}},
  author = {Williamson, Jon},
  abstract = {Kyburg goes half-way towards objective Bayesianism. He accepts that frequencies constrain rational belief to an interval but stops short of isolating an optimal degree of belief within this interval. I examine the case for going the whole hog.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/5BI4WQ2M/Williamson Motivating Objective Bayesianism From Empirical Constraints to Objective Probabilities.pdf}
}

@article{wilsonTenSimpleRules2019,
  title = {Ten Simple Rules for the Computational Modeling of Behavioral Data},
  author = {Wilson, Robert C and Collins, Anne GE},
  editor = {Behrens, Timothy E},
  year = {2019},
  month = nov,
  journal = {eLife},
  volume = {8},
  pages = {e49547},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.49547},
  urldate = {2024-12-01},
  abstract = {Computational modeling of behavior has revolutionized psychology and neuroscience. By fitting models to experimental data we can probe the algorithms underlying behavior, find neural correlates of computational variables and better understand the effects of drugs, illness and interventions. But with great power comes great responsibility. Here, we offer ten simple rules to ensure that computational modeling is used with care and yields meaningful insights. In particular, we present a beginner-friendly, pragmatic and details-oriented introduction on how to relate models to data. What, exactly, can a model tell us about the mind? To answer this, we apply our rules to the simplest modeling techniques most accessible to beginning modelers and illustrate them with examples and code available online. However, most rules apply to more advanced techniques. Our hope is that by following our guidelines, researchers will avoid many pitfalls and unleash the power of computational modeling on their own data.},
  keywords = {computational modeling,model fitting,reproducibility,validation},
  file = {/Users/paultalma/Zotero/storage/FWQNDG2S/Wilson and Collins (2019) - Ten simple rules for the computational modeling of behavioral data.pdf}
}

@book{wrightFregesConceptionNumbers1983,
  title = {Frege's {{Conception}} of {{Numbers}} as {{Objects}}},
  author = {Wright, Crispin},
  year = {1983},
  publisher = {Aberdeen University Press},
  address = {Aberdeen}
}

@incollection{wrightHumesPrincipleAnalytic2001,
  title = {Is {{Hume}}'s {{Principle Analytic}}?},
  booktitle = {The {{Reason}}'s {{Proper Study}}: {{Essays Towards}} a {{Neo-Fregean Philosophy}} of {{Mathematics}}},
  author = {Wright, Crispin},
  editor = {Hale, Bob and Wright, Crispin},
  year = {2001},
  publisher = {Clarendon Press}
}

@article{zafforablandoLearningTheoreticCharacterizationMartinLof2021,
  title = {A {{Learning-Theoretic Characterization}} of {{Martin-L{\"o}f Randomness}} and {{Schnorr Randomness}}},
  author = {Zaffora Blando, Francesca},
  year = {2021},
  month = jun,
  journal = {The Review of Symbolic Logic},
  volume = {14},
  number = {2},
  pages = {531--549},
  issn = {1755-0203, 1755-0211},
  doi = {10.1017/S175502031900042X},
  urldate = {2024-11-03},
  abstract = {Numerous learning tasks can be described as the process of extrapolating patterns from observed data. One of the driving intuitions behind the theory of algorithmic randomness is that randomness amounts to the absence of any effectively detectable patterns: it is thus natural to regard randomness as antithetical to inductive learning. Osherson and Weinstein [11] draw upon the identification of randomness with unlearnability to introduce a learning-theoretic framework (in the spirit of formal learning theory) for modelling algorithmic randomness. They define two success criteria---specifying under what conditions a pattern may be said to have been detected by a computable learning function---and prove that the collections of data sequences on which these criteria cannot be satisfied correspond to the set of weak 1-randoms and the set of weak 2-randoms, respectively. This learning-theoretic approach affords an intuitive perspective on algorithmic randomness, and it invites the question of whether restricting attention to learning-theoretic success criteria comes at an expressivity cost. In other words, is the framework expressive enough to capture most core algorithmic randomness notions and, in particular, Martin-L{\"o}f randomness---arguably, the most prominent algorithmic randomness notion in the literature? In this article, we answer the latter question in the affirmative by providing a learning-theoretic characterisation of Martin-L{\"o}f randomness. We then show that Schnorr randomness, another central algorithmic randomness notion, also admits a learning-theoretic characterisation in this setting.},
  langid = {english},
  file = {/Users/paultalma/Zotero/storage/VK38UBCI/Zaffora Blando - 2021 - A LEARNING-THEORETIC CHARACTERISATION OF MARTIN-LÖF RANDOMNESS AND SCHNORR RANDOMNESS.pdf}
}

@book{zellnerSimplicityInferenceModelling2002,
  title = {Simplicity, {{Inference}} and {{Modelling}}: {{Keeping}} It {{Sophisticatedly Simple}}},
  shorttitle = {Simplicity, {{Inference}} and {{Modelling}}},
  editor = {Zellner, Arnold and Keuzenkamp, Hugo A. and McAleer, Michael},
  year = {2002},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511493164},
  urldate = {2024-11-30},
  abstract = {The idea that simplicity matters in science is as old as science itself, with the much cited example of Ockham's Razor, 'entia non sunt multiplicanda praeter necessitatem': entities are not to be multiplied beyond necessity. A problem with Ockham's razor is that nearly everybody seems to accept it, but few are able to define its exact meaning and to make it operational in a non-arbitrary way. Using a multidisciplinary perspective including philosophers, mathematicians, econometricians and economists, this 2002 monograph examines simplicity by asking six questions: what is meant by simplicity? How is simplicity measured? Is there an optimum trade-off between simplicity and goodness-of-fit? What is the relation between simplicity and empirical modelling? What is the relation between simplicity and prediction? What is the connection between simplicity and convenience? The book concludes with reflections on simplicity by Nobel Laureates in Economics.},
  isbn = {978-0-521-80361-8},
  file = {/Users/paultalma/Zotero/storage/ID9Q4WP5/Zellner et al. (2002) - Simplicity, Inference and Modelling Keeping it Sophisticatedly Simple.pdf;/Users/paultalma/Zotero/storage/IKVLZGZG/A170CFFE6EA3FC7A4326710430EBDF0B.html}
}
