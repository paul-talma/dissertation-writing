Bayesian Decision Theory provides a standard for evaluating an agent's behavior, in light of his preferences and credal states.
Reinforcement Learning provides algorithms allowing agents to learn behavioral policies on the basis of their experience.
These policies serve as solutions to sequential decision problems.
Thus, it is natural to ask, how do the solutions provided by Reinforcement Learning algorithms fare by the lights of Bayesian Decision Theory?

Hutteger [CITE] has shown that under certain conditions, a certain class of reinforcement learning algorithms is in fact Bayes optimal.
His analysis, however, is highly abstract, and it is not clear how the algorithms he considers relate to those used in contemporary reinforcement learning.
In this section, we first recapitulate Hutteger's framework and results.
We then translate them to the contemporary setting of Markov Decision Processes.
This setting clarifies the import of Hutteger's results for reinforcement learning algorithms, and highlights questions it leaves unanswered.
Finally, we address the questions which our broader framework allows us to pose.
