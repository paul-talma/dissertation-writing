# behavioral flexibility

- kinds of behavioral flexibility: (1) quickly adapting to changes in the environment, (2) succeeding at a task across a sufficiently wide range of conditions, (3) being able to do achieve a goal in many ways.
    1. All RL agents demonstrate _some_ degree of adaptability: indeed, the whole point of RL is to learn from experience. Even the stupidest model-free baby will adapt to changes in its environment.
        - So what kind of adaptation is made possible by possessing a model, or other model-like cognitive structures?
        - A model allows for _rapid_ adaptation? Well, so does model-free learning, if the learning rate is set high enough (this would be bad for independent reasons, however).
        - A model allows for adaptation without visitation: action values can be adapted without taking said action.
        - Now we distinguish two kinds of changes to which we might need to adapt.
            - Changes in the environment's statistical structure: the distribution of next states conditional on current state and action changes.
                - this would be the case if the physical layout of the environment changed (would there be other causes?)
            - Changes in the environment's reward structure: the distribution of rewards conditional on current state and action changes
                - this would be the case if some outcome became devalued (or revalued).
            - A _full model_ of the environment (one that includes both the state and reward distributions) would allow adaptation to both kinds of changes.
            - A partial model would only allow adaptation to one kind of change.
            - What about something like the successor representation? One question is what exactly is encoded in the representation. If I've understood correctly, the representation of $s$ encodes, for each state $s^\prime$, something like the limiting frequency of visiting $s^\prime$ when starting in $s$ (I say something like because it's really the discounted frequency, so visiting $s^*$ after one step increments the counter by $1 - \gamma$, but visiting it after $10$ steps increments the counter by $(1 - \gamma)^10$).

- one kind of behavioral flexibility is the ability to quickly adapt to changes in one's environment, or to succeed at a task across a sufficiently wide range of conditions.
- RL helps us tease apart different kinds of counterfactuals, and the cognitive and neural underpinnings of the corresponding kinds of flexibility.
- MF learning: relatively inflexible: the learner must re-learn how to act if the state or reward probabilities change.

# Generalization

# representation learning
- large state spaces necessitate function approximation: the value of the approximation function on its uncountably infinitely many inputs must be determined by a finite number of parameters (this follows from turing computability? if the outputs must be given after some finite amout of time, uniformly across inputs? the organism must learn a finitely specifiable rule covering infinitely many inputs)

- learning and planning is made more efficient by grouping actions together (the learned policy in the limit can only be worse, but a policy learned by a given point in time can certainly be better).
- how to group actions, however, is not clear   
    - a good set of options is one that makes it easier to learn the task: makes convergence to a good policy faster.
    - but a good option space is one that makes it easier to learn good option sets. In other words, if there are too many options available, it will be harder to select the ones most conducive to the goal.

- large state spaces necessitate generalizing from seen to unseen states: only a small fraction of the states will be encountered more than once.

- Observation Problems: states are constructed out of observation histories. They are expected to compress the information available in the history of observations into a single, constant size representation.
    - a state is markov if it carries as much predictive information as the whole history itself (lossless compression)
    - a history-to-state function should ensure that the next state is a simple function of the current state and the observations
    - bayesian approach (POMDPS): the observations are probabilistically generated by latent causes

# Environment selection
- RL agent must not only learn about his environment but also learn what environment he's in.
    - the RL framework provides tools for learning how to act in an environment.
    - but it doesn't tell you how to figure out that you're in a new environment.
    - if the environment changes abruptly, an RL learner will just continue to adapt to the new environment. This learning will both be slowed by the information gathered about the previous environment, and will erase this information
    - thus, having a way of detecting and switching between environments is crucial to reinforcement learning across environments

# Task learning
- RL is sometimes greatly sped up by the use of sub-tasks.
- The idea is to supplement sparse and difficult to achieve rewards related to the main or final objective with more proximal and easier to achieve rewards.
    - This is analogous to shaping in instrumental conditioning
- To be effective, the subtasks need to bear a good relation to the goal.
    - They should steer the agent toward the goal, but they shouldn't come to supplant it.
        - One behavioral trap: getting stuck chasing subtask reward instead of main task reward.


